%\documentclass[ngerman,11pt,a4paper]{article} % evtl. besser 11 pt
\documentclass[english,11pt,a4paper,table]{article} % evtl. besser 11 pt
\usepackage[utf8]{inputenc} % bei Nutzung unter Linux oder anderen UTF-8 Systemen
%\usepackage[ansinew]{inputenc} % bei Nutzung unter Windows
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{xspace}
\usepackage{longtable}
\usepackage{rotating}
\usepackage{listings} % source code printing
\usepackage{graphicx,color} %%For loading graphic files
\usepackage{fancyhdr} %%Fancy headings
\usepackage{amsmath,amsfonts,amssymb}
\usepackage[font=small,labelfont=bf,format=hang]{caption}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=newest}
\usepackage{booktabs,colortbl}
\usepackage[english]{babel}
\usepackage{float}
\usepackage[left=2.9cm,right=2.9cm,top=3cm,bottom=3cm]{geometry}
\usepackage{multirow}
\usepackage{appendix}
\usepackage[T1]{fontenc}
\usepackage{csquotes}
%\usepackage[style=alphabetic,maxbibnames=3,backend=bibtex,doi=false,isbn=false,url=false,urldate=comp,dateabbrev=false]{biblatex}
\usepackage[backend=biber,style=ieee]{biblatex}
\usepackage{pdfpages}
\usepackage{flafter}
\usepackage{parskip}
\usepackage[capitalise]{cleveref}
\usepackage{physics}
%\usepackage{minted}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}
\usepackage{wrapfig}
\usepackage[section]{minted}
\usepackage{tabularx}
\usepackage{verbatim}
%\usepackage[a-2b,mathxmp]{pdfx}[2018/12/22]

\definecolor{LightGray}{gray}{0.9}
\usemintedstyle{vs}

\usepgfplotslibrary{groupplots} % LATEX and plain TEX
\usepgfplotslibrary[groupplots] % ConTEXt
\usetikzlibrary{pgfplots.groupplots} % LATEX and plain TEX
\usetikzlibrary[pgfplots.groupplots] % ConTEXt
\usetikzlibrary{shapes,arrows}


%\setlength{\parindent}{0pt} % Einzug in erster Zeile des Absatzes auf 0

\newcommand{\C}{\,^{\circ}C} % Befehl erzeugt °C
\def\TReg{\textsuperscript{\textregistered}} % Befehl erzeugt R-Symbol
\def\TCop{\textsuperscript{\textcopyright}} % Befehl erzeugt C-Symbol
\def\TTra{\textsuperscript{\texttrademark}} % Befehl erzeugt TM-Symbol
\DeclareMathOperator{\sgn}{sgn}

\newcommand{\settocdepth}[1]{%
  \addtocontents{toc}{\protect\setcounter{tocdepth}{#1}}} % Die einzelnen Anhänge sollen nicht im Inhaltsverzeichnis erscheinen.

\renewcommand{\theequation}{\arabic{section}.\arabic{equation}} % Nummerierung der Gleichungen nach Kapitel.Gleichungsnummer
\renewcommand\thefigure{\arabic{section}.\arabic{figure}} % Nummerierung der Abbildungen nach Kapitel.Abbildungsnummer
\renewcommand\thetable{\arabic{section}.\arabic{table}} % Nummerierung der Tabellen nach Kapitel.Tabellennummer
\let\stdsection\section % Jedes Kapitel auf einer neuen Seite beginnen
\renewcommand\section{\clearpage\newpage\stdsection} % Jedes Kapitel auf einer neuen Seite beginnen

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}




\makeatletter
\RequireBibliographyStyle{numeric-comp}
\makeatother
\bibliography{literatur} % Literaturdaten befinden sich in literatur.bib
\providecommand{\apashortdash}{-}
\DeclareFieldInputHandler{shorthand}{\def\NewValue{}}
%\input{biblatex.conf}

\begin{document}

% insert coversheet
\input{coversheet.tex}

% set page margins for document except coversheet
\newgeometry{left=2.2cm,right=2.2cm,top=3cm,bottom=3cm}

\input{firstPages.tex} % Titelseite, Abstract, Zusammenfassung, Inhaltsverzeichnis, Formelzeichen und Indizes, Abkürzungen

\section{Introduction}
\label{intro}

Python is a high-level programming language,
which has become very widespread in recent years and can currently be considered to
be the most widely used general programming language. \cite{tiobe_2022}
Python owes its popularity to its readability, conciseness and low barrier of entry.
It has been widely adopted in many fields of research and science.
This use case in science and engineering fostered and benefitted greatly from what is often called the "scientific Python stack",
a set of essential packages, which enable the faster execution of common numerical tasks.
The package Numpy \cite{numpy} is the most influential of those packages, since it defines
a general and versatile class for arrays of arbitrary dimension: \texttt{ndarray}\cite{ndarray}.

\subsection{Properties and Limitations of CPython}
\label{dis}

The advantages of Python such as its dynamic-typing and high level of abstraction come at a cost.
This section discusses this cost by examining the CPython version of Python.
It is the most popular flavor of Python and is usually considered the reference implementation.
Before Python code is run, it is compiled into so-called Python bytecode, which is then interpreted
by the Python interpreter. This turns the simple multiplication of two numbers,
which could otherwise be expressed in a single x86 CPU instruction, into a lengthy process of checking
which types are involved and garbage-collection.

For example a function which multiplies two Python objects and the bytecode it gets compiled to is displayed in the following listing \ref{bytecodelst} (The produced bytecode can be inspected by using the \texttt{dis}-module \cite{dis}):

\begin{listing}[H]
	\begin{minipage}{.45\textwidth}
		\begin{minted}
	[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	linenos
	]{python}
def multiply(a,b):
    return a*b
\end{minted}
	\end{minipage}\hfill%
	\begin{minipage}{.45\textwidth}
		\begin{minted}
		[
		frame=lines,
		framesep=2mm,
		baselinestretch=1.2,
		bgcolor=LightGray,
		fontsize=\footnotesize,
		linenos
		]{python}
2       0 LOAD_FAST                0 (a)
	2 LOAD_FAST                1 (b)
	4 BINARY_MULTIPLY
	6 RETURN_VALUE
	\end{minted}
	\end{minipage}%
	\caption{Function definition and corresponding bytecode}
	\label[code]{bytecodelst}
\end{listing}


The bytecode shown on the right is interpreted by code defined in CPython's \texttt{ceval.c},
which is located at the core of the interpreter and consists of a very large switch-case-statement that determines how to handle the individual commands like \texttt{LOAD\_FAST} and \texttt{BINARY\_MULTIPLY} (so-called opcodes). \cite{ceval}

Listing \ref{BINARY_MULTIPLY} shows the Interpreter code executed for \texttt{BINARY\_MULTIPLY} in detail. In this case, it deals with the multiplication between the objects \texttt{a} and \texttt{b}.
It takes the two values (using \texttt{POP}, which removes the uppermost object from the stack and returns a pointer to that object and \texttt{TOP}, which retrieves a pointer to the uppermost object on the stack) from the stack (where they were stored by the preceding \texttt{LOAD\_FAST} op-codes), calculates the result by calling \texttt{PyNumber\_Multiply} (displayed in listing \ref{BINARY_MULTIPLY})defined in CPython's \texttt{abstract.c} (which is actually a misnomer because it handles the multiplication of all Python-objects, not just numbers) and stores the result on the top of the stack (using \texttt{SET\_TOP}, which does not grow the stack, but just overwrites the top with a new value - so after calling \texttt{POP}, \texttt{TOP} and \texttt{SET\_TOP} the original arguments of the functions, which were on top of the stack are now no longer on the stack at all), where the op-code \texttt{RETURN\_VALUE} retrieves it from and returns it back to the caller of the function.

\begin{listing}[H]
	\begin{minted}
			[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			firstnumber=1618,
			linenos
			]{c}
case TARGET(BINARY_MULTIPLY): {
	PyObject *right = POP();
	PyObject *left = TOP();
	PyObject *res = PyNumber_Multiply(left, right);
	Py_DECREF(left);
	Py_DECREF(right);
	SET_TOP(res);
	if (res == NULL)
		goto error;
	DISPATCH();
}
\end{minted}
	\caption{Excerpt from CPython's source code (ceval.c from Python 3.9 \cite{ceval})}
	\label[code]{BINARY_MULTIPLY}
\end{listing}

\texttt{PyNumber\_Multiply} in turn calls the function \texttt{binary\_op1} \cite[lines 848-881 in abstract.c]{abstractc} and passes it as arguments the two PyObjects to be multiplied and the slot where the desired binary operation is located.

\begin{listing}[H]
	\begin{minted}
			[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			firstnumber=1046,
			linenos
			]{c}
PyObject *
PyNumber_Multiply(PyObject *v, PyObject *w)
{
	PyObject *result = binary_op1(v, w, NB_SLOT(nb_multiply));
	if (result == Py_NotImplemented) {
		PySequenceMethods *mv = Py_TYPE(v)->tp_as_sequence;
		PySequenceMethods *mw = Py_TYPE(w)->tp_as_sequence;
		Py_DECREF(result);
		if  (mv && mv->sq_repeat) {
			return sequence_repeat(mv->sq_repeat, v, w);
		}
		else if (mw && mw->sq_repeat) {
			return sequence_repeat(mw->sq_repeat, w, v);
		}
		result = binop_type_error(v, w, "*");
	}
	return result;
}
\end{minted}
	\caption{Excerpt from CPython's source code (\texttt{abstract.c} from Python 3.9 \cite{abstractc})}
	\label[code]{PyNumber_Multiply}
\end{listing}

The function \texttt{binary\_op1} finally calls the appropriate function the objects point to for multiplication.
Objects containing floating-point numbers for example point to the function in listing \ref{float_mul}.
Note that all floating point numbers in Python are always double precision floating point numbers and all Integers are always of infinite precision.
Additionally, everything in Python is an Object and needs to be boxed within a PyObject as done on line 569 of listing \ref*{float_mul}.
As a consequence a floating point number in Python does not require 8 bytes of memory as expected from a double-precision floating point number, but 24 bytes because the PyObject has several other properties that need to be stored besides the double value it represents.

\begin{listing}[H]
	\begin{minted}
			[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			firstnumber=562,
			highlightlines = 568,
			linenos
			]{c}
static PyObject *
float_mul(PyObject *v, PyObject *w)
{
    double a,b;
    CONVERT_TO_DOUBLE(v, a);
    CONVERT_TO_DOUBLE(w, b);
    a = a * b;
    return PyFloat_FromDouble(a);
}
\end{minted}
	\caption{Excerpt from CPython's source code (texttt{floatobject.c} from Python 3.9 \cite{floatobjectc})}
	\label[code]{float_mul}
\end{listing}

Also, noteworthy are the calls to \texttt{Py\_DECREF}, these calls indicate that the variables \texttt{left} and \texttt{right} (which are no longer on the stack and contained pointers to the function arguments \texttt{a} and \texttt{b}) are no longer needed in this context and that their respective reference counters should be decremented by one.
Once the reference count of an object reaches zero, it will become de-allocated (also known as being garbage-collected).
This way of keeping track of objects being in use has far-reaching consequences for the performance of Python code (see section \ref{GIL-section}).

By downloading the Python source code and compiling it with debug-options enabled, we can generate a debuggable binary and use it to step-through the interpreter's code as it is executed.
This allows us to fully trace what happens on execution of the \texttt{BINARY\_MULTIPLY} op-code.
The interdependencies of functions involved is displayed in a condensed form in figure \ref{py_mult_trace}.

\begin{figure}[H]
	\includegraphics[width = \textwidth]{../py_multipy/trace.pdf}
	\caption{Call graph of the functions involved in the \texttt{BINARY\_MULTIPLY} op-code executed for a pair of floating point objects}
	\label{py_mult_trace}
\end{figure}

From this short exploration of CPython's source code it is already evident that multiplying two numbers in Python consists of more than a hundred lines of compiled C-Code and is certainly not as fast as the single x86 CPU-instruction, it ultimately boils down to (note the similarity between the highlighted line in listing \ref{float_mul} and line two in the left hand-side of listing \ref{c-mult}).
A function that multiplies two floating point numbers in C or C++ and the resulting assembly code is displayed in listing \ref{c-mult}.
The dynamic nature of the language, however, is also very powerful and allows Python to perform multiplication between all kinds of objects with the same function.
If, for example, one of the factors of the multiplication can be interpreted as a sequence and the other one as a positive Integer, the output is that sequence repeated by the value of the other factor (lines 1105-1116 in listing \ref{PyNumber_Multiply}), as demonstrated in listing \ref{non-numeric}. This would not be possible to be expressed as concisely in a language like C/C++, where functions need to have typed signatures. Adding type-annotations to Python functions like specified in a Python Enhancement Proposal (PEP 484 \cite{pep484}), also does not change this behavior.

\begin{listing}[H]
	\begin{minted}
				[
				frame=lines,
				framesep=2mm,
				baselinestretch=1.2,
				bgcolor=LightGray,
				fontsize=\footnotesize,
				breaklines=true,
				linenos
				]{python}
>>> multiply('Abc',5)
'AbcAbcAbcAbcAbc'
>>> multiply(5,'Abc')
'AbcAbcAbcAbcAbc'
	\end{minted}
	\caption{Example of the function applied to non-numeric data}
	\label[code]{non-numeric}
\end{listing}


\begin{listing}[H]
	\begin{minipage}{.45\textwidth}
		\begin{minted}
	[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	linenos
	]{C}
double multiply(double a, double b) {
	return a * b;
	}
\end{minted}
	\end{minipage}\hfill%
	\begin{minipage}{.45\textwidth}
		\begin{minted}
		[
		frame=lines,
		framesep=2mm,
		baselinestretch=1.2,
		bgcolor=LightGray,
		fontsize=\footnotesize,
		linenos
		]{asy}
multiply:
	mulsd   xmm0, xmm1
	ret
	\end{minted}
	\end{minipage}%
	\caption{C function definition and corresponding assembly output for x86-64; compiled with clang 14.0.0 using compiler flag \texttt{-O2}}
	\label[code]{c-mult}
\end{listing}

\subsubsection{Concurrency in Python - The Global Interpreter Lock}
\label{GIL-section}

Many programming languages offer the functionality to spread work over several threads to parallelize and effectively accelerate the execution of data. Python offers the threading-module \cite{threading} to perform tasks \emph{concurrently} (in an inter-leaved fashion), but not to perform them in a truly parallel way (separate hardware performing separate tasks at the same time).

The example presented in listing \ref{incrementing-non-thread} is used to demonstrate this:

Assuming we have a list with two integer entries and would like to increment both entries a specified number of times.
In a first step we will do this sequentially:

\begin{listing}[H]
	\begin{minted}
				[
				frame=lines,
				framesep=2mm,
				baselinestretch=1.2,
				bgcolor=LightGray,
				fontsize=\footnotesize,
				breaklines=true,
				linenos
				]{python}
>> x = [0,0]
>> count = 100_000_000		
>> for i in range(count):
>> 	x[0] += 1			
>> for i in range(count):
>> 	x[1] += 1

>> display(x)

[100000000, 100000000]
	\end{minted}
	\caption{Increasing the entries sequentially}
	\label[code]{incrementing-non-thread}
\end{listing}

This example took around 16 seconds to run, which equates to a counting frequency of 12.8 Mhz, not satisfactory considering CPU clock frequencies are ranging in the order of several Gigahertz, we investigate the threading-module and write code, which increases both values at the same time as shown in listing \ref{threading_1}.

We define two threads, start them both with the \texttt{start()} function calls and then wait for them to conclude with the \texttt{join()} function calls.
Without the \texttt{join()} function calls, the program would immediately display an intermediate result like \texttt{[316865, 160535]} without waiting for the two thread to finish their tasks of incrementing the values the specified number of times.
Waiting for all threads to finish is a form of synchronization, which is an important concept in parallel computing.

\begin{listing}[H]
	\begin{minted}
				[
				frame=lines,
				framesep=2mm,
				baselinestretch=1.2,
				bgcolor=LightGray,
				fontsize=\footnotesize,
				breaklines=true,
				linenos
				]{python}
>> def increase(var,idx, limit):
>> 	for i in range(limit):
>> 		var[idx] += 1
>> 	print(f"finished incrementing {idx}")

>> c = [0,0]
>> t1 = threading.Thread(target=increase, args=(x, 0, count))
>> t2 = threading.Thread(target=increase, args=(x, 1, count))
>> t1.start()
>> t2.start()
>> t1.join()
>> t2.join()

>> display(x)

finished incrementing 1
finished incrementing 0

[100000000, 100000000]
	\end{minted}
	\caption{Concurrently increasing both entries}
	\label[code]{threading_1}
\end{listing}

This code is only slightly faster at 13.7 seconds and this speed-up cannot be attributed to the fact that we use two-threads.
In fact the statement-sequence $\texttt{t1.start()} \rightarrow \texttt{t1.join()} \rightarrow \texttt{t2.start()} \rightarrow \texttt{\texttt{t2.join()}}$ would be exactly as fast without pretending to increment both values at the same time.
The reason why one cannot achieve speedups of computationally intensive tasks by utilizing Python's threading library is the GIL (Global Interpreter Lock).
The GIL ensures that only one thread at a time can interpret Python bytecode. At first this might seem like an arbitrary restriction, but it is a measure that eases Python's internal bookkeeping of \texttt{reference\_counts}.
If two threads were to modify the \texttt{refcnt} of a Python object at the same time, a so-called data-race (or race condition) might occur and the result could be incorrect.
Bugs due to data-races can be notoriously hard to track down and debug. \cite{Valgrind}

The GIL might be removed from Python at some future point, various attempts to remove it have been undertaken so far. \cite{Gilectomy, GIL2}

The presence of the GIL, however, does not guarantee, that code utilizing the threading-module is free of data races. An example that displays a data-race is presented in listing \ref{threading_2} If we task both threads with incrementing the first entry we would expect the entry to hold the value \text{200\_000\_000} after the following code concludes:

\begin{listing}[H]
	\begin{minted}
				[
				frame=lines,
				framesep=2mm,
				baselinestretch=1.2,
				bgcolor=LightGray,
				fontsize=\footnotesize,
				breaklines=true,
				linenos
				]{python}
>> def increase(var,idx, limit):
>> 	for i in range(limit):
>> 		var[idx] += 1
>> 	print(f"finished incrementing {idx}")

>> c = [0,0]
>> t1 = threading.Thread(target=increase, args=(x, 0, count))
>> t2 = threading.Thread(target=increase, args=(x, 0, count))
>> t1.start()
>> t2.start()
>> t1.join()
>> t2.join()

>> display(x)

finished incrementing 0
finished incrementing 0

[125290242, 0]
	\end{minted}
	\caption{Concurrently increasing the first entry}
	\label[code]{threading_2}
\end{listing}

But we received an incorrect value of \texttt{125\_290\_242} and we would have yielded further, most-likely incorrect, values if repeatedly executed.
The result of the code is non-deterministic.
This happens because threads waiting to execute byte-code will request the currently running thread to be suspended (often referred to as releasing the GIL) after waiting a set amount of time (this amount of time can be set globally by calling \texttt{sys.setswitchinterval()}, by default the value is set to 5 milliseconds).
After such a request has been raised, the currently running thread will not process any further op-codes.

To understand why this behavior is problematic in this situation, one needs to inspect the bytecode of the \texttt{increase}-function, which can be seen in listing \ref{bc increase}:

\begin{itemize}
	\item at position 18 the instruction \texttt{BINARY\_SUBSCR} loads the object from \text{var[idx]} onto the top of the stack.
	\item at position 20 the instruction \texttt{LOAD\_CONST} loads the constant value 1 onto the top of the stack.
	\item at position 22 the instruction \texttt{INPLACE\_ADD} adds the upper two elements of the stack together and puts the result onto the top of the stack, this calculates the value \texttt{var[idx] + 1}.
	\item at position 24 the instruction \texttt{ROT\_THREE} rotates the top three elements of the stack, so the order of elements (index, container and value) are in the expected order for the next operation.
	\item at position 26 \texttt{STORE\_SUBSCR} puts the calculated value at the correct position within the container.
\end{itemize}
\begin{listing}[H]
	\begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			linenos
			]{python}
2           0 LOAD_GLOBAL              0 (range)
	    2 LOAD_FAST                2 (limit)
	    4 CALL_FUNCTION            1
	    6 GET_ITER
      >>    8 FOR_ITER                20 (to 30)
	   10 STORE_FAST               3 (i)

3          12 LOAD_FAST                0 (var)
	   14 LOAD_FAST                1 (idx)
	   16 DUP_TOP_TWO
	   18 BINARY_SUBSCR
	   20 LOAD_CONST               1 (1)
	   22 INPLACE_ADD
	   24 ROT_THREE
	   26 STORE_SUBSCR
	   28 JUMP_ABSOLUTE            8

4     >>   30 LOAD_GLOBAL              1 (print)
	   ... code related to the print statement omitted
	   44 LOAD_CONST               0 (None)
	   46 RETURN_VALUE
	\end{minted}
	\caption{Bytecode of the increase-function}
	\label[code]{bc increase}
\end{listing}

The section ranging from position 18 to 26 forms a critical section.
If the thread is forced to release the GIL while in this area,
the value of \texttt{var[idx]} might be changed by other threads and once this threads resumes to be active, it will not be aware of that fact and overwrite the entry with the wrong value. \cite{ceval,ceval_gil,Understanding_the_Python_GIL}

It is also noteworthy how Python compiles our input into bytecode but does not perform any optimization at all,
whereas other compiler-language combinations (like C compiled with Clang or GCC) would first transform our input into an intermediate representation and perform optimizations on that before emitting executable code.
A possible optimization here would be to replace the for-loop with a \texttt{var[idx] += limit} statement.
Python cannot perform these optimizations because at the time the \texttt{increase}-function is declared, nothing is known about the arguments it will receive (After all it could be an object of arbitrary type with an arbitrarily overloaded \texttt{\_\_add\_\_}-method).

One could still make the code work using a mutex (mutually exclusive) lock guarding the critical section (see listing \ref{locked}),
which again is a common pattern in parallel computing, but does not at all help with performance in this case.
To the contrary, it dramatically decreases performance, because now the two threads still demand the GIL from one another but cannot actually perform any work if they acquire the GIL but fail to acquire the lock.
If that happens, the processor sits idle until the thread, which holds the lock gets another turn.
Using this version of the \texttt{increase}-function causes the runtime to rise to 2 minutes and 17 seconds, albeit returning the expected, correct result.

\begin{listing}
	\begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			linenos
			]{python}
lock = threading.Lock()

def increase(var,idx, limit):
	for i in range(limit):
		lock.acquire()
		var[idx] += 1
		lock.release()
	\end{minted}
	\caption{Increase function with a lock guarding the critical section}
	\label[code]{locked}
\end{listing}

This example demonstrates why even such a simple task as counting can become difficult when using multiple threads in Python as well as in languages with truly parallel multi-threading, further discussions can be found by McKenney. \cite[Chapter 5]{mckenney2009parallel}


\subsection{Circumventing the Limitations}

After having discussed the shortcomings (many layers of abstraction and indirection which are inefficient,
the dynamic nature of the language which prevents compile-time optimizations and
the GIL, which prevents compute-bound tasks from benefitting from multiple threads) of the CPython-interpreter for scientific computing,
one might wonder how Python has become so popular in that field.

When looking at the sources of Python packages commonly used in scientific programming like Python it quickly becomes obvious that most of the offered
computationally intensive functionality is not defined using Python code,
but using C/C++ or Fortran code that is called from Python.
Or in case of basic and ubiquitous building blocks of scientific programming like matrix-matrix-multiplications Numpy preferably
delegates the computation to an existing CBLAS \cite{BLAS} (C-bindings for Basic Linear Algebra Subprograms) compatible Installation if possible. \cite{numpy_matmul_impl}

BLAS-Libraries are generally very well optimized and tailored to specific hardware.
In fact many hardware-vendors offer their own implementations of BLAS.
For CPUs there is OpenBLAS \cite{OpenBLAS}, the Intel MKL (Math Kernel Library) \cite{MKL} and BLIS (AMD's implementation) \cite{amdblis} to name just a few.

Those libraries offer a range of common operations in linear algebra such as (but not limited to) the following functions:

\begin{itemize}
	\item vector operations (Level 1 - $\mathcal{O}(n)$)
	      \begin{itemize}
		      \item $x = \alpha x$, scaling a vector
		      \item $y = \alpha x + y$, adding vectors
		      \item $\left\langle x, y \right\rangle$, dot-product between vectors
		      \item $||x||_2$, calculating the magnitude
	      \end{itemize}
	\item matrix-vector operations (Level 2 - $\mathcal{O}(n^2)$)
	      \begin{itemize}
		      \item $x = \mathbf{A}x$, Matrix-vector Multiplication
		      \item $x = \mathbf{A}^{-1}x$, inverse Matrix vector Multiplication
		      \item $y = \alpha \mathbf{A} x + \beta y$, Matrix-vector multiplication plus vector
	      \end{itemize}
	\item matrix-matrix operations (Level 3 - $\mathcal{O}(n^3)$)
	      \begin{itemize}
		      \item $\mathbf{C} = \alpha\mathbf{A}\mathbf{B} + \beta\mathbf{C}$, General Matrix-Matrix Multiplication
		      \item special cases for symmetric, triangular and hermitian matrices
		      \item $y = \alpha \mathbf{A} x + \beta y$, Matrix-vector multiplication plus vector
	      \end{itemize}
	\item and variations of those, see \cite{BLAS} for a complete List.
\end{itemize}


\subsubsection{Comparing BLAS-Implementations}
\label{blas-numpy}

Naturally Numpy's performance for those operations is heavily dependent on the BLAS implementation it was linked against.
When installing the binary distribution of Numpy from anaconda \cite{anaconda} it can be linked against either OpenBLAS or MKL, if installed from PyPI it is linked against OpenBLAS according to Numpy's documentation.

For the sake of comparing the default Numpy distribution, which uses OpenBLAS, against a Numpy-installation linked against a different Library,
we download the sources for both Numpy and AMD's version of BLIS (because the test is performed on AMD Hardware) and compiled them.
One has to be careful when doing so, because both Numpy and BLIS need some compilation configuration for best performance and one might end up without any BLAS acceleration in Numpy if done wrongly.
This must be avoided since Numpy's fallback algorithms are not as sophisticated and do not claim to deliver comparable performance.

The following setups were compared:

\begin{itemize}
	\item{ Blis compiled with openmp:
	            \begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			]{bash}
./configure --enable-cblas --enable-threading=openmp zen2
make
	\end{minted}
	      }
	\item{ Blis compiled with pthreads:
	            \begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			]{bash}
./configure --enable-cblas --enable-threading=pthreads zen2
make
	\end{minted}
	      }
	\item{ Blis compiled with no multi-threading:
	            \begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			]{bash}
./configure --enable-cblas zen2
make
	\end{minted}
	      }
	\item Default Numpy installed from pip including OpenBLAS
	\item Numpy not linked against any BLAS library
\end{itemize}

Explanation of arguments:
\begin{itemize}
	\item \texttt{--enable-cblas} tells blis to build the necessary interface to call it from c-code (which Numpy is subsequently able to utilize).
	\item \texttt{--enbale-threading} is necessary to enable multithreading, one can choose between OpenMP \cite{OpenMP} (a rather high level abstraction to use multiple threads using compiler pragmas around sections that can be run in parallel)
	      or Pthreads \cite{pthreads} (short for POSIX Threads, a more-low level standard originating from Unix systems), which are the two most common ways of adding multi-threading capabilities to programs with OpenMP being the preferred option because it allows for fine-grained control over which thread is handled by which CPU-Core.
	\item \texttt{zen2} tells blis to optimize for AMD's Zen2 Architecture.
\end{itemize}

Checking whether all different implementations do in fact produce correct results is a non-trivial task, because usually Numpy's results are our trusted source of truth.
So for verification purposes we performed the calculations on a set of random but reproducible matrices (setting the random number generators seed ahead of execution) with the known good Numpy distribution and saved the complete results as binary files, which could later be read and compared with the other results (using \texttt{numpy.all\_close}).

\underline{Caution:} Because we are testing multithreaded functions that deal with (double precision) floating point numbers, the order of operations is not deterministic and since addition of floating point numbers is not associative, the results may vary slightly from run to run and from implementation to implementation. Calculating hashes based on the result and comparing these hashes might seem easy and convenient at first but does not work due to the used algorithms being non-deterministic.
This non-determinism stems from the fact that the order of operations is not deterministic and since addition of floating point numbers is not associative, the results may vary slightly from run to run and from implementation to implementation.

Figure \ref{Blas-Comparison} and Table \ref{blas-table} show that there are significant differences between the two different BLAS implementations and that it can be very beneficial to check whether the used Numpy installation is actually using an optimized and correctly configured BLAS-Backends.
The BLIS Backend we compiled from source with OpenMP and optimizations for the specific CPU-Architecture enabled was up to 40\% faster on small matrices ($100 \times 100$) and about 15\% faster for larger matrices ($4000 \times 4000$).
It can also be seen that not multithreading the computation slows it down by a factor of 4, which is a speed-up within the expected range for a CPU with 6 cores, and we can conclude that the OpenBLAS library bundled with Numpy must also take advantage of multithreading to achieve its performance.
In fact this can be verified by looking up the shared library dependencies of the \texttt{libopenblas} binary inside the directory containing the numpy extension using \texttt{ldd} (unix tool for \textbf{l}isting \textbf{d}ynamic \textbf{d}ependencies) and indeed we can recognize that is linked against pthreads.
More important than having a multithreading-enabled BLAS-backend is having any correctly set-up at all.
If Numpy cannot be linked against a BLAS library during the compilation process it will use the algorithms defined in its own C-codebase, which take anywhere between tens and hundreds times as long as the already suboptimal algorithms provided by OpenBLAS library in this example.


\begin{figure}[H]
	\includegraphics{figures/BLAS_comparison.pdf}
	\caption{Result of comparing the different BLAS implementations}
	\label[fig]{Blas-Comparison}
\end{figure}

Note: This is not meant to be a comprehensive and fair benchmark comparing OpenBLAS and BLIS.
It is a demonstration that displays the importance of the BLAS backend when using Numpy and highlights the fact that even large projects do not reinvent the wheel, but rely on existing and proven solutions for common problems.


\begin{table}[H]
	\rowcolors{4}{}{lightgray}
	\center
	\newcommand{\NA}{\cellcolor{white}}
	\begin{tabular}{lrccc}
		\toprule
		%\rowcolor{gray!50}
		\multirow{2}{*}{BLAS Backend}
		                             &              & \multicolumn{3}{c}{Relative Time compared to OpenBLAS}                       \\
		\cmidrule{3-5}
		                             & matrix sizes & 100x100                                                & 600x600 & 4000x4000 \\
		\midrule
		OpenBLAS                     & \NA          & 1                                                      & 1       & 1         \\
		BLIS OpenMP                  & \NA          & 0.54                                                   & 0.927   & 0.835     \\
		BLIS pthreads                & \NA          & 1.17                                                   & 0.895   & 0.789     \\
		BLIS singlethreaded          & \NA          & 0.96                                                   & 4.463   & 4.038     \\
		no working BLAS installation & \NA          & 13.7                                                   & 75.07   & 161.6     \\
		\bottomrule
	\end{tabular}
	\caption{GEMM-Performance using different BLAS Backends}
	\label[table]{blas-table}
\end{table}

\subsubsection{Interfacing C-Code}
\label{lower-level-packages}
%Todo rewrite

As explored in the previous section Python relies on calls to functions and libraries implemented in other languages, usually low-level, compiled languages like C/C++ or Fortran, for enhancing its numeric computing capabilities.
Because of this need to interface with code in written in other languages the Python ecosystem offers several packages which simplify this process or offer the functionality to take Python code and transpile it to one of those languages and compile it either just-in-time (JIT) or ahead of time (AOT).

The advantages of using compiled code are the ability to efficiently express algorithms in lower-level languages, which don't have to deal with the abstractions of the CPython interpreter and the ability to write multi-threaded code (since the interpreter is not involved, neither is the Global Interpreter Lock) and compiled code greatly benefits from compile-time optimization techniques like auto-vectorization, loop-unrolling and function-inlining among others.

Because of the relative ease involving linking libraries written in other programming languages to Python, Python is often called a "glue language".
Using this approach the Python language allows users to be more productive than the low-level languages would, while still reaching C-level performance using C-Code, which is hidden from the user.


Tool commonly used to interface libraries written in C/C++:
\begin{itemize}
	\item {Cython \cite{cython}

	      Cython (not to be Confused with CPython) is a programming language and compiler that allows users to annotate regular Python code with static type declarations.
	      Cython then takes that code and compiles it to C and subsequently a binary shared library which is embedded into an importable module.}
	\item {Numba \cite{lam2015numba}

	      Numba is a package that takes decorated Python functions and tries to compile them to LLVM-IR (an intermediate representation used in the low level virtual machine compiler framework) and subsequently binary code using the LLVM compiler.\cite{llvm}
	      The LLVM compiler can target many architectures, it includes backends for Nvidia and AMD GPUs, which enables Numba to compile Python code to GPU Kernels.
	      }
	\item {PyBind11 \cite{pybind11}

	      PyBind11 is a package which allows users to easily write bindings for existing C/C++ code and make it accessible from within Python.
	      }
	\item {Pythran \cite{guelton2015pythran}

	      Pythran is a project that compiles Python Code to  C++ Code, which is then compiled to a native binary. Not unlike Cython it is ahead of time compiled and produces an importable module.
	      Whereas the Cython language is a superset of the Python language, Pythran supports a subset of Python and therefore ensures backwards-compatibility.
	      Pythran code never interacts with the Python interpreter (this is optional in both Numba and Cython).
	      Despite its name Pythran is unrelated to Fortan.
	      }
\end{itemize}

\subsubsection{Case Study: Writing a matrix-multiplication algorithm using the mentioned libraries}

In a further step we explore and compare the mentioned libraries by implementing the naive non-tiling matrix-matrix-multiplication algorithm.
It should take two two-dimensional numpy-arrays $\mathbf{A}, \mathbf{B}$ as arguments and return the result $\mathbf{A}\mathbf{B}$.

\paragraph{Pure Python}
\label{pure-python-dgemm}

The simplest version of the algorithm would consist of three nested loops expressed using pure Python code.
This is a very slow way of performing this task because of the reasons mentioned before.

\begin{listing}[H]
	\begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			linenos
			]{python}
def mat_mul(a,b):
	result = np.zeros((a.shape[0], b.shape[1]), dtype=np.float64)
	for x_dim in range(result.shape[0]):
		for y_dim in range(result.shape[1]):
			for k in range(result.shape[0]):
				result[x_dim,y_dim] += a[x_dim, k]*b[k,y_dim]
return result
	\end{minted}
	\caption{Matrix-Matrix Multiplication algorithm expressed using Python}
	\label[code]{locked}
\end{listing}

\paragraph{Cython}
Cython (the programming language) is meant to be a superset of Python (almost all valid Python code is also valid cython code, but not vice-versa).
It allows users to annotate code with type annotations, which in this case means we can specify that the arguments passed to the function are two-dimensional numpy-arrays.
Additionally, so-called typed memoryviews can be defined as visible on lines 7-9, these allow the compiled function to read and or write the memory areas underlying the PyObjects without interacting with the Python Interpreter at all. \cite{TypedMem2:online}
This enables Cython to aot-compile (ahead of time compile) the function to C-Code and in a further step to a callable native library.

The annotations require minimal modifications to the previous Python-code and the function achieves a significant performance gain.
This is an acceptable trade-off as long as the types of the arguments and return value are known and fixed.
If we also wanted a similar function for single-precision floating point numbers or integers, the whole code would have to be repeated with that modification either manually, which is bad for maintainability,  or programmatically using a templating language (this is for example extensively done in the codebase of Numpy).


\begin{listing}[H]
	\begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			linenos
			]{python}
@cython.boundscheck(False)
@cython.wraparound(False)
def matmul_simple(numpy.ndarray[double, ndim=2] a, numpy.ndarray[double, ndim=2] b):

	result = np.zeros((arr_1.shape[0],arr_2.shape[1]), dtype=np.float64, order = 'C')

	cdef double[:, ::1] result_view = result
	cdef double[:, ::1] a_view = a
	cdef double[:, ::1] b_view = b

	for dim_x in range(result_view.shape[0]):
		for dim_y in range(result_view.shape[1]):
			for k in range(result_view.shape[0]):
				result_view[dim_x,dim_y] += a_view[dim_x,k]*b_view[k,dim_y]

	return result
	\end{minted}
	\caption{Matrix-Multiplication using Cython}
	\label[code]{locked}
\end{listing}

\paragraph{Cython using Vector instructions and multithreading} The strength of Cython lies within its ability to mix annotated Python code with user-written C-Code. In the example at hand we can replace the innermost loop that calculates dot-products between rows of $\mathbf{A}$ and columns of $\mathbf{B}$ with a more efficient function using AVX2-instructions.
AVX2 (Advanced Vector Instructions 2) is an extension to the x86-64 instruction set supported by recent Intel and AMD processors.
These instructions enable the CPU to process 256-bit wide vectors (which fit 4 double precision floats) of data at a time with a single instruction.
For example the instruction \texttt{\_mm256\_fmadd\_pd} accepts three of those vectors, each containing four floating point numbers and calculates the following expression elementwise $a \times b + c$, with the vectors being called $a$, $b$ and $c$ respectively.
Processing data in this way is called SIMD (Same Instruction Multiple Data)-Programming because the same instructions are applied to multiple elements of data at once.
Vector instructions however are not always easy to employ, they have high requirements to spatial nemory-locality for optimal performance.

Figure \ref{Memory Layout} visualizes how arrays can be laid out differently in memory, which can have big impacts on performance.
By default, Numpy creates row-major arrays, but this can be controlled with the \texttt{order} keyword argument of many array-creating functions in numpy. CPUs retrieve memory contents one cacheline at a time, with a cacheline ranging in size from 32 to 256 bytes (64 bytes fitting 8 64-bit floats on the Zen 2 architecture).\cite{Zen2Micr44:online}
Knowing this we can see that calculating the dot-product between rows and columns of row-major aligned matrices is not very efficient, since consecutive elements of a column are located in memory locations far-apart for all but the smallest arrays.
Transposing the second array or converting it using the Numpy function \texttt{asfortranarray} to minimize the cache misses before performing the inner loop dramatically increases performance.

\begin{figure}[H]
	\centering
	\includegraphics{figures/memory_layout.drawio.pdf}
	\caption{The two major ways of storing a 2D array in memory}
	\label[fig]{Memory Layout}
\end{figure}

Listing \ref{refactor} show the adapted code with the innermost loop refactored into a specialized function, which calculates the dot-product.
One also can see that the matrix $\mathbf{B}$ gets transposed (line 9) before a view into its memory is created to benefit from the changed layout in memory
and that some memory \texttt{temp} is allocated (line 12) and a pointer to those memory regions passed to the dotproduct-function.
The dotproduct-function will use this function to store temporary values in that memory, it is important to only allocate that memory once outside the function instead of allocating and freeing it in the function over and over again for performance reasons.
It is also space for 12 times 64 bytes (12 whole cachelines) double-precision floating point numbers was allocated, even though we only ever want to store 12 256-bit (32-byte) AVX2 vectors.
Dedicating a whole cacheline to each of the up to 12 threads improves performance, by avoiding false sharing. False sharing occurs when two or more threads try to access the same cacheline at the same time, this is not possible and could lead to the program effectively being serialized. \cite{doi:https://doi.org/10.1002/9781119695547.ch27}%(todo: see fig.??)

Another change affects the outermost loop starting in line 17.
Instead of \texttt{range} it now reads \texttt{prange(..., nogil=True)}
, this indicates to Cython that the resulting for-loop in c-code should be annotated with an \texttt{omp for} directive,
which means that this for-loop should be split across multiple threads (exact behavior can be controlled by various methods including environment variables).
The \texttt{nogil=True} argument means that the global interpreter lock should be released for this section of the code, this means that no interaction with the Python interpreter can happen inside this section and that the Python interpreter could switch to other threads in the meantime.

This style of programming is far-removed from the high-level way of programming Python owes its popularity to.
Additionally manually managing memory allocations and pointers is an error-prone process that can lead to programs which either produce false results or outright crash.
Automating the process of writing or relying on trusted, well-tested versions of these performance-critical low-levels parts of algorithms is therefore highly desirable.

\begin{listing}[H]
	\begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			linenos
			]{python}
@cython.boundscheck(False)
@cython.wraparound(False)
def matmul_transpose(numpy.ndarray[double, ndim=2] a, numpy.ndarray[double, ndim=2] b):

    result = np.zeros((a.shape[0], b.shape[1]), dtype=np.float64, order = 'C')

    cdef double[:, ::1] result_view = result
    cdef double[:, ::1] a_view = a
    cdef double[:, ::1] b_view = b.T.copy()

    cdef double* temp
    posix_memalign_wrapper(<void **> &inter, 64, 12*64)

    cdef int dim_x
    cdef int dim_y

    for dim_x in prange(result_view.shape[0], nogil=True):
        for dim_y in range(result_view.shape[1]):
            result_view[dim_x,dim_y] = dot_prod_simd(
                &a_view[dim_x,0],
                &b_view[dim_y,0],
                result_view.shape[0],
                &temp[64*cython.parallel.threadid()]
                )

    free(temp)
    return result
\end{minted}
	\caption{Matrix-Multiplication using Cython with the innermost loop replaced}
	\label[code]{refactor}
\end{listing}

The function \texttt{dot\_prod\_simd} meanwhile is defined in a separate file and normal C-Code, unaware of its use in a Python context. This code can be seen in Listing \ref{dotprod_c}.
Explanation of the code:

\begin{itemize}
	\item The instruction \texttt{\_mm256\_setzero\_pd()} at line 7-9 initializes a 256-bit vector of packed doubles.
	\item The instruction \texttt{\_mm256\_load\_pd(\&arr\_1[pos])} at line 13-14 loads 256-bit beginning from the specified memory address into a vector register.
	\item As mentioned earlier the instruction \texttt{\_mm256\_fmadd\_pd(a0, b0, c0)} at line 15 performs a fused multiply add with the specified vectors ($\mathbf{c} = \mathbf{a} \cdot \mathbf{b} + \mathbf{c}$). This operation accumulates the result of the dot-product in the four entries within the $\mathbf{c}$ vector.
	      \item{ The instruction \texttt{\_mm256\_hadd\_pd(a, b)} at line 21 performs a horizontal add:

	                  $c[0] = a[0] + a[1]$

	                  $c[1] = b[0] + b[1]$

	                  $c[2] = a[2] + a[3]$

	                  $c[3] = b[2] + b[3]$

	                  This enables a way to calculate the sum of $\mathbf{c}$ more efficiently.
	            }
	\item The instruction \texttt{\_mm256\_store\_pd(\&inter[0], c0)} at line 22 stores the contents of $\mathbf{c}$ in the specified memory location.
	\item Finally the loop ranging from lines 26 to 30 takes care of the remaining elements that were not processed with the SIMD-instructions (this happens if the vector length is not a multiple of 4).
\end{itemize}

The downside of using AVX2-intrinsics is that the resulting code is less portable than before, since AVX2 instructions are an extension of x86-64, they are not supported by legacy hardware or different architectures like ARM or RISC-V, those architectures have their own set of vector instruction and the code might be easily portable, but it is not an automatic process.
Trade-offs between code-portability and performance are a recurring theme in high-performance-computing.

\begin{listing}[H]
	\begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			linenos
			]{c}
double dot_prod_simd(const double *arr_1, const double *arr_2, unsigned int n, double *inter)
{
	unsigned int pos = 0;
	double *arr_1_ptr = arr_1;
	double *arr_2_ptr = arr_2;
			
	__m256d a0 = _mm256_setzero_pd();
	__m256d b0 = _mm256_setzero_pd();
	__m256d c0 = _mm256_setzero_pd();
				
	// equivalent for-statement: for(unsigned int i = 0; i < n/4;i++)
	while (pos + 4 <= 4 * (n / 4))
	{				
		a0 = _mm256_load_pd(&arr_1[pos]);
		b0 = _mm256_load_pd(&arr_2[pos]);
		c0 = _mm256_fmadd_pd(a0, b0, c0);
								
		pos += 4;
	}
			
	c0 = _mm256_hadd_pd(c0, c0);	
	_mm256_store_pd(&inter[0], c0);
			
	double result = inter[0] + inter[2];
			
	while (pos < n)
	{
		result += arr_1[pos] * arr_2[pos];
		pos++;
	}
			
	return result;
};
\end{minted}
	\caption{C-function for calculating the dot-product between vectors using SIMD-Instructions, abridged for readability}
	\label[code]{dotprod_c}
\end{listing}

Tables \ref{speed_up_simd_100} and \ref{speed_up_simd_1000} show the speed-up of using multiple threads (controlled by the environment variable \texttt{OMP\_NUM\_THREADS})
and unrolling the loop containing the AVX2-instructions.
For this embarrassingly parallel problem performance scales very well with the number of threads, but manually unrolling the for-loop is also necessary to achieve maximum performance,
apparently the compiler is unable to correctly unroll the loop (independently of whether we express the loop using a \texttt{while} or \texttt{for} statement) on its own.

We can recognize that large matrices benefit more from the parallelization and vectorization.
Using all 12 threads and unrolling the loop speeds the calculation up by a factor of 5.7 for small matrices of size $100 \times 100$, while we can achieve a speedup of up to 13 for larger matrices like $1000 \times 1000$.

\begin{minipage}{.45\textwidth}
	\begin{table}[H]
		\centering
		Matrix sizes: $100 \times 100$ \\
		\input{generated_docs/slowest_100.tex}
		\caption{Relative speed-ups when using multi-threading and loop-unrolling}
		\label[table]{speed_up_simd_100}
	\end{table}
\end{minipage}\hfill%
\begin{minipage}{.45\textwidth}
	\begin{table}[H]
		\centering
		Matrix sizes: $1000 \times 1000$
		\input{generated_docs/slowest_1000.tex}
		\caption{Relative speed-ups when using multi-threading and loop-unrolling}
		\label[table]{speed_up_simd_1000}
	\end{table}
\end{minipage}%

Having seen that utilizing multithreading and vectorization scales fairly well, we can compare our matrix-matrix-multiplication functions performance with that of Numpy's matmul.
The results of that comparison can be seen in Figure \ref{Cython-Comparison}.
We can see that the version where we replaced the innermost loop with a C-function featuring AVX2-intrinsics is a lot faster than the most naive version and even slightly faster than BLAS for small matrices.
It cannot compete with BLAS when tested with larger matrices because there are more sophisticated algorithms involved, namely blocked matrix-matrix multiplications, which optimize for fewer cache-misses.
Those algorithms still have a time complexity of $\mathcal{O}(n^3)$, but make better use of the hierarchical nature of the memory-caches.
Theoretically better algorithms like the Strassen-Algorithm are traditionally not found in BLAS implementations, because they are only superior for very large matrices. \cite{https://doi.org/10.48550/arxiv.1605.01078} 
A better asymptotic complexity does not necessarily translate to a better performance for all problem sizes, after all constant factors and terms of lower order are usually neglected when stating complexities (i.e. $O(2 n^2 + 4 n)$ simplifies to $O(n^2)$). 
The compiler failed to automatically vectorize the version written purely in Cython (same as in listing \ref{locked}, except the outermost \texttt{range} was replaced with a \texttt{prange} to distribute the work among the available CPU-cores).
Often flags like \texttt{-ffastmath} have to be passed to the compiler to indicate that the compiler is
allowed to reorder operations on floating-point numbers, due to floating point number addition not being associative, which affects precision but is sometimes needed for vectorization.
It does however not enable the compiler to vectorize the loop in this case.

\paragraph{Numba}

Numba takes a different approach to accelerating Python code.
Unlike Cython it does not require a compilation step ahead of execution that has to be manually invoked,
instead it only generates the required IR-Code and binaries once the function is called (JIT-compilation).
This allows Numba to know the types of the passed arguments before compilation without the user explicitly defining them beforehand.
The downside of this approach is that the first invocation of the function is slowed down due to the necessary compilation.

Numba works by decorating existing Python function.
Decorators in Python are higher order functions, they take functions, transform them in some way and return the transformed functions.
In case of Numba the decorator ingests the Python function and analyzes its bytecode (using Python's \texttt{dis} package as discussed in section \ref{dis}) and tries to compile it to LLVM-IR and subsequently to native binary code.

\begin{listing}[H]
	\begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			linenos
			]{python}
@numba.jit(nopython=True)
def matmul_numba(a,b):
  result = np.zeros((a.shape[0], b.shape[1]), dtype=a.dtype)
  for x_dim in numba.prange(result.shape[0]):
	for y_dim in range(result.shape[1]):
	  for k in range(result.shape[1]):
		result[x_dim, y_dim] += a[x_dim,k]*b[k,y_dim]
			
  return result
\end{minted}
	\caption{Matrix-Matrix multiplication example using Numba}
	\label[code]{dotprod_c}
\end{listing}

Numba offers to dump the generated intermediate representations and assembly code for user inspection.
But interpreting these outputs is not always practical since they are not meant to be human read-able and require some time and effort to reason about and
therefore Numba can, at times, appear to be a black box, with the generated code's performance being hard to predict given only the decorated function.
Users can only tweak the decorated function, benchmark the resulting native code and observe which changes improve performance and which ones do not,
unlike with Cython where it is possible to rewrite critical sections directly in C.


\begin{figure}[H]
	\includegraphics{figures/Cython_Numba_Comparison.pdf}
	\caption{Result of comparing the different matrix-matrix-multiplication implementations}
	\label[fig]{Cython-Comparison}
\end{figure}

Figure \ref{Cython-Comparison} shows that the Numba functions seem to have a significant startup-overhead
associated with them, which prevent them to be competitive when dealing with small matrices.
We can rule out that this overhead is the time it takes Numba to jit-compile the function.
The times used to create the plot were the lowest runtimes measured several runs.
The jit-compile penalty however is only present the first time a Numba function is called with a new function signature and is about 0.6 seconds
(effectively another order of magnitude larger than the fastest runtimes recorded for Numba functions) for this function.
The figure does not include the pure Python implementation, as introduced in section \ref{pure-python-dgemm}, since presentating that version as a baseline would be disingenuous and misleading.
That version is never used in practice and would be slower by a factor of approximately 400.


For larger matrices Numba only manages to compete with the most naive Cython implementation and is about an order of magnitude slower than the Cython
version using AVX-intrinsics, even though the LLVM-compiler backend Numba is using would be free to also use vector instructions, it just fails to do so to the same extent.

\paragraph{Pybind11}
\label{pybind11-cublas}

Similarly to Cython, Pybind11 can be used to interface with arbirary C/C++-Code.
Listing \ref{pybind-cblas} for example shows how BLAS functions like \texttt{dgemm} (Double Precision General Matrix Multiplication) can be wrapped.
The function takes two Numpy arrays \texttt{arr1} and \texttt{arr2}, requests information about their shapes
and memory locations and passes that information to the \texttt{cblas\_dgemm} function.

The API CBLAS offers is very versatile, but a lot less comfortable to use than one would expect coming from high-level languages like Python.
This is in part due to the fact that arrays in C are just pointers to the beginning of the memory location storing that array.
A function that is passed one of such pointers has no way of knowing the size of the allocated memory block or the shape of the matrix that block of memory could represent.
We therefore have to tell \texttt{cblas\_dgemm} whether the used matrices are row major aligned or column major aligned in Memory and how many rows and columns they consist of,
otherwise the BLAS library would not know how to interpret the pointer it got passed.
The performance of this approach is comparable to that of Numpy as discussed in section \ref{blas-numpy}.

Just as with Cython we have to take care of manually allocating and de-allocating memory on the heap with \texttt{malloc} and \texttt{free} or \texttt{new} and \texttt{delete} in the C/C++-Code.
In this example this is done by the so-called capsule defined on lines 23 to 25.
The code defined in that capsule is executed when the associated Python object is garbage collected.
This is necessary to avoid memory-leaks.
Pybind11's documentation on how to best return newly created Numpy arrays is unfortunately lacking and most information regarding this topic can only be found in a GitHub issue. \cite{NotclearPybind}
This seems to be solved better in Cython where new Numpy arrays can simply be instantiated by calling \texttt{np.zeros} or similar functions, this however comes at a higher overhead than \texttt{malloc} and requires the GIL to be held.

\begin{listing}[H]
	\begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			linenos
			]{c}
py::array_t<double> dgemm_blis(pybind11::array_t<double> arr1, pybind11::array_t<double> arr2)
{
  pybind11::buffer_info a = arr1.request();
  pybind11::buffer_info b = arr2.request();

  double *C = new double[a.shape[0] * b.shape[1]];

  cblas_dgemm(CblasRowMajor,
              CblasNoTrans,
              CblasNoTrans,
              a.shape[0],
              b.shape[1],
              a.shape[1],
              1.0,
              (double *)(a.ptr),
              a.shape[1],
              (double *)(b.ptr),
              b.shape[1],
              0.0,
              C,
              b.shape[1]);

  py::capsule free_when_done(C, [](void *f){
            double *C = reinterpret_cast<double *>(f);
            delete[] C; });

  return py::array_t<double>(
      {a.shape[0], b.shape[1]},
      (const double *)C,
      free_when_done);
}
\end{minted}
	\caption{C++-function for calculating the dot-product between vectors using SIMD-Instructions, abridged for readability}
	\label[code]{pybind-cblas}
\end{listing}

Since Pybind11 (and Cython for that matter) can utilize arbitrary C/C++ Code, it can also be used to expose special hardware features to Python.
To continue this example of matrix-matrix-multiplication one can also use Nvidia's cuBLAS to offload the calculation to a connected CUDA capable GPU.
This is demonstrating in the following commented code sections:



\begin{minted}
		[
			frame=topline,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			linenos
			]{c}
template <typename T>
py::array_t<T> gemm_cublas(pybind11::array_t<T> arr1, pybind11::array_t<T> arr2)
{
  cublasHandle_t handle;
  cublasCreate(&handle);

  pybind11::buffer_info a = arr1.request();
  pybind11::buffer_info b = arr2.request();

  T *devPtrA;
  T *devPtrB;
  T *devPtrC;

  cudaMalloc(&devPtrA, a.shape[0] * a.shape[1] * sizeof(T));
  cudaMalloc(&devPtrB, b.shape[0] * b.shape[1] * sizeof(T));
  cudaMalloc(&devPtrC, a.shape[0] * b.shape[1] * sizeof(T));

  cudaMemcpy(devPtrA, a.ptr, a.shape[1]*a.shape[0]*sizeof(T), cudaMemcpyHostToDevice);
  cudaMemcpy(devPtrB, b.ptr, b.shape[1]*b.shape[0]*sizeof(T), cudaMemcpyHostToDevice);

\end{minted}

\begin{itemize}
	\item On Lines 13-15 memory for the three arrays is allocated on the device memory.
	\item On Lines 17 and 18 the memory contents of \texttt{arr\_1} and \texttt{arr\_2} are copied to device memory.
\end{itemize}

\begin{minted}
		[
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			firstnumber=last,
			linenos
			]{c}

  const double alpha = 1.0;
  const double beta = 0.0;

  gemm_cuda(handle,
              CUBLAS_OP_N,
              CUBLAS_OP_N,
              b.shape[1],
              a.shape[0],
              a.shape[1],
              &alpha,
              devPtrB,
              b.shape[1],
              devPtrA,
              a.shape[1],
              &beta,
              devPtrC,
              b.shape[1]);
  
  cudaFree(devPtrA);
  cudaFree(devPtrB);

	\end{minted}
\begin{itemize}
	\item {On Line 22 cuBLAS' version of DGEMM is called and unlike BLIS' DGEMM it always assumes arrays to be laid out in column major order in memory. Since we copied row-major-ordered matrices into the memory, but cuBLAS interprets them as column-major they are effectively transposed.

	      We would like to calculate:

	      $$
		      \mathbf{A} \mathbf{B} = \mathbf{C}
	      $$

	      but we only have $\mathbf{A}^T$ and $\mathbf{B}^T$ in column-major-ordered memory, instead of inefficiently re-ordering the matrix entries to get $\mathbf{A}$ and $\mathbf{B}$ in column-major-order, we can also calculate:

	      $$
		      \mathbf{B}^T \mathbf{A}^T = \left( \mathbf{A} \mathbf{B}\right)^T = \mathbf{C}^T
	      $$

	      The resulting $\mathbf{C}^T$ is also in column-major-order and therefore exactly the matrix $\mathbf{C}$ we wanted to calculate in row-major-order.
	      It can be seen that the fact that the cuBLAS interface expects column-major-ordered matrices adds another level of complexity and additional opportunities for possible errors to occur.
	      }
	\item After calling DGEMM we don't need the matrices $\mathbf{A}$ and $\mathbf{B}$ on the GPU memory anymore and can free the memory using a \texttt{cudaFree} call.

\end{itemize}
\begin{listing}[H]
	\begin{minted}
			[
				frame=bottomline,
				framesep=2mm,
				baselinestretch=1.2,
				bgcolor=LightGray,
				fontsize=\footnotesize,
				breaklines=true,
				firstnumber=last,
				linenos
				]{c}

  T *C = new T[a.shape[0] * b.shape[1]];
  
  cudaMemcpy(devPtrC, C, b.shape[1]*a.shape[0]*sizeof(T), cudaMemcpyDeviceToHost);
  cudaFree(devPtrC);

  py::capsule free_when_done(C, [](void *f){
            double *T = reinterpret_cast<T *>(f);
            delete[] C; });
  
  return py::array_t<double>(
      {a.shape[0], b.shape[1]},
      (const T *)C,
      free_when_done);
\end{minted}
	\caption{C++-function wrapping cuBLAS' DGEMM}
	\label[code]{pybind-cublas}
\end{listing}

\begin{itemize}
	\item On Line 39  host memory is allocated for storing the result of the calculation.
	\item On Line 41 transfer the memory is transferred from the Device to the Host Memory.
	\item After copying the results from device memory to host memory, the memory containing $\mathbf{C}$ on the device can be freed as done on line 42.
\end{itemize}

Notice how the C++ templating system can be used to express the function in a type-agnostic way with the type \texttt{T} being used as a placeholder type. The function \texttt{gemm\_cuda} (called in line 23, defined as shown in listing \ref{cublas-wrapper}), which is overloaded to handle both single and double precision floating point numbers, then goes on to invoke the correct cublas-kernels.
Employing the templating functionality in this fashion minimizes code-duplication.

The two resulting versions of the function can then be registered as functions of the Python module that will be compiled using pybind11 as shown in listing \ref{pybind-def}. The exposed functions can then be imported in a Python script and offer the functionality to multiply two matrices similar to Numpy's matmul (albeit more basic since Numpy's version offers sophisticated options like treating matrices of orders higher than two as a stack of two-dimensional matrices) but using GPU-acceleration.

Figure \ref{cuBLAS-Comparison} shows the result of comparing the performance of the pyBind11 wrapped cuBLAS implementation against Numpy's matmul function.
The comparison was done using matrices of sizes ranging from 100x100 to 3000x3000.
The times for cuBLAS include the time for copying the matrices from host memory to device memory and back (the function is used as defined in listing \ref{pybind-cublas}).
Despite that overhead the cuBLAS implementation is significantly faster than Numpy's matmul function for single precision inputs and about as fast for double precision inputs.
This example introduces the GPU as a suitable device for offloading computationally expensive tasks.
In the following chapters we will examine how this works in detail, how the GPU can be used to execute arbitrary code, why the single precision calculation was faster and how algorithms need to adapted for good performance on this specialized hardware.


\begin{figure}[H]
	\includegraphics{figures/numpy_cublas_comparison.pdf}
	\caption{Result of comparing Numpy against the pyBind11 wrapped cuBLAS implementation}
	\label[fig]{cuBLAS-Comparison}
\end{figure}

\begin{listing}[H]
	\begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			linenos
			]{c}
PYBIND11_MODULE(gpu_library, m)
{
  m.def("gemm_cublas", &gemm_cublas<double>, py::arg("arr1").noconvert(), py::arg("arr2").noconvert());
  m.def("gemm_cublas", &gemm_cublas<float>, py::arg("arr1").noconvert(), py::arg("arr2").noconvert());
}
\end{minted}
	\caption{Registration of the resulting Python extension's member function}
	\label[code]{pybind-def}
\end{listing}

\begin{listing}[H]
	\begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			linenos
			]{c}
/* ---
wrapper for DGEMM (double-precision general matrix-matrix multiplication)
--- */
void inline gemm_cuda(cublasHandle_t handle, 
          cublasOperation_t transa,
          cublasOperation_t transb, 
          int m, int n, int k,
          const double *alpha, 
          const double *A, int lda,
          const double *B, int ldb, 
          const double *beta, 
          double *C, int ldc)
{

  cublasDgemm_v2(handle, transa, transb, m, n, k, alpha, A, lda, B, ldb, beta, C, ldc);
          
}

/* ---
wrapper for SGEMM (single-precision general matrix-matrix multiplication)
--- */
void inline gemm_cuda(cublasHandle_t handle, 
          cublasOperation_t transa,
          cublasOperation_t transb, 
          int m, int n, int k,
          const float *alpha, 
          const float *A, int lda,
          const float *B, int ldb, 
          const float *beta,
          float *C, int ldc)
{

  	cublasSgemm_v2(handle, transa, transb, m, n, k, alpha, A, lda, B, ldb, beta, C, ldc);

}
\end{minted}
	\caption{Overloaded wrappers for \texttt{dgemm} and \texttt{sgemm}}
	\label[code]{cublas-wrapper}
\end{listing}

\section{GPGPU}
\label{GPGPU}

After having outlined the limitations of Python and methods to overcome those by calling native compiled code this chapter focuses on the basics of utilizing the GPU's computing capabilities for general computations.
The previous example (listings \ref{pybind-cublas} to \ref{cublas-wrapper}) introduced this concept for matrix multiplication, which is a common operation in scientific computing.
This chapter follows this approach further and provides an overview on how GPGPU (General-purpose computing on graphics processing units) works in principle,
how algorithms can be adapted to be processed efficiently on these specialized many-core devices instead of sequentially on a CPU.
Chapter \ref{cuda_python} will then go on to discuss how to utilize the GPU's capabilities in Python, relating back to the first chapter.

\subsection{Hardware structure of a GPU}
\label{GPU-structure}

\begin{figure}[H]
	\includegraphics[width = \textwidth]{figures/GA104.png}
	\caption{Block diagram of the GA104 chip used in the RTX 3070 (as published in Nvidia's whitepaper on the ampere microarchitecture \cite{NVIDIAam19:online})}
	\label[fig]{block-GA104}
\end{figure}

GPUs, which were traditionally used exclusively in rendering graphics to screens, have evolved to feature an ever-increasing number
of (in comparison to x86-architecture cores rather simple) processing cores (Nvidia documentation refers to those cores as SMs (streaming multiprocessors),
other vendors use deviating terminology) and memory.

Figure \ref{block-GA104} schematically displays the components of a modern GPU.
It is connected to the host (typical term for the Computer which controls the GPU, a host can manage several GPUs) via a PCI Express interface and to it's own memory
(usually ranging in size from a few to a few dozens of gigabytes distributed over several ram-chips, which are placed in close proximity to the main chip of the GPU)  via several memory controllers.
The chip further includes 6 GPCs (GPU processing clusters), which in turn each contain TPCs (Texture processing clusters), finally these TPCs contain the SMs (their structure is outlined in figure \ref{block-SM}) and on-chip storage (like the L2-cache and lower levels of cache within the SMs).

The hierarchical nature of this design is of great importance when using these devices.
The device can only communicate with the host over the relatively slow PCI Express interface. Access to its own off-chip memory (typically referred to as global memory, because it is shared by the whole chip) is faster by orders of magnitude,
the on-chip memory is faster by another few orders of magnitude but very limited in size.
Efficient programs have to take these differences in bandwidth and access latency into account.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.5\textwidth]{figures/GA10x_SM.png}
	\caption{Block diagram of the SMs within the GA104 chip used in the RTX 3070 (as published in Nvidia's whitepaper on the ampere microarchitecture \cite{NVIDIAam19:online}))}
	\label[fig]{block-SM}
\end{figure}

Each SM (as depicted in figure \ref{block-SM}) is divided into 4 so-called partitions, a ray-tracing core (not relevant for computing purposes), 128 kilobytes of shared memory and 2 FP64 FPUs, which are omitted from the diagram supplied by Nvidia.
These partitions each have two datapaths, one that can execute 16 FP32 instructions per clock and one that can execute either 16 FP32 or 16 INT32 instructions per clock.
To summarize each SM can execute up to 128 FP32 or 2 FP64 floating point operations per clock-cycle.

With the chip having a maximum clock rate of 1725 Mhz one can calculate it's maximum compute performance:

\begin{align*}
	\text{FLOPS}_{\text{max,FP64}} & = 1725 \text{ MHz} \cdot 46 \text{ SM}_{\text{count}} \cdot 2  \frac{\text{FLOP}}{\text{cycle}} \cdot 2 = 317 \times 10^{9}  \frac{\text{FLOP}}{s}     \\
	\text{FLOPS}_{\text{max,FP32}} & = 1725 \text{ MHz} \cdot 46 \text{ SM}_{\text{count}} \cdot 128  \frac{\text{FLOP}}{\text{cycle}} \cdot 2 = 20.3 \times 10^{12}  \frac{\text{FLOP}}{s}
\end{align*}

\label{flop-calculation}
Note the calculations must include the factor 2 to account for the GPU's ability to perform a fused-multiply-add (i.e. two floating point operations) in a single instruction and cycle.

This $64:1$ disparity between single and double-precision performance might seem like a large limitation in the usefulness of consumer-grade GPUs for scientific workloads, but it's still an appealing proposition, even for workloads requiring double-precision, since many algorithms are bandwidth limited (see section \ref{roofline-section} for an explanation of the roofline-performance model).
Alternatively there are a number of techniques to effectively "emulate" extended precision by using multiple lower precision floating point numbers to represent a single number. Those techniques include Kahan summation, Knuth summation and others as discussed by Thall \cite{10.1145/1179622.1179682}.

\subsection{The CUDA Programming Language and Model}

Traditionally GPUs were only used for rendering graphics to screens, but in the last decade they have become increasingly popular for general-purpose computing.
These graphics related tasks consist of a number of operations that are performed on a large number of vertices and pixels and are therefore easily parallelizable.
This drove the development of modern GPUs, which are now capable of performing many operations in parallel.

These operations are defined by shaders, which are programmed using so-called shader languages (various competing standards like the High-level shader language (HLSL) or the OpenGL Shading Language (GLSL), but they all have similar C-like syntax).
Such a (vertex-) shader for example displace the vertices of a 3D-model depending on its position and time, which could be used to mimic waves in water by displacing the vertices as governed by a sine-function.
The website shadertoy.com \cite{Shaderto53:online} showcases the possibilities of such shaders and their (often remarkably concise) source-code, which can be edited and re-compiled immediately to see the effects of modifications.

Around 2005 researchers started to employ GPUs for non-graphics uses like performing simulations and linear-algebra.
Initially programs were written in shader languages and extensive knowledge about computer graphics was needed as laid out in a chapter on how to map problems onto GPU by M. Harris \cite{Chapter323:online}.

In 2007 Nvidia introduced CUDA \cite{CUDA1_0:online} (then an abbreviation for computational unified device architecture, now often just the umbrella term for Nvidia's GPGPU offerings).
CUDA encompasses a wide suite of tools like profilers, debuggers, compilers, but it is also the common name for the programming languages used to write programs which utilize the GPU in this CUDA framework. Note that CUDA is not the only possible way to achieve this.

There exist official bindings for C/C++, Fortran and Python (note that the official Python bindings do not allow users to express computational kernels in Python code as of this writing, but there are other efforts trying to achieve that; see section \ref{python-packages} for a detailed discussion). The C/C++ implementation can be considered the reference implementation.

\subsubsection{Competing Programming Languages and Vendors}

There are currently three major GPU vendors: Nvidia, AMD and Intel (with Intel only recently trying to establish itself in the higher-performance segments relevant for scientific GPGPU), with Nvidia being the dominant contender in this industry with a market share amounting to 81\% of shipped GPUs in Q4 of 2021 \cite{Q4’21see94:online}.
AMD meanwhile also has attractive GPU offerings for scientific computing.
the newly built supercomputer Frontier at Oak Ridge National Laboratory, for example, which claims the top spot among supercomputers worldwide \cite{ORNL’sFr28:online}, is equipped with purpose built AMD Instinct GPUs (similar to the comercially available AMD Instinct 250X accelerators) per Node \cite{Frontier75:online}.

The mentioned hardware manufacturers offer different languages and software stacks to facilitate programming their hardware, with code not necessarily portable between hardware from different manufacturers.
Nvidias proprietary CUDA language for example only supports Nvidia hardware, whereas competing offerings like OpenCL (Open Computing Language) by a consortium of Vendors or, the much later introduced, ROCm (Radeon Open Compute platforM) HIP (Heterogeneous Interface for Portability) by AMD aim to support both AMD- and Nvidia-GPUs and in the case of OpenCL even non-GPU accelerators like FPGAs, DSPs or traditional CPUs (where OpenMP as backend).

A short, incomplete overview of the different languages and their respective ecosystems:

\begin{itemize}
	\item CUDA - the ecosystem (consisting of the language, compilers, profilers and debuggers) created by Nvidia
	\item OpenCL - a more portable C-like language supporting many devices
	\item ROCm HIP - a recent addition to the landscape of GPGPU-languages, very similar to CUDA using the same terminology aiming to ease the transition for developers
	\item SYCL - a higher level C++ abstraction, initially on top of OpenCL, recent versions can also target HIP (hipSYCL)
\end{itemize}

CUDA has several advantages over its competitors like a comprehensive manual, sophisticated tools for profiling and debugging, support for Linux and Windows (AMDs ROCm only supports Linux at the time of writing) and a level of maturity not yet reached by ROCm \cite{otterness_et_al:LIPIcs:2020:12373}.
This thesis therefore focuses on the use of CUDA in conjunction with Nvidia Hardware.
Regardless of this, the introduced concepts and findings also apply to competing hardware programmed with different languages.
AMD even offers a set of tools called hipify \cite{hipify:online}, which claims to be able to automatically translate programs written using CUDA to HIP C++ (and enable those programs to work on AMD hardware by doing so).

A more comprehensive overview of the compatibility relationship between accelerated computing platforms, languages and hardware of the three major vendors is offered as a table by Herten \cite{Herten2022-zs}. 

\subsubsection{Example CUDA kernel}

First we need to introduce essential CUDA terminology.

\begin{itemize}
	\item \texttt{grid} - A (up to) three-dimensional grid of blocks
	\item \texttt{block} - A (up to) three-dimensional block of threads, a block is the largest unit that is guaranteed to be worked on in parallel. Multiple blocks can be scheduled to be executed at the same time, but it can also be limited to a single block at a time. Inter-block synchronization is therefore not supported. A block can at most consist of 1024 threads.
	\item \texttt{warp} - A group of 32 threads that execute in lock-step on a SM. The threads within a warp can either perform an action or idle (if other threads within the warp exited earlier or took a different branch, this is called warp-divergence and is to be avoided for best performance).
	\item \texttt{thread} - Each thread is located in a warp, within a block, within a grid.
\end{itemize}

All threads execute the same instructions, but they do not necessarily operate on the same data (the SIMD paradigm of programming).
To achieve this threads need to have a way of inferring their position within their warp, block or within the whole grid.
For this the following variables are always within scope in code that is executed on the device:

\begin{itemize}
	\item \texttt{gridDim} - a tuple of 3 ints specifying the dimensions of the grid; the contents of this variable is the same for all threads
	\item \texttt{blockDim} - a tuple of 3 ints specifiyng the dimensions of the blocks; the contents of this variable is the same for all threads, all blocks within a grid have the same dimensions
	\item \texttt{warpsize} - the size of a warp, 32 for all CUDA-capable GPUs, might however change in the future and is therefore queryable at runtime
	\item \texttt{blockIdx} - a tuple of 3 ints, which identifies the block the executing thread is in, values ranging from $0$ to $\text{gridDim}.x|y|z$-1
	\item \texttt{threadIdx} - a tuple of 3 ints, which identifies the executing thread within its block, values ranging from $0$ to $\text{blockDim}.x|y|z$-1
\end{itemize}

These variables can then be used by the threads to identify their share of work to be done at run-time.
If, for example, one wants to program a kernel which increments every element of a vector by a given value that kernel could be implemented as seen in listing \ref{example_kernel}.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{C++}
__global__ void increment(float *input, int size, float x)
{
	// tid uniquely identifies 
	int tid = blockDim.x * blockIdx.x + threadIdx.x;
	
	// do nothing if thread-id is outside of valid range
	// accessing memory that is out of bounds can lead to segmentation faults
	if(tid < size){
		input[tid] += x;
	}
}
\end{minted}
\captionof{listing}{example of a kernel which increments the values of an array \label[code]{example_kernel}}

\subsection{Occupancy}

Whenever a kernel is launched the blocks within a grid are distributed and scheduled across the streaming multiprocessors of the GPU.
To distribute this in an optimal way (each SM of Compute Capability 8.6 can at most have 1536 resident threads at a time), the blocks have to be shaped in a certain way.
If, for example, a block consists of 1024 threads one can at most fit a single of those block on an SM at a time and reach a maximum occupancy of $66.67\%$.
On the GPU we used throughout this thesis, one can therefore fit 70656 threads at a time (1536 threads per SM across 46 SMs).
As a consequence of this the GPU is only used to its potential when a workload consisting of hundreds of thousands of threads is launched.
Small workloads are not well suited for GPGPU.

The number of threads a SM can fit at a time is however not the only constraint.
There are also limitations regarding the shared memory per SM (the memory region indicated in figure \ref{block-SM} is limited to a certain size) and the use of registers per SM, as well as the maximum use of registers per thread.
The number of registers per thread or block is often referred to as register-pressure.

Higher occupancy is not necessarily always better, because there are often trade-offs involved like storing fewer data in sharing memory, which in turn might require more slow reads from global memory.
If it's possible to reach a high degree of occupancy without sacrificing other aspects of performance it is an important step in optimizing performance, especially the performance of compute-bound kernels, which benefit the most from a greater number of active threads.

Note: Not all blocks resident on a SM have to belong to the same grid or kernel. It's possible to launch one kernel with a blocksize of 1024 and another kernel with a blocksize of 512 and achieve perfect occupancy (given the need of other limited resources is also within the constraints).

\subsection{Contextualizing Amdahl's Law}
\label{amdahl section}

Amdahl's famous law \cite{10.1145/1465482.1465560}  puts a limit to the benefit of distributing tasks over a number $n$ of processors with $r_s$ being the fraction of the program that has to be executed in a strictly serial fashion and can't be parallelized and $r_p$ being the fraction of the program that benefits from parallelization; in this model $r_s$ and $r_p$ add up to one.

$$
	Speedup = \frac{1}{r_s + \frac{r_p}{n}}
$$

This model shows that the ratio of $\frac{r_p}{r_s}$ has to be rather high for a meaningful speedup to be had.
In all cases the speedup is limited by $\frac{1}{r_s}$.
This model is simplified in the sense that it does not take into account the overhead of distributing the work to the processors or the overhead of communication between the processors.
In practice the costs associated with these overheads can be significant and can even cause a slowdown as the problem is distributed among an excessive number of workers.

When offloading work to a specialized accelerator with a dedicated memory such as a GPU the parallelizable part of the problem can usually be solved very efficiently and quickly, but this comes at the cost of transferring the data to and from the accelerator.
This memory copy operations can be interpreted as being part of the non-parallelizable parts of the program $r_s$ which limit the speed-up.

Profiling tools like Nvidia's Nsight Compute and Nsight Systems \cite{NVIDIADe48:online} allow developers to profile and analyze code as it is executed on GPUs.

If we for example profile the code laid out in listing \ref{pybind-cublas}, we can examine precisely how long the \texttt{memcpy} operations and the kernel execution takes:

\begin{figure}[H]
	\includegraphics[width = \textwidth]{figures/nsight.drawio.pdf}
	\caption{Timeline resulting from profiling the cuBLAS sgemm-kernel with Nsight Systems (the time axis in this illustration does not use a consistent scale)}
	\label[fig]{Nsight-Sgemm4000}
\end{figure}

Figure \ref{Nsight-Sgemm4000} depicts the timeline of the execution of a cuBLAS sgemm-kernel for inputs of size $4000 \times 4000$. In a first step both input matrices are copied to the device memory, this is abbreviated with HtoD (Host to Device). Since the matrices are of size $4000 \times 4000$ and each element being 32 bits large (single precision) the total size of each of the three involved matrices in bytes is $4000 \cdot 4000 \cdot 4 \text{ byte} = 64 \text{ MB}$.
Then the actual computation is performed and finally the result is retrieved from the device memory in a DtoH (Device to Host) Memcopy operation.
In the context of Amdahl's Law we recognize that the runtime of our execution is, in this case, not only determined by the number of FLOPS (floating point operations per second) our GPU can perform i.e. how fast the kernel actually executes but by how long the data transfers to and from the GPU take.
In this example we spend roughly 27 milliseconds waiting for memory contents to be copied and only about 15 milliseconds waiting for calculations to finish.

For optimal performance it is of utmost importance to keep the GPU's processors working and not starved for data to process. There are several ways to achieve this:

\begin{itemize}
	\item Aiming for a high degree of arithmetic intensity (ratio of compute to I/0, measured in floating point operations per transferred byte); Transfer speeds to and from the GPU memory make it prohibitively expensive to perform a simple elementwise operation like squaring the elements of a vector on the GPU. The cost of the memory copy operation has to be outweighed by accelerating a large enough computation on the data.
	\item Pipelining and overlapping tasks; multiple kernels can execute simultaneously and more importantly copy operations can take place while computations are executed (as discussed in the section on CUDA Streams \ref{stream_section}).
	\item Maximizing copy throughput. When judging copy speeds it is necessary to be aware of the theoretical maximum performance achievable on the used hardware. In the example depicted in figure \ref{Nsight-Sgemm4000} a Nvidia RTX 3070 GPU was used.
	This GPU has a maximum memory data-rate of 14 GBps \cite{NVIDIAam19:online}, this speed is almost reached in the HtoD-transfers but should also be achieved DtoH-transfers. One would expect the transfer rates to be equal in both direction, therefore the transfer of the resulting matrix $\mathbf{C}$ to the host memory should be about three-times as fast. This is a potential performance increase worth look into (CUDA offers the functionality to allocate so-called pinned memory on the host which is never swapped from memory for better performance)
\end{itemize}

\subsection{CUDA Streams}
\label{stream_section}

CUDA Streams can be used to perform multiple tasks at the same time on a single GPU.
Tasks within a single stream are always serialized (e.g. performed on after another, as they were scheduled), but tasks issued to different streams can be executed at the same time if resources allow (occupancy limits or memory bandwidth).

If one for example wants to perform more than a single matrix multiplication (unlike the example discussed in the previous section \ref{amdahl section}), one could schedule the individual, independent operations in different streams.
This approach was tested as displayed in listing \ref{} and the resulting behavior profiled with Nvidia Nsight Systems.
The resulting timeline when using a single stream is displayed in figure \ref{1_stream}.
This timeline looks exactly as expected and very similar to the timeline presented in figure \ref{Nsight-Sgemm4000}: every kernel-execution is preceded by two HtoD-Memcopy operations to copy the matrices, which are to be multiplied, into device memory (drawn in teal) and sucedeed by a single DtoH-Memcopy operation to retrieve the result from the device (drawn in violet).
There are large sections where the GPU is unable to perform computations because the data needed is not on the device yet, this situation is often referred to as GPU \emph{starvation} and has to be avoided for maximum efficiency.


\begin{figure}[H]
	\includegraphics[width = \textwidth]{../stream/1_stream.png}
	\caption{Timeline resulting from profiling a series of dgemm-kernel calls scheduled within a single stream}
	\label[fig]{1_stream}
\end{figure}

The timeline shown in figure \ref{2_stream} shows the behavior when the operations are distributed over multiple streams (two in this case).
The operations within a stream are unchanged, they still consist of a series of kernel calls, with each kernel being preceded by two HtoD-operations and followed by a single DtoH-operation, but the two streams are now operating at the same time.
The GPU streaming processors always have tasks to execute (no GPU starvation) and the copy engines can copy data to and from the host over the otherwise unused PCI-Express link in the meantime.
This and related techniques are referred to as latency hiding, because the cost of moving the data to and from the GPU is now hidden and effectively gone.


\begin{figure}[H]
	\includegraphics[width = \textwidth]{../stream/2_stream.png}
	\caption{Timeline resulting from profiling a series of dgemm-kernel calls distributed over 2 streams}
	\label[fig]{2_stream}
\end{figure}

When using more than two streams, even DtoH- and HtoD-memcpy operations can be performed at the same time as displayed in figure \ref{3_stream}.

\begin{figure}[H]
	\includegraphics[width = \textwidth]{../stream/3_stream.png}
	\caption{Timeline resulting from profiling a series of dgemm-kernel calls distributed over three streams}
	\label[fig]{3_stream}
\end{figure}

Of course the effectiveness of this technique heavily depends on the workload.
It works especially well in cases where the duration of memcopy-operations and the duration of the actual kernel-execution is about equal.
It works a lot less in cases where either the time taken by the kernel-execution (those cases are not problematic since they already benefit from the great performance of the GPU) or by the memory copy operations dominate.

\subsubsection{Example containing streams}

Listing \ref{cupy_stream} showcases the program that was profiled in figures \ref{1_stream}-\ref{3_stream}.
Unlike the previous CUDA examples like listing \ref{example_kernel}, this example was written in Python and makes use of CUDA by importing the CuPy-package \cite{cupy_learningsys2017}, which will be discussed in detail along similar alternatives in section \ref{cuda_python}.
For now, it is sufficient to call CuPy a drop-in replacement for Numpy, which executes code on the GPU.
The syntax and naming is very similar: \texttt{cp.matmul} is a function which multiplies two matrices which reside within the GPUs memory, just like \texttt{np.matmul} multiplies two matrices, which reside in the regular host-memory.

The script itself is also not unlike a regular Numpy script:
\begin{itemize}
	\item On line 1 the CuPy package is imported.
	\item On line 17 a number of CUDA-streams are created.
	\item On lines 20-23 random matrices \texttt{a} and \texttt{b} are created and space for the results is allocated.
	\item On lines 27-29 memory on the device is allocated. Note that only memory for one instance of the matrices \texttt{a}, \texttt{b} and \texttt{c} per stream is allocated. Not all inputs or results have to be stored on the GPU at the same time.
	\item On line 34 the stream to use for the next operations is set.
	\item On lines 36 and 37 the matrices \texttt{a} and \texttt{b} are loaded onto the device.
	\item On line 40 the matrices \texttt{a} and \texttt{b} are multiplied and the result stored in the memory allocated for the respective stream.
	\item This result is transferred back to the host in line 42.
\end{itemize}

This all is a rather straightforward process, except for the function \texttt{pinned\_memory} defined on lines 5-10.
Copying memory to and from the device in an asynchronous way (i.e. interleaved with other scheduled operations) requires the host-memory involved in that operation to be pinned (see CUDA Programming Guide v11.7.0 \cite[section 3.2.6.3]{Programming_Guide:online}).
The concept of pinned memory is well explained in a blog post by Harris \cite{HowtoOpt83:online}, even though the importance of using pinned memory seems to have waned as fast CPUs and fast memory are now more widespread than they were at the time of that blog post.
This is not a big restriction since data needed for high-performance calculations should not be swapped out of the main memory in any case, but Numpy does not make this assumption.
By default, memory allocated by Numpy is not pinned, Numpy (or C's \texttt{malloc} for that matter) also has no problem with allocating memory for arrays which are larger than the total installed host-memory.

To deal with this CUDA offers the functions \texttt{cudaMallocHost} and \texttt{cudaHostAlloc}, which CuPy wraps using cython and then in-turn offers to allocate memory using \texttt{cupy.cuda.alloc\_pinned\_memory}.
Combined with Numpy's \texttt{frombuffer} it is possible to use that memory for Numpy arrays i.e. create Numpy arrays using pinned memory, which can be used for asynchronous memory transfers.

\begin{minted}
	[
		frame=lines,
		framesep=2mm,
		baselinestretch=1.2,
		bgcolor=LightGray,
		fontsize=\footnotesize,
		breaklines=true,
		linenos
		]{python}
import cupy as cp
import numpy as np
		
		
def pinned_array(array):
	# first constructing pinned memory
	mem = cp.cuda.alloc_pinned_memory(array.nbytes)
	src = np.frombuffer(mem, array.dtype, array.size).reshape(array.shape)
	src[...] = array
	return src

# number of matrices to multiply
N = 100
# number of streams to utilize
streams = 3
# creating the streams
s = [cp.cuda.Stream() for i in range(streams)]
	
# creating matrices N matrices a and b on the host
a = [pinned_array(np.random.rand(400,400)) for i in range(N)]
b = [pinned_array(np.random.rand(400,400)) for i in range(N)]
# allocating memory for the results on the host
c = [pinned_array(np.empty(shape=(400,400))) for i in range(N)]
		
# allocating memory for the matrices a,b and c
# each stream needs its own memory to avoid data-races
a_d = [cp.empty_like(a[0]) for i in range(streams)]
b_d = [cp.empty_like(a[0]) for i in range(streams)]
c_d = [cp.empty_like(a[0]) for i in range(streams)]
		
		
for i in range(N):
	# determines which of the three streams should be used
	s[i%streams].use()
	# load contents into the device arrays
	a_d[i%streams].set(a[i])
	b_d[i%streams].set(b[i])
		
	# calculate the result and store it in the respective device memory
	c_d[i%streams] = cp.matmul(a_d[i%streams],b_d[i%streams])
	# transfer the result back to the host
	c_d[i%streams].get(out=c[i])		
\end{minted}
\caption{Example of CuPy-code making use of streams \label[code]{cupy_stream}}


\subsection{The Roofline Performance Model}
\label{roofline-section}

The roofline performance model proposed by Williams \cite{10.1145/1498765.1498785} provides a simple model to determine whether a kernel is compute- or memory-bound on a given hardware.
It does so by relating the computational intensity of a kernel with the hardware's maximum FLOPS and memory bandwidth and therefore turns the rather vague statements made in the previous section into measurable figures and guides further optimization.

Figure \ref{roofline-example} is an example of the roofline performance model visualized.
The slope on the left-hand side indicates how fast the memory can supply the processor with data, whereas the horizontal lines indicate the peak computing performance of the chip.
This figure tells us that peak performance cannot be achieved for simple tasks like multiplying every element of a single-precision vector by a fixed value.
A kernel performing that task would need to transfer 2 4-byte floating point numbers and perform a single floating point operation per element, resulting in a computational intensity of $1/8$ and a performance of less than 100 GFlops.
This example would lead to a memory-bound kernel.
Similarly, the displayed kernels \textbf{A} and \textbf{B} are memory-bound.
The kernel \textbf{C} does reaches neither the memory-limit nor the performance limit.
This can be due to several reasons, it either performs non-coalesced memory-accesses, which would significantly lower the achievable memory-bandwidth (as discussed in section \ref{memory-access-pattern}) or perform the "wrong" floating-point operations (as discussed in section \ref{flop-calculation}, peak performance can only be achieved by executing FMA-instructions).
Those limitations could be modeled as additional ceilings to get a clearer picture of the effective bottleneck.

\begin{figure}[H]
	\centering
	\includegraphics[width = \textwidth]{figures/roofline.png}
	\caption{Three different kernels placed in an annotated roofline chart as presented by Nsight Compute}
	\label{roofline-example}
\end{figure}

Note figure \ref{roofline-example} is drawn for a memory-bandwidth of 448 GBps, the bandwidth of the connection between the GPU and its dedicated off-chip memory.
If one were to draw a roofline chart considering the bandwidth between the GPU and the host memory (a mere 14 GBps), the sloped line would be shifted down and the ridge points shifted right by about 1.5 orders of magnitude.
This property, once-again, demonstrates why offloading work to the GPU is only useful for certain kinds of tasks.
Namely large tasks with superlinear time-complexities (like $\mathcal{O}(n^3)$ for matrix-matrix multiplications or $\mathcal{O}(n \log{n})$ in the case of the fast fourier transform) resulting in large arithmetic intensities.

\subsection{Case study: FFTs on the GPU}
\label{fft-case-study}

The FFT (Fast Fourier Transform) Algorithm is ubiquitously used in many domains of science, especially in signal processing.
In this case study we explore a number of different variations of this algorithm and try to adapt them to GPUs.
The algorithms that will be examined and adapted in this section stem from the comprehensive book on FFTs by Chu \cite{ChuEleanor2000ItFb}.

A more complete introduction to the discrete Fourier transform a derivation of the fast fourier transform will be given in section \ref{fft-introduction}.

\subsubsection{The Stockham auto-sort Algorithm}


The first algorithm explored is the DIF (Decimation in Frequency) FFT for complex 1D inputs and outputs in natural order as outlined in chapter 6 (specifically algorithm 6.1 "The (ordered) radix-2 $\text{DIF}_{\text{NN}}$ FFT algorithm.") of the aforementioned book \cite{ChuEleanor2000ItFb}.

In a first step the algorithm was expressed typical Numpy syntax (i.e. as few loops as possible, trying to express as much as possible using operations on slices of arrays instead of using explicit Python loops) as displayed in listing \ref*{fft_dif_it_numpy}.

\begin{listing}[H]
	\inputminted[
		frame=lines,
		framesep=2mm,
		baselinestretch=1.2,
		bgcolor=LightGray,
		fontsize=\footnotesize,
		breaklines=true,
		linenos
	]{Python}{../fft/fft_dif_it_numpy.py}
	\caption{Implementation of the Stockham FFT using Numpy}
	\label[code]{fft_dif_it_numpy}
\end{listing}

Next we expressed the very same algorithm using CUDA C as displayed in listing \ref{fft_dif_it_cuda}.
This algorithm does not operate on the input in-place, but requires a second array of equal size to store intermediate results in, which is why the function signature indicates pointers to two arrays of floating point numbers (\texttt{const float* input}, \texttt{float* output}).
This is less obvious in the Numpy implementation (it uses the arrays \texttt{res} \texttt{y\_l} and \texttt{z\_l}), where we do not have to manually manage memory.

In this implementation every thread performs exactly one butterfly combination consisting of combining two complex numbers from the input array into two different complex numbers, which are then stored in the output array.
It is noteworthy how the programming language is, unlike Numpy or Python in general, not aware of complex numbers and we consciously have to load and store the real and complex parts of a number from consecutive memory addresses.

In this implementation we also encounter one of the limitations of CUDA. Whereas, in the Numpy implementation, we initiate new iterations (the loop defined on line 12) until $\log2(N)$ iterations have been performed. Although \texttt{for} and \texttt{while} constructs are possible in CUDA kernel code, performing multiple iterations within a kernel call is still not as simple in the CUDA implementation.
It would require all threads to have performed their butterfly combination, communicate that fact among all threads (synchronization between threads), swap the pointers for input and output and advance an iteration.
CUDA however does not offer any way to synchronize more than a single block (this can be achieved by using the \texttt{\_\_syncthreads()} methods) due to the fact that it is by no means guaranteed that all blocks are even processed at the same time.
Since we want to be able to process problems larger than $N=2^{11}=2048$\label{size-limitation}, we have to fall back on a different synchronization technique: consecutive kernel invocations with different arguments (swapping the pointers for the input and output arrays and incrementing the iteration). This procedure also highlights the computational complexity of the FFT algorithm, launching $\log2(N)$ with $\frac{N}{2}$ threads each and every thread performing a single butterfly leads us to the well known complexity of $\mathcal{O}(N \log(N))$.


\begin{listing}[H]
	\begin{minted}[
		frame=lines,
		framesep=2mm,
		baselinestretch=1.2,
		bgcolor=LightGray,
		fontsize=\footnotesize,
		breaklines=true,
		linenos
		]{C}
extern "C" __global__
  void my_fft(const float* input, float* output, int iteration, int original_size, float omega_real, float omega_imag) {
    int tid = blockDim.x * blockIdx.x + threadIdx.x;
    
    int n_Problems = 1<<(iteration);
    int distance = 1<<iteration;
    int problem = tid/(original_size/(n_Problems*2));
    int posInProb = tid%(original_size/(n_Problems*2));

    float2 omega = make_float2(omega_real, omega_imag);
    float2 twiddle = raise_omega(omega, -posInProb*n_Problems);

    float2 first = make_float2(
       input[(posInProb*n_Problems+problem)*2],
       input[(posInProb*n_Problems+problem)*2+1]);
    float2 second = make_float2(
       input[(posInProb*n_Problems+problem)*2+original_size],
       input[(posInProb*n_Problems+problem)*2+1+original_size]);

    output[(posInProb*n_Problems*2+problem)*2] = first.x + second.x;
    output[(posInProb*n_Problems*2+problem)*2+1] = first.y + second.y;

    float2 toTwiddle = make_float2(first.x - second.x, first.y - second.y);
    complex_mult(&toTwiddle, twiddle);

    output[(posInProb*n_Problems*2+problem+distance)*2] = toTwiddle.x;
    output[(posInProb*n_Problems*2+problem+distance)*2+1] = toTwiddle.y;
  }
\end{minted}
	\caption{Implementation of the Stockham FFT using CUDA C}
	\label[code]{fft_dif_it_cuda}
\end{listing}


\paragraph{Benchmark}

Normally we would compare our CUDA implementation against the Numpy-baseline as defined in listing \ref*{fft_dif_it_numpy}, but since this is an FFT algorithm we have access to a wide selection of heavily optimized implementations from Numpy, scipy or FFTW3 \cite{FFTW3}. For this first preliminary comparison we compare it against \text{numpy.fft.fft}, being well aware of the fact that, that function might not be the fastest CPU implementation.

Note that Numpy's FFT-routine does not handle single precision floats, they are cast to double precision before computation and the output is also of double precision. The comparison pictured in \ref{cuda_naive_numpy} also does not take the time it takes to transfer data to and from the GPU, which can be significant, into account. More elaborate comparisons for 1D FFT-transformations are presented in section \ref{1d_transforms}, but this first comparison already shows promising results, considering the crude GPU implementation.

\begin{figure}[H]
	\includegraphics[width = \textwidth]{../fft/naive_cuda_numpy.pdf}
	\caption{Comparison between the naive CUDA implementation developed in this section and Numpy for single precision 1D-FFTs}
	\label[figure]{cuda_naive_numpy}
\end{figure}

\paragraph{Memory access patterns}\mbox{}\\

Additionally to timing the whole procedure we can also get timings for the individual iterations and memory transfers to and from the GPU using the Nvidia Nsight Profiler.
In Table \ref{run-times_cuda_stockham} the absolute run times of the individual kernel invocations for a problem of size $N=2^{27}$ are displayed.
This data reveals that there are great discrepancies between the run-times of the consecutive kernel invocations even though we can easily convince ourselves of the fact that the number of threads and the work each threads performs is unchanged.

The middle stage take significantly longer to execute than those at the beginning or end.
For example the tenth stage takes almost 90 milliseconds, about 17-times as long as the fastest stage, which completes in 5.17 milliseconds.

\begin{table}[H]
	\centering
	\begin{tabular}{l|rr}
		\toprule
		iteration & Duration (ms) & rel. Duration                                         \\
		\midrule
		0         & 5.43          & {\cellcolor[HTML]{00441B}} \color[HTML]{F1F1F1} 1.05  \\
		1         & 13.15         & {\cellcolor[HTML]{006328}} \color[HTML]{F1F1F1} 2.54  \\
		2         & 23.47         & {\cellcolor[HTML]{19833E}} \color[HTML]{F1F1F1} 4.54  \\
		3         & 27.64         & {\cellcolor[HTML]{278F48}} \color[HTML]{F1F1F1} 5.34  \\
		4         & 33.80         & {\cellcolor[HTML]{38A156}} \color[HTML]{F1F1F1} 6.53  \\
		5         & 44.95         & {\cellcolor[HTML]{68BE70}} \color[HTML]{000000} 8.69  \\
		6         & 58.43         & {\cellcolor[HTML]{A3DA9D}} \color[HTML]{000000} 11.29 \\
		7         & 83.03         & {\cellcolor[HTML]{ECF8E8}} \color[HTML]{000000} 16.05 \\
		8         & 82.75         & {\cellcolor[HTML]{ECF8E8}} \color[HTML]{000000} 15.99 \\
		9         & 89.45         & {\cellcolor[HTML]{F7FCF5}} \color[HTML]{000000} 17.29 \\
		10        & 84.58         & {\cellcolor[HTML]{EFF9EC}} \color[HTML]{000000} 16.35 \\
		11        & 71.98         & {\cellcolor[HTML]{D1EDCB}} \color[HTML]{000000} 13.91 \\
		12        & 53.85         & {\cellcolor[HTML]{90D18D}} \color[HTML]{000000} 10.41 \\
		13        & 27.17         & {\cellcolor[HTML]{258D47}} \color[HTML]{F1F1F1} 5.25  \\
		14        & 18.56         & {\cellcolor[HTML]{097532}} \color[HTML]{F1F1F1} 3.59  \\
		15        & 32.14         & {\cellcolor[HTML]{339C52}} \color[HTML]{F1F1F1} 6.21  \\
		16        & 36.55         & {\cellcolor[HTML]{40AA5D}} \color[HTML]{F1F1F1} 7.06  \\
		17        & 41.44         & {\cellcolor[HTML]{58B668}} \color[HTML]{F1F1F1} 8.01  \\
		18        & 41.34         & {\cellcolor[HTML]{56B567}} \color[HTML]{F1F1F1} 7.99  \\
		19        & 41.70         & {\cellcolor[HTML]{58B668}} \color[HTML]{F1F1F1} 8.06  \\
		20        & 35.12         & {\cellcolor[HTML]{3CA559}} \color[HTML]{F1F1F1} 6.79  \\
		21        & 31.56         & {\cellcolor[HTML]{329B51}} \color[HTML]{F1F1F1} 6.10  \\
		22        & 11.28         & {\cellcolor[HTML]{005B25}} \color[HTML]{F1F1F1} 2.18  \\
		23        & 5.17          & {\cellcolor[HTML]{00441B}} \color[HTML]{F1F1F1} 1.00  \\
		24        & 5.35          & {\cellcolor[HTML]{00441B}} \color[HTML]{F1F1F1} 1.03  \\
		25        & 6.79          & {\cellcolor[HTML]{00491D}} \color[HTML]{F1F1F1} 1.31  \\
		26        & 5.18          & {\cellcolor[HTML]{00441B}} \color[HTML]{F1F1F1} 1.00  \\
		\bottomrule
	\end{tabular}
	\caption{Run times of the individual stages for a problem of size $N=2^{27}$ and therefore 27 iterations}
	\label[table]{run-times_cuda_stockham}
\end{table}

To understand why this happens it is necessary to look at the way this algorithm accesses the global memory (illustrated in figure \ref{stockham-access-pattern}).
In this figure the memory access pattern for a problem size of $N = 2^6 = 64$ is pictured (a rather small problem size, but it already displays the property that will be discussed and similar visualizations for larger problems are not feasible).
To transform the input 6 consecutive kernel launches with 32 threads each are necessary.
The operations are color-coded to indicate which thread is responsible for that operation, with colors ranging from yellow for thread 0 to green for thread 31.
It is visually evident that the memory is accessed in well-behaved ways (desirable memory access would be described by consecutive threads accessing consecutive locations in memory, this behavior is described by the term \emph{unit stride}) in the first few and last few stages, but in the middle stages the access and store strides are very unfavorable.

This leads us to the importance of efficient access of global (off-chip) memory.
Due to the very nature of DRAM chips reading consecutive bytes of memory (as long as they are located within the same row or page) is far faster than reading bytes from random positions.
This topic was discussed in a blog post by Harris \cite{HowtoAcc88:online} in 2013.
To confirm the relevancy of that post's finding for our modern hardware we decided to reproduce its experiment on strided memory access.


\begin{figure}[H]
	\includegraphics[width = \textwidth]{../fft/fft.pdf}
	\caption{Memory access pattern of the Stockham FFT for a problem of size $N = 2^6 = 64$}
	\label[fig]{stockham-access-pattern}
\end{figure}

\paragraph{Effect of strided access on memory bandwidth}\mbox{}\\
\label{memory-access-pattern}

In this experiment a large array of memory is allocated and subsequently a CUDA kernel (defined in listing \ref{cuda_strided}) is launched to increment elements of that array by one.
Consecutive Threads however do not necessarily increment neighboring elements of the array.
Instead, the elements of the array are accessed and modified in a strided fashion, controllable by the parameter \texttt{s} passed to the kernel. This allows us to observe the varying performance of these operations.

In this timed benchmark there are always 4 megabytes of data read, modified and written back to global memory (note that 4 megabytes of data are a different number of elements for different data types, since data types are not necessarily equal in size).
The amount of data moved is then divided by the time taken to obtain a figure for the achieved throughput.

\begin{listing}[H]
	\begin{minted}[
		frame=lines,
		framesep=2mm,
		baselinestretch=1.2,
		bgcolor=LightGray,
		fontsize=\footnotesize,
		breaklines=true,
		linenos
		]{C++}
template <typename T>
__global__ void stride(T *a, int s)
{
    int i = (blockDim.x * blockIdx.x + threadIdx.x) * s;
    a[i] = a[i] + 1;
}
	\end{minted}
	\caption{The CUDA Kernel for benchmarking the strided memory accesses, uses templates to be able to handle a variety of different data types.}
	\label[code]{cuda_strided}
\end{listing}

The described experiment was carried out for a number of different data types:

\begin{itemize}
	\item \texttt{char} - size: 1 byte - Number of elements in 4 megabytes / Nr. of threads launched: 4194304
	\item \texttt{int\_16} - size: 2 byte - Number of elements in 4 megabytes / Nr. of threads launched: 2097152
	\item \texttt{float} - size: 4 byte - Number of elements in 4 megabytes / Nr. of threads launched: 1048576
	\item \texttt{double} - size: 8 byte - Number of elements in 4 megabytes / Nr. of threads launched: 524288
	\item \texttt{2 packed doubles} - size: 16 byte - Nr. of elements in 4 MB / Nr. of threads launched: 262144
	\item \texttt{4 packed doubles} - size: 32 byte - Nr. of elements in 4 MB / Nr. of threads launched: 131072
	\item \texttt{8 packed doubles} - size: 64 byte - Nr. of elements in 4 MB / Nr. of threads launched: 65536
\end{itemize}

The packed doubles are a custom data type as defined in listing \ref{packed}.
It was created to observe the performance of accessing larger consecutive structures of data.
This is similar to accessing complex numbers which are essentially 2 packed floating point values which are usually accessed together.


The result of those measurements is visualized in figure \ref{coalesced_access_effect}.
It is evident that accessing memory in a strided fashion is associated with very high performance penalties.
Memory Bandwidth utilization is over 300 GB/s for all data types when accessing elements with a unit-stride, but not exactly the same because each thread is only accessing a single element and therefore a lot more threads are involved when processing the \texttt{char} types than when processing the \texttt{float} or \texttt{double} types and those additional threads incur performance overhead.

It can be observed that throughput decline with a $\frac{1}{\text{stride}}$ relationship (linear in the logarithmic plot) until every memory row accessed only contains a single type of the element.
From this data we can conclude that in such a memory row one can fit 32 \texttt{char}s, 16 \texttt{int\_16}s, 8 \texttt{float}s or 4 \texttt{double}s (note the kinks in the respective graphs where performance stops to decrease with the $\frac{1}{\text{stride}}$ relationship), all datapoints agree that such a row must be 32 bytes large.
The size of these rows however can vary from GPU model to GPU model, the results of a very similar experiment presented by Stephen Jones at Nvidia GTC (GPU Technology Conference) 2022 \cite{HowCUDAP46:online} indicate that this value is 64 bytes on the A100, a high-end accelerator for HPC and AI workloads.

This dramatic drop in performance cannot (or to a much lesser extent) be observed when the data, every thread has to fetch, is almost as big or bigger than a single memory page.
When accessing two packed doubles, which have a combined size of 32 bytes, the loss in performance when going from \text{stride=1} to \texttt{stride=2} is still noticeable, but not when accessing 4 or 8 packed doubles at a time.
Memory accesses for such large consecutive chunks of data are, by definition, always coalesced and don't suffer from the same dramatic performance degradation.

\begin{figure}[H]
	\centering
	\includegraphics[]{../coalesced/coalesced_access_effect.pdf}
	\caption{Effects on memory throughput when using strided memory access patterns.}
	\label[fig]{coalesced_access_effect}
\end{figure}

One can also recognize that there are local maxima in performance which far from coalesced access patterns like accessing 2 packed doubles with a stride of 32 (and similarly for doubles at a stride of 64, floats at a stride of 128 and presumably for chars at a stride of 256).
These maxima are very likely to stem from aligned accesses one layer higher in the hierarchy of memory (i.e. the elements are on the same position of consecutive pages of memory).
The insights this experiment provides us with are crucial to understanding the differences in runtime of the different stages of the FFT-algorithm as presented in table \ref{run-times_cuda_stockham}.

The fact that the benchmark reports slightly higher figures for the bandwidth when using unit-stride as stated possible by the datasheet (448 GB/s) is due to caching effects.

The code and data related to this section is located in the folder \texttt{coalesced}.

\begin{listing}[H]
	\begin{minted}[
		frame=lines,
		framesep=2mm,
		baselinestretch=1.2,
		bgcolor=LightGray,
		fontsize=\footnotesize,
		breaklines=true,
		linenos
		]{C++}
template <int n, typename T>
class packed
{
public:
    T vals[n];

    __host__ __device__ packed()
    {
        #pragma unroll
        for (size_t i = 0; i < n; i++)
        {
            vals[i] += 0;
        }
    };

    __host__ __device__ packed operator+(T x)
    {
        packed res;
        #pragma unroll
        for (size_t i = 0; i < n; i++)
        {
            res.vals[i] += x;
        }
        return res;
    };
};
\end{minted}
	\caption{Custom class \texttt{packed} to hold several packed values of a specified type. The qualifiers \texttt{\_\_host\_\_} and \texttt{\_\_device\_\_} are necessary to indicate to the compiler that versions for execution on the CPU and GPU must be generated. The compiler directive \texttt{\#pragma unroll} is used to hint to the compiler that this loop should be unrolled in the compilation. This is possible because all template parameters must be known at compile time and it saves some memory on the device since the threads do not have to keep track of the variable \texttt{i}.}
	\label[code]{packed}
\end{listing}

\paragraph{Kernel for problems of limited size}\mbox{}\\
\label{size-limitation}
This knowledge, of course, raises the question of how fast a kernel, without those far-apart memory accesses, could be.
As hinted at in \ref{size-limitation} threads within a single block are guaranteed to be alive at the same time.
This is due to the fact that they are all executed on the same SM and as we can infer from the schematic of an SM (figure \ref{block-SM}), SMs offer 128 KB of fast Shared Memory the threads on that SM can use for fast inter-thread communication and data exchange.

Each block can at most allocate 49152 bytes (48 kilobytes are guaranteed to be addressable by all GPU models regardless of compute capability, the actual amount of memory addressable by GPUs of certain compute capabilities varies between 48 and 164 kilobytes, allocating more than the standard 48 kilobytes needs to be done in a slightly different way than shown in listing \ref{fft_dif_it_cuda_shared}: dynamic shared memory allocations instead of static shared memory allocation within the kernel) of that shared memory, which would equal 12288 single precision floats or 6144 double precision floats. To process an FFT of size $N = 2048$ using the largest block possible consisting of 1024 threads using the proposed algorithm, we would need 2 arrays of size $2048 \cdot 2$ (due to the need to store both the complex and imaginary part). In total 8192 floating point values.
Knowing this limitation we can at most perform an $N=2048$ single precision or $N=1024$ double precision FFT within a block.

Using the shared memory we can perform so-called coalesced reads from the global off-chip memory into the faster on-chip shared memory. This can be seen on lines 11-14 in listing \ref{fft_dif_it_cuda_shared}. Every thread reads four floats from adjacent memory positions, which was shown to be much more efficient in the previous section. The exact same behavior can be observed on lines  53-56 where the results are written back to global memory.

\begin{minted}[
		frame=lines,
		framesep=2mm,
		baselinestretch=1.2,
		bgcolor=LightGray,
		fontsize=\footnotesize,
		breaklines=true,
		linenos
		]{C}
extern "C" __global__
  void my_fft(const float* input, float* output, int original_size, float omega_real, float omega_imag) {
    __shared__ float t1[1024*2*2];
    __shared__ float t2[1024*2*2];
    
    int tid = blockDim.x * blockIdx.x + threadIdx.x;
    int iteration = 0;
    
    // each thread loads 4 floats into the shared memory
    // -- the real and imaginary parts of two complex64 values
    t1[threadIdx.x*4] = input[tid*4];
    t1[threadIdx.x*4+1] = input[tid*4+1];
    t1[threadIdx.x*4+2] = input[tid*4+2];
    t1[threadIdx.x*4+3] = input[tid*4+3];
    
    float* input_;
    float* output_;
    
    input_ = t1;
    output_ = t2;
    int problemsize = 2048;
    __syncthreads();
    while(problemsize > 1){
        int n_Problems = 1<<(iteration);
        int distance = 1<<iteration;
        int problem = threadIdx.x/(original_size/(n_Problems*2));
        int posInProb = threadIdx.x%(original_size/(n_Problems*2));

        float2 omega = make_float2(omega_real, omega_imag);
        float2 twiddle = raise_omega(omega, -posInProb*n_Problems);
        __syncthreads();

        float2 first = make_float2(input_[(posInProb*n_Problems+problem)*2], input_[(posInProb*n_Problems+problem)*2+1]);
        float2 second = make_float2(input_[(posInProb*n_Problems+problem)*2+original_size], input_[(posInProb*n_Problems+problem)*2+1+original_size]);

        output_[(posInProb*n_Problems*2+problem)*2] = first.x + second.x;
        output_[(posInProb*n_Problems*2+problem)*2+1] = first.y + second.y;

        float2 toTwiddle = make_float2(first.x - second.x, first.y - second.y);
        complex_mult(&toTwiddle, twiddle);

        output_[(posInProb*n_Problems*2+problem+distance)*2] = toTwiddle.x;
        output_[(posInProb*n_Problems*2+problem+distance)*2+1] = toTwiddle.y;
        
        float* temp;
        temp = input_;
        input_ = output_;
        output_ = temp;
        problemsize /= 2;
        iteration++;
    };

    output[tid*4] = input_[threadIdx.x*4];
    output[tid*4+1] = input_[threadIdx.x*4+1];
    output[tid*4+2] = input_[threadIdx.x*4+2];
    output[tid*4+3] = input_[threadIdx.x*4+3];
  }
\end{minted}
\captionof{listing}{implementation of the Stockham FFT using CUDA C and shared memory \label[code]{fft_dif_it_cuda_shared}}

The kernel proposed here allows us to perform a great number (only limited by the size of global memory) of medium-sized 1D-FFTs with a single kernel launch.
This is very useful when computing FFTs of higher-dimensions, which consist of repeated 1D-FFTs over different axis of the data.

It is also noteworthy that this kernel does not need a second array of size $N$ within the global memory since all intermediate results are stored in shared memory. In a way this algorithm could be categorized as performing the computation in-place.

For benchmarking the benefit of this approach we compare the time needed for 1024 FFTs for a problem size of $N = 1024$. The kernel from listing \ref{fft_dif_it_cuda} requires $10240$ (1024 FFTs each consisting of 10 iterations) kernel launches and performs a great number of unaligned memory accesses.

The optimized kernel calculates all 1024 FFTs in a kernel launch that lasts for $160  \mu\text{s}$, while the less optimized version needs $289 \ \text{ms}$ for the computation of a \emph{single} FFT of size 1024.
It can therefore be assumed that 1024 of those computations would take almost 5 minutes.
After all figure \ref{cuda_naive_numpy} shows clearly that our previous kernel was slower than the Numpy function running on the CPU for all but the largest FFTs.

We can conclude that the optimized version is far better suited for applications involving the calculation of numerous medium-sized FFTs. A more elaborate comparison with the recommended reference implementation of FFTs on Nvidia GPUs (cuFFT) follows in section \ref{cufft_section}.

\subsubsection{Interactive Demonstration of the implemented algorithm using OpenCV}
\label{opencv_demo}

With an efficient implementation of the algorithm for batched DFTs of medium size, we can demonstrate this algorithm by employing it a real-time signal processing application.
For this application we make use of OpenCV \cite{opencv_library}.
OpenCV is an important library for real-time computer vision.
We utilize OpenCV for this application because it supports CUDA and OpenGL interoperability, which allows developers to use it to render the contents of CUDA arrays residing on the GPU's global memory directly to the screen without having to transfer those memory regions from the device memory to the hosts memory and displaying them like a traditional matrix, residing on the CPU, with tools like matplotlib \cite{Hunter:2007}.
This concept is referred to as zero-copy data exchange.
The CUDA-OpenGL interoperability is described in section 3.2.13.1 (OpenGL Interoperability) of the CUDA Programming Guide v11.7.0 \cite{Programming_Guide:online}.

This demonstration application reads in an image from a file, calculates the corresponding two-dimensional frequency spectrum, applies a filter to that spectrum, performs the inverse FFT on the filtered spectrum and displays the modified image as well as the filtered spectrum.
The parameters of the filter are interactively modifiable by the user and the resulting image updated as fast as possible to convey a sense of the achieved real-time performance.

\paragraph{Compiling OpenCV}\mbox{}\\
\label{opencv-compilation}
For this application to work as intended it is necessary to use an OpenCV version with both CUDA, OpenGL support.
For the subsequent version of this program in Python, OpenCV also needs to be built with Python3 bindings.
This is already a slightly exotic requirement from OpenCV and not all the distributed binaries of OpenCV might support this feature-set, since getting such version of OpenCV compiled is not trivial, we provide the options OpenCV was configured with before compilation.
Note: OpenCV was compiled with the QT UI-framework instead of GTK because GTK proved to be incompatible with OpenGL; the OpenCV version used was release 4.6.0.

After compilation, it might be necessary to copy the resulting Python extension into the \texttt{site-packages} folder of the desired Python environment.

\begin{minted}
		[
			frame=lines,
			framesep=2mm,
			baselinestretch=1.2,
			bgcolor=LightGray,
			fontsize=\footnotesize,
			breaklines=true,
			]{bash}
cmake \
-D CMAKE_BUILD_TYPE=RELEASE \
-D CMAKE_INSTALL_PREFIX=/usr/local\ \
-D INSTALL_C_EXAMPLES=ON \
-D INSTALL_PYTHON_EXAMPLES=ON \
-D OPENCV_GENERATE_PKGCONFIG=ON \
-D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib/modules \
-D BUILD_EXAMPLES=ON \
-D BUILD_opencv_world=ON \
-D WITH_CUDA=ON \
-D WITH_OPENGL=ON \
-D WITH_QT=ON \
-D WITH_GTK=OFF \
-D BUILD_opencv_cvv=OFF \
-D BUILD_opencv_python3=ON \
-D PYTHON3_EXECUTABLE=$(which python3) \
-D PYTHON_INCLUDE_DIR=$(python3 -c "from distutils.sysconfig import get_python_inc; print(get_python_inc())") \  
-D PYTHON_INCLUDE_DIR2=$(python3 -c "from os.path import dirname; from distutils.sysconfig import get_config_h_filename; print(dirname(get_config_h_filename()))") \
-D PYTHON_LIBRARY=$(python3 -c "from distutils.sysconfig import get_config_var;from os.path import dirname,join; print(join(dirname(get_config_var('LIBPC')),get_config_var('LDLIBRARY')))") \
-D PYTHON3_NUMPY_INCLUDE_DIRS=$(python3 -c "import numpy; print(numpy.get_include())") \
-D PYTHON3_PACKAGES_PATH=$(python3 -c "from distutils.sysconfig import get_python_lib; print(get_python_lib())") \
-D PYTHON_DEFAULT_EXECUTABLE=$(which python3) \
-D HAVE_opencv_python3=ON \
-D BUILD_NEW_PYTHON_SUPPORT=ON \
..
make
sudo make install
	\end{minted}
\captionof{listing}{Commands used to compile and install OpenCV}

\paragraph{Program flow}\mbox{}\\

Figure \ref{opencv_schematic} outlines which steps are necessary to take the 2D-FFT of a grayscale image, filter it in the frequency domain and then transform it back to a viewable image.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.8\textwidth]{../opencv_demo/schematic.drawio.pdf}
	\caption{Schematic flow of the program, the labeled boxes indicate operations performed by CUDA-Kernels on the GPU. The two rightmost outputs from the bottom row are mapped to OpenGL-textures and displayed.}
	\label[fig]{opencv_schematic}
\end{figure}

The procedure consists of 9 individual CUDA-Kernel launches (4 batched 1D FFTs, 3 Transpositions, 1 arbitrary filter and a fftshift to get the quadrants in the expected positions for visualization purposes). Some of these operations (namely the FFT, FFT-shift and Filter-Kernels) can be performed in-place others (the kernel to transpose the matrix, even though it would be possible to write an in-place transposition kernel, the kernel implemented in opencv, which is used in this demonstration, does not operate in-place) require separate matrices for input and output.

This whole procedure is repeated for every change of the filter-parameters, even though the results of the first three kernels are obviously independent of the choice of filter. This is done intentionally and indicates that this approach would be sufficiently fast to also process constantly changing input images in real-time i.e. videos (or in a more general sense: real-time 2D sensor data) instead of still images.

\paragraph{Adapting the FFT-Kernel for inverse FFT-calculations}\mbox{}\\

The Kernel presented in listing \ref{fft_dif_it_cuda_shared} had to be altered only slightly to be able to perform the inverse-FFT.
The necessary changes are on the highlighted lines:
\begin{itemize}
	\item {line 1: changing the kernel to be templated, this allows us to easily generate kernels for various sizes of inputs and the template parameter \texttt{inverse} determines whether the kernel should perform the DFT or inverse DFT.
	      Alternatively one could pass these parameters as functions arguments, but that practice comes with the downside of run-time overhead.
	      In that case every thread would have to check at run-time if it is currently supposed to perform the regular or inverse transformation, whereas with template arguments it is known at run-time and the compiler creates entirely separate versions, which also benefit from better compile-time optimization.}
	\item {line 32: this line is responsible for calculating the twiddle-factors, in case of the inverse DFT, one has to use the opposite sign for the exponent when calculating the needed complex roots of unity.}
	\item {lines 54-57: Those lines are responsible for writing the results back to global memory. Depending on convention normalization factors have to be used either in the forward or backward transformations. Either the result of the forward or backward transformation has to be multiplied with $\frac{1}{N}$ or both results have to be scaled with $\frac{1}{\sqrt{N}}$. This version applies the normalization factor $\frac{1}{N}$ when performing the inverse transformation, this is consistent with the default choice of Numpy.}
\end{itemize}

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos,
	highlightlines={1,32,54-57}
	]{C++}
template <unsigned int size, bool inverse>
__global__ void my_fft_c2c(float *input, float *output, int original_size, float omega_real, float omega_imag)
  {
	__shared__ float t1[size * 2];
	__shared__ float t2[size * 2];
	
	int tid = blockDim.x * blockIdx.x + threadIdx.x;
	int iteration = 0;
	
	// each thread loads 4 floats into the shared memory
	// -- the real and imaginary parts of two complex64 values
	t1[threadIdx.x * 4] = input[tid * 4];
	t1[threadIdx.x * 4 + 1] = input[tid * 4 + 1];
	t1[threadIdx.x * 4 + 2] = input[tid * 4 + 2];
	t1[threadIdx.x * 4 + 3] = input[tid * 4 + 3];
	
	float *input_;
	float *output_;
	
	input_ = t1;
	output_ = t2;
	int problemsize = size;
	while (problemsize > 1)
	{
	
	  int n_Problems = 1 << (iteration);
	  int distance = 1 << iteration;
	  int problem = threadIdx.x / (original_size / (n_Problems * 2));
	  int posInProb = threadIdx.x % (original_size / (n_Problems * 2));
	
	  float2 omega = make_float2(omega_real, omega_imag);
	  float2 twiddle = raise_omega(omega, (inverse ? 1 : -1) * posInProb * n_Problems);
	  __syncthreads();
	  float2 first = make_float2(input_[(posInProb * n_Problems + problem) * 2], input_[(posInProb * n_Problems + problem) * 2 + 1]);
	  float2 second = make_float2(input_[(posInProb * n_Problems + problem) * 2 + original_size], input_[(posInProb * n_Problems + problem) * 2 + 1 + original_size]);
	
	  output_[(posInProb * n_Problems * 2 + problem) * 2] = first.x + second.x;
	  output_[(posInProb * n_Problems * 2 + problem) * 2 + 1] = first.y + second.y;
	
	  float2 toTwiddle = make_float2(first.x - second.x, first.y - second.y);
	  complex_mult(&toTwiddle, twiddle);
	
	  output_[(posInProb * n_Problems * 2 + problem + distance) * 2] = toTwiddle.x;
	  output_[(posInProb * n_Problems * 2 + problem + distance) * 2 + 1] = toTwiddle.y;
	
	  float *temp;
	  temp = input_;
	  input_ = output_;
	  output_ = temp;
	  problemsize /= 2;
	  iteration++;
 	};
	__syncthreads();
	output[tid * 4] = input_[threadIdx.x * 4] * (inverse ? 1. / size : 1.);
	output[tid * 4 + 1] = input_[threadIdx.x * 4 + 1] * (inverse ? 1. / size : 1.);
	output[tid * 4 + 2] = input_[threadIdx.x * 4 + 2] * (inverse ? 1. / size : 1.);
	output[tid * 4 + 3] = input_[threadIdx.x * 4 + 3] * (inverse ? 1. / size : 1.);
}
\end{minted}
\captionof{listing}{implementation of the Stockham FFT using CUDA C and shared memory \label[code]{fft_dif_it_cuda_shared_inverse}}

Note that all FFT Kernels in this example assume the inputs to be complex-valued, even though they are clearly not.
The first FFT-kernel for example which consumes the images' pixels ranging from 0-255 (data type: \texttt{unsigned char}) casts those values to floats and interprets them as the real part of a complex value and sets the imaginary part to zero.
This is inherently inefficient since it is known that the transformation of real-valued inputs has specific properties like symmetry.
Using these properties it is possible to reduce the computation of a real-valued FFT of size N to a complex-valued FFT of size N/2 as explained in detail in chapter 14.1 by Chu \cite{ChuEleanor2000ItFb}.

This optimization was not employed in this example to not limit the generality of the application to real-valued inputs like images.
\paragraph{Resulting images}\mbox{}\\

Figures \ref{uniform_opencv} and \ref{horizontal_opencv} display two filtered sample and their respective frequency spectra.
The sliders depicted in the right-hand images control the parameters of the applied filter, which is a 2D-gaussian function:

\begin{align*}
	\begin{pmatrix}
		x^\prime \\ y^\prime
	\end{pmatrix} & = \begin{bmatrix}
		                  \cos(\alpha) & -\sin(\alpha) \\
		                  \sin(\alpha) & \cos(\alpha)
	                  \end{bmatrix} \begin{pmatrix}
		                                x-\frac{n_x}{2} \\ y-\frac{n_y}{2}
	                                \end{pmatrix}                                                                                                              \\
	f(x,y)               & = \exp \left( -\left( \frac{\left(x^\prime-\frac{n_x}{2}\right)^2}{\sigma_x^2} + \frac{\left(y^\prime-\frac{n_y}{2}\right)^2}{\sigma_y^2}\right)\right)
\end{align*}

Of course there are many other valid filters which could be used to alter the spectrum, this is a basic example for a low-pass filter.

Every time on of the sliders' values changes, the procedure outlined in figure \ref{opencv_schematic} is re-calculated and the updated results are displayed.
\begin{figure}[H]
	\begin{minipage}{.48\textwidth}
		\includegraphics[width = \textwidth]{../opencv_demo/screenshots/image_uniform_blur.png}
	\end{minipage}\hfill%
	\begin{minipage}{.48\textwidth}
		\includegraphics[width = \textwidth]{../opencv_demo/screenshots/fft_uniform_blur.png}
	\end{minipage}%
	\caption{Image with a uniform low-pass filter applied and the corresponding frequency spectrum}
	\label[figure]{uniform_opencv}
\end{figure}

\begin{figure}[H]
	\begin{minipage}{.48\textwidth}
		\includegraphics[width = \textwidth]{../opencv_demo/screenshots/horizontal_blur_image.png}
	\end{minipage}\hfill%
	\begin{minipage}{.48\textwidth}
		\includegraphics[width = \textwidth]{../opencv_demo/screenshots/FFT_horizontal_blur.png}
	\end{minipage}%
	\caption{Image with a low-pass filter, which preserves vertical frequencies but filters high horizontal frequencies, applied and the corresponding frequency spectrum}
	\label[figure]{horizontal_opencv}
\end{figure}


\paragraph{Performance}\mbox{}\\

Profiling the program with Nsight Systems reveals how long the whole procedure and each individual kernel takes when a re-calculation is triggered by a user-made change of the filter-parameters.
Table \ref{opencv_profile} lists those timings and we can recognize that, first of all the calculations are clearly fast enough, for real-time user interaction.
The combined run-times of the 9 kernels add up to $940 \mu \text{s}$, that is however not the actual time elapsed between the invocation of the first kernel to the conclusion of the last kernel, because there are short phases in between the kernels.
Nsight Systems reports that time as $1050 \mu \text{s}$, so we can recognize that the used hardware is able to perform this calculations hundreds of times per second for this choice of algorithms and input sizes (a 1024 x 1024 grayscale image).

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\input{../opencv_demo/benchmark/open_cv.tex}
	\caption{Runtime of the individual kernels within the update-procedure. The kernels listed in this table correspond with those illustrated in figure \ref{opencv_schematic}}
	\label[table]{opencv_profile}
\end{table}

A surprisingly large share of the time is needed for the filter-kernel, which should be rather simple and embarrassingly parallel.
A closer look at that kernels source-code (see listing \ref{filter-source}) reveals some potential performance pitfalls.
One glaring flaw is the fact that the value of angle is passed to this kernel and then the kernel calculates the sine and cosine of that value.
There is nothing inherently wrong with calculating the sine and cosine within the kernel, but it is questionable at best, when all $2^20$ threads calculate the sine and cosine for the same argument.
Surely it would be advantageous to calculate those two values once on the host-CPU and pass them to the kernel instead of the angle itself.

To a lesser extent, the same argument could be made for the parameters $s_x$ and $s_y$ representing the standard deviations in the $x$ and $y$-directions.
These values are only used once and squared, so passing the kernel $s_x^2$ and $s_y^2$ directly, would save two floating point operations per thread and must be more efficient as well.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos,
	highlightlines={20}
	]{C++}
template <unsigned int size>
__global__ void filter(float *input, float *output, float s_x, float s_y, float angle)
{
	int tid = blockDim.x * blockIdx.x + threadIdx.x;

	// lambda function to calculate filter,
	auto coeff = [&s_x, &s_y](int y, int x)
	{
		return expf(-(
			((x - size/2) / s_x) * ((x - size/2) / s_x) + ((y - size/2) / s_y) * ((y - size/2) / s_y)));
	};

	// each thread handles a pixel
	// the offset of 512 and modulo take care of shifting the quadrants
	int x = (threadIdx.x + size/2) % 1024;
	int y = (blockIdx.x + size/2) % 1024;

	float cosval;
	float sinval;
	sincosf(angle, &sinval, &cosval);
	float x_ = (x - size/2) * cosval - (y - size/2) * sinval;
	float y_ = (x - size/2) * sinval + (y - size/2) * cosval;
	float c = coeff(x_ + size/2, y_ + size/2);
	output[tid * 2] = c * input[tid * 2];
	output[tid * 2 + 1] = c * input[tid * 2 + 1];
}
\end{minted}
\captionof{listing}{source code of the filter-kernel \label[code]{filter-source}}

The run-times presented in table \ref{opencv_profile} were recorded with a preliminary version (see listing \ref{filter-source-proto} for a section of that kernel), which was not templated with the parameter \texttt{size}, but where the size was hardcoded into the source like:

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos,
	firstnumber=9
	]{C++}
return expf(-(((x - 512.) / s_x) * ((x - 512.) / s_x) + ((y - 512.) / s_y) * ((y - 512.) / s_y)));
\end{minted}
\captionof{listing}{source code of the filter-kernel during the prototyping stages \label[code]{filter-source-proto}}

And it was not possible to reproduce the run-time with the templated version, furthermore the templated version did provide a far better performance (around $43 \mu\text{s}$) without incorporating any of the possible improvements, like passing the sine and cosine values directly to the kernel.
This naturally led to the question of why such a seemingly small change like replacing \texttt{512.} with \text{size/2} leads to a 4-fold speedup.
Further testing concluded that the performance degradation was entirely due to how the compiler treats literal-floating point numbers.
If a number has no suffix (like \texttt{512.}) it is treated as a double-precision floating point number, as a consequence of this most other variables in the expression will also be promoted to double-precision and double-precision floating point operations will be performed, which are far more time-consuming than single-precision or int32 operations.
It is therefore good practice to either avoid literal values like that completely or at least specify the intended data-type with an appropriate suffix (\texttt{f} or \texttt{F} for single-precision, \texttt{l} or \texttt{L} for long-double, there is no suffix for regular double-precision).
When using the appropriate data type (either single-precision float or int32), the code does not benefit from passing in the values for sine or cosine, since the kernel is memory-bound at that point.

\paragraph{Kernel fusing}\mbox{}\\
\label{kernel-fusing}

The procedure showcased in figure \ref{opencv_schematic} lists two consecutive kernels which are somewhat similar, both the filter and the shift kernel operate purely elementwise and they even share some of their computation since the filter kernel also calculates the shifted positions to apply the filter correctly.
Since there is no limitation that prevents a kernel from writing to two distinct arrays (i.e. having two outputs) one might try to fuse the two kernels. 
The resulting kernel should have a significantly higher computational intensity than the two individual kernels, since fewer data has to be retrieved from global off-chip memory.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{C++}
template <unsigned int size>
__global__ void shift(float *input, uchar *output)
{
	int tid = blockDim.x * blockIdx.x + threadIdx.x;
	
	// each thread handles a pixel
	int x = (threadIdx.x + size/2) % size;
	int y = (blockIdx.x + size/2) % size;
	// arbitrary scaling factor to get a decent contrast 
	// when displaying values in the range [0,255]
	float scaling_factor = 30;
	output[y * blockDim.x + x] = scaling_factor * log10(sqrt(input[tid * 2] * input[tid * 2] + input[tid * 2 + 1] * input[tid * 2 + 1]));
}
\end{minted}
\captionof{listing}{source code of the shift-kernel \label[code]{shift-source}}

Listing \ref{shift-source} shows the source of the shift-kernel, all it does is calculate the new position of a pixel in lines 7 and 8 and then calculates $\log_{10}(|f(x,y)|)$ with $f(x,y)$ being the complex-valued value of the spectrum at the specified index after the filter has been applied.

The two kernels presented in listing \ref{filter-source} and \ref{shift-source} can be combined (this process is referred to as kernel fusing) into a single kernel as shown in listing in \ref{fused-source}.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{C++}
template <unsigned int size>
__global__ void fused_filter_shift(float *input, float *output, uchar *output_viz, float s_x,float s_y, float angle)
{
	int tid = blockDim.x * blockIdx.x + threadIdx.x;
	
	// lambda function to calculate filter,
	auto coeff = [&s_x, &s_y](int y, int x)
	{
		return expf(-(
			((x - (float)(size)/2) / s_x) * ((x - (float)(size)/2) / s_x) + ((y - (float)(size)/2) / s_y) * ((y - (float)(size)/2) / s_y)));
	};
	
	// each thread handles a pixel
	// the offset of 512 and modulo take care of shifting the quadrants
	int x = (threadIdx.x + size/2) % size;
	int y = (blockIdx.x + size/2) % size;
	
	float cosval;
	float sinval;
	sincosf(angle, &sinval, &cosval);
	float x_ = (x - (float)(size)/2) * cosval - (y - (float)(size)/2) * sinval;
	float y_ = (x - (float)(size)/2) * sinval + (y - (float)(size)/2) * cosval;
	float c = coeff(x_ + (float)(size)/2, y_ + (float)(size)/2);
	float real_part = c * input[tid * 2];
	float imag_part = c * input[tid * 2 + 1];
	output[tid * 2] = real_part;
	output[tid * 2 + 1] = imag_part;
	
	output_viz[y * blockDim.x + x] = 30.f * log10(sqrt(real_part * real_part + imag_part * imag_part));
	}
\end{minted}
\captionof{listing}{source code of the fused-kernel \label[code]{fused-source}}

Comparing the resulting new kernel with the two separate kernels used before reveals significant performance benefits.
The two individual kernels needed:
\begin{itemize}
	\item filter-kernel $~43 \mu\text{s}$
	\item shift-kernel $~29 \mu\text{s}$
\end{itemize}
Whereas the new kernel only needs $~48 \mu\text{s}$ for both operations, that is a speedup of 33\%.
This is an expected result for memory-throughput-bound kernels.
One could also fuse this kernel with the preceding FFT-Kernel and following inverse FFT-Kernel and doing so will save some execution time, but at that point the kernel will become unwieldy and very specific to this application, which is in conflict with the principles of modular, reusable and testable code. Fusing an FFT-Kernel with a Transposition-Kernel would not be possible as easily, since those kernels both make heavy use of local memory to enable strided memory access, but each kernel in a different way.
The FFT-Kernel keeps a single row of data in local memory, unlike the Transposing-Kernel, which decomposes the input-matrix in many small tiles and processes those tiles within the shared memory of a SM.

Just like it usually pays of to perform as many operations on the GPU before retrieving the data back to the host-memory it is also advantageous to perform as many operations on the data as long as it is in the chip's memory.
As discussed in section \ref{amdahl section} this is a recurring theme in GPGPU-Computing even though the respective bandwidths are an order of magnitude apart in bandwidth: PCI-Express between the Host and this specific GPU (the PCI-Express standard could support higher bandwidths) at 14 GB/s; The Bus-System between the off-chip memory and the streaming multiprocessors at 448 GB/s.

Despite the new kernel featuring a higher computational intensity than the two kernels it stems from, an analysis with Nsight Compute reports that this kernel is still memory bound and would not benefit from reducing the number of floating point calculation it has to perform per thread.


The complete C++ source code which can be compiled with nvcc (nvidia c compiler) and a demonstration video, which shows the responsiveness of the program is located in the \texttt{opencv\_demo} folder.
This same program is also implemented using Python and its opencv bindings (see section \ref{opencv-demo-python})

\section{GPGPU and Python}
\label{cuda_python}

After having discussed Python, its limitations and ways to circumvent them in section \ref{intro} and the possibility of utilizing GPUs for scientific-numerical workloads in section \ref{GPGPU}, this section will explore Python packages and projects which aim to enable Python users to enhance the performance of their programs or scripts using GPUs.

\subsection{Available packages}
\label{python-packages}

It is possible to expose the functionality offered by the CUDA C/C++ to Python using tools like Cython and Pybind11 as discussed in section \ref{lower-level-packages} and showcased in the example involving cuBLAS presented in section \ref{pybind11-cublas}.
The immense popularity of the Python language (especially the popularity of Python in the domain of Artificial Intelligence and Machine Learning, fields which utilize GPGPU to a great extent) has given rise to a number Python libraries which try to bridge this gap between Python and the traditionally lower-level CUDA-API.
This section provides an overview over the most important and influential of those packages with a focus on their intended use, differences and interoperability.

\paragraph*{CuPy}\mbox{}\\

CuPy \cite{cupy_learningsys2017} is an open-source library, which is primarily developed by the japanese company preferred networks, designed as a drop-in replacement for NumPy and SciPy.
It aims to provide the same functionality, but accelerating that functionality by utilizing the GPU.
One of CuPy's strengths is its ability to allocate memory and create n-dimensional arrays residing on the GPU in a very concise and familiar way similar to the way Numpy is used for arrays on the host memory using functions like \texttt{cupy.zeros}, \texttt{cupy.empty} and \texttt{cupy.ones}.

CuPy makes heavy use of templated kernels (not in the sense of C++-Templates though) to create the required kernels and compile them just-in-time using NVRTC (Nvidia Runtime Compiler).
By doing so it also enables users to write CUDA C/C++-kernels directly in Python as strings (it is also possible to change those strings, which define the kernels, by using f-strings or more sophisticated templating systems to instantiate arbitrarily complex kernels at run-time) and call them after they have been compiled.
Alternatively since version 9.0 released in April 2021 CuPy supports defining GPU-kernels in pure Python instead of C/C++, it is able to generate C/C++ by analyzing the function's AST (abstract syntax tree).

Since version 7.0 released in December 2019 CuPy offers support for AMD hardware using ROCm HIP

CuPy relies on the previously discussed Cython library to interface with the CUDA-API.

\paragraph*{Numba}\mbox{}\\

As discussed in section \ref{lower-level-packages} Numba \cite{lam2015numba} is a package that can parse Python code and tries to compile it to equivalent code in the LLVM intermediate representation language, which can then optimized by LLVM.
In a subsequent step the optimized IR-Code can be transformed to machine-executable code using the backends integrated into the LLVM compiler framework.
Since LLVM has a backend for Nvidia GPUs, Numba is able to generate code executable on GPUs.

Additionally, to this method where regular Python code is compiled to GPU Kernels without much manual intervention, Numba also offers a dialect of Python referred to as "CUDA Python", in which CUDA-Kernels can be described very similarly to how they would be expressed in CUDA C/C++, but with Python Syntax.

Similarly to CuPy, Numba also offers functionality to allocate memory and create arrays within the device memory or copy existing Numpy arrays to the device memory using functions like \texttt{numba.cuda.device\_array} or \texttt{numba.cuda.to\_device}.

Numba added support for AMD hardware with version 0.40.0 in September 2018 and removed it with version 0.54.0 in August 2021

\paragraph*{PyCUDA}\mbox{}\\
PyCUDA \cite{kloeckner_pycuda_2012} is a library developed and maintained by Andreas Klöckner that aims to provide a complete wrapper of the CUDA C/C++ (it also includes bindings for the less frequently used parts of the API like the OpenGL interoperability, which both CuPy and Numba lack).
From a usage point of view PyCUDA is very similar to using CuPy with user-defined kernels: memory can be allocated on the device, copied to and from the device and GPU-Kernels are defined using triple-quoted multi-line Python strings, which are compiled by NVRTC as needed.
Unlike CuPy it does not feature any implementations for common operations like matrix-multiplications, matrix inversions or calculating the absolute value of elements in a matrix.
There are however packages that are built on top of PyCUDA and implement a number of those algorithms (as listed on the homepage of the PyCUDA documentation \cite{pycuda2092:online}).

PyCUDA uses boost, a library similar to pybind11 for interfacing with the CUDA C/C++ API.

\paragraph*{PyOpenCL}\mbox{}\\
PyOpenCL \cite{kloeckner_pycuda_2012} is a sister project to PyCUDA, aiming to provide a complete set of bindings for the OpenCL API.
It is also developed and maintained by Andreas Klöckner.
PyOpenCL uses pybind11 for interfacing with the OpenCL C/C++ API.


\paragraph*{PyTorch}\mbox{}\\

Meta AI's PyTorch \cite{NEURIPS2019_9015} is one of the leading frameworks used in deep learning research leveraging GPGPU for acceleration.
Besides uses in the artificial intelligence domain its GPU-accelerated tensor library can of course also be used to perform scientific computations.
It features a Numpy-like interface for such tasks.
Similarly to the other mentioned libraries PyTorch also exposes many features of the CUDA API (like streams) and ways to efficiently move data objects (PyTorch refers to its n-dimensional as tensors) between the host and device.

After initially only supporting Nvidia GPUs, PyTorch release 1.8 in March 2021 added support for AMD GPUs using ROCm.

\paragraph*{Official CUDA-Python bindings}\mbox{}\\

Starting from April 2021 \cite*{Unifying91:online} Nvidia began publishing its own official package \cite{CUDAPyth5:online} containing bindings for the CUDA-API using Cython.
This set of bindings is designed to replace the individual bindings other packages like CuPy or Numba maintain and build on.
As of time of writing Numba, for example uses these official bindings instead of its own, if it detects an installation of Nvidia's cuda-python package with plans to ultimately deprecate and remove its own bindings.
It can be assumed that other packags will follow and that this will become a common layer of abstraction between Python packages and the CUDA driver.

\paragraph*{JAX}\mbox{}\\

JAX \cite{jax2018github} is a framework from Google that makes use of the XLA \cite{XLAOptim3:online} (accelerated linear algebra) compiler to just-in-time compile a series of operations into a single kernel (opposed to calling multiple pre-compiled or pre-defined kernels).
JAX is predominantly used in conjunction with deep learning frameworks like PyTorch or TensorFlow, but it can also be on its own as a GPU-accelerated alternative to NumPy, whose API it closely follows.
Additionally, to supporting GPUs and CPUs, JAX can also target TPUs (Tensor Processing Units), hardware specifically designed to accelerate typical AI-workloads (i.e. large matrix multiplications).

JAX offers experimental support for ROCm.

\subsection{Examples using the mentioned packages}

To highlight the differences in functionality and programming approaches between the mentioned packages, we present a basic saxpy-kernel (saxpy is short for single precision $\mathbf{z} = \alpha \mathbf{x}+ \mathbf{y}$, using the naming convention used by the BLAS standard, the double-precision variant would be referred to as daxpy) implemented with each of the different packages.

\subsubsection{CuPy}

Listing \ref{cupy-naive} shows one of the ways CuPy can be used.
On lines 5-6 the inputs are created as traditional NumPy ndarrays and then transferred to the GPU on lines 9-10, at this point memory on the device is allocated.
The allocation on line 12 is not strictly, as that memory would have been allocated either way when executing line 15 needed but good practice.
On line 15 the actual computation is executed and on line 18 that result is retrieved from the device to the host using a DtoH-memcopy.

As mentioned in the previously given description CuPy is utilizing pre-defined templated-kernels to instantiate the right kernel just-in-time.
This is done to avoid having to define an exponentially increasing number of kernels (there would have to be kernel for every combination of data-types and operation involved), the code we defined on line 15, for example, would work the same if $a$, $\mathbf{x}$ and $\mathbf{y}$ were of type \texttt{float64}.
The same code could be re-used for both saxpy and daxpy.

By setting the environment variable \texttt{CUPY\_CACHE\_SAVE\_CUDA\_SOURCE} to \texttt{TRUE} prior to execution CuPy logs the human-readable C++ source-code of the kernels it created to files.
\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{Python}
import cupy
import numpy as np

# creating inputs on the host
x_host = np.arange(10_000, dtype=np.float32)
y_host = np.arange(10_000, dtype=np.float32)

# copying them to device memory
x = cupy.asarray(x_host)
y = cupy.asarray(y_host)
a = cupy.float32(5)
z = cupy.empty_like(x)

# calculating the result on the GPU
z = a*x + y

# retrieving the result from the GPU
z_host = cupy.asnumpy(z)
\end{minted}
\captionof{listing}{source code of the saxpy-kernel using CuPy \label[code]{cupy-naive}}

In this case for example CuPy created two separate kernels, one for the multiplication of $a$ and $\mathbf{x}$ and another one for the addition of $a \mathbf{x}$ and $y$.
These kernels are presented in listings \ref{cupy-multiply} and \ref{cupy-add}.
The kernels are very similar and both show signs (like the redundant semicolon in the second to last line) of being computationally generated from the same template.

\begin{minipage}[t]{.48\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true
	]{C++}
#include <cupy/complex.cuh>
#include <cupy/carray.cuh>
#include <cupy/atomics.cuh>

typedef float in0_type;
typedef float in1_type;
typedef float out0_type;

extern "C" __global__ void 
cupy_multiply__float_float32_float32(
	const float in0,
	const CArray<float, 1, 1, 1> _raw_in1,
	CArray<float, 1, 1, 1> _raw_out0,
	CIndexer<1, 1> _ind)
{
  ;
#pragma unroll 1
  CUPY_FOR(i, _ind.size())
  {
    _ind.set(i);
    const in1_type in1(_raw_in1[_ind.get()]);
    out0_type out0;
    out0 = in0 * in1;
    _raw_out0[_ind.get()] = out0;
    ;
  };
}
\end{minted}
	\captionof{listing}{generated kernel for the multiplication \label[code]{cupy-multiply}}
\end{minipage}\hfill%
\begin{minipage}[t]{.48\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true
	]{C++}
#include <cupy/complex.cuh>
#include <cupy/carray.cuh>
#include <cupy/atomics.cuh>

typedef float in0_type;
typedef float in1_type;
typedef float out0_type;

extern "C" __global__ void 
cupy_add__float32_float32_float32(
	const CArray<float, 1, 1, 1> _raw_in0,
	const CArray<float, 1, 1, 1> _raw_in1,
	CArray<float, 1, 1, 1> _raw_out0, 
	CIndexer<1, 1> _ind)
{
  ;
#pragma unroll 1
  CUPY_FOR(i, _ind.size())
  {
    _ind.set(i);
    const in0_type in0(_raw_in0[_ind.get()]);
    const in1_type in1(_raw_in1[_ind.get()]);
    out0_type out0;
    out0 = in0 + in1;
    _raw_out0[_ind.get()] = out0;
    ;
  };
}
\end{minted}
	\captionof{listing}{generated kernel for the addition \label[code]{cupy-add}}
\end{minipage}%

By inspecting the CuPy source it becomes evident that both of these kernels were created by adapting the template defined in the cython file \texttt{\_kernel.pyx} \cite[lines 47-73]{cupykern44:online} (as shown in listing \ref{cupy-template}).
As tested in section \ref{kernel-fusing} this way of creating separate kernels for different parts of this simple saxpy-operation is inefficient and comes at the cost of additional kernel-launch overheads and memory transfers.
This is the default behavior of CuPy, but CuPy offers ways to circumvent that behavior with the \texttt{cupy.fuse} function decorator as used in listing \ref{cupy-fuse}.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	firstnumber=47,
	linenos
	]{Cython}
cdef function.Function _get_simple_elementwise_kernel(
        tuple params, tuple arginfos, str operation, str name,
        _TypeMap type_map, str preamble, str loop_prep='', str after_loop='',
        tuple options=()):
    # No loop unrolling due to avoid 64-bit division
    module_code = string.Template('''
    ${typedef_preamble}
    ${preamble}
    extern "C" __global__ void ${name}(${params}) {
      ${loop_prep};
      #pragma unroll 1
      CUPY_FOR(i, _ind.size()) {
        _ind.set(i);
        ${operation};
      }
      ${after_loop};
    }
    ''').substitute(
        typedef_preamble=type_map.get_typedef_code(),
        params=_get_kernel_params(params, arginfos),
        operation=operation,
        name=name,
        preamble=preamble,
        loop_prep=loop_prep,
        after_loop=after_loop)
    module = compile_with_cache(module_code, options)
    return module.get_function(name)
\end{minted}
\captionof{listing}{underlying template \label[code]{cupy-template}}

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{Python}
@cupy.fuse(kernel_name='saxpy')
def saxpy(a,x,y):
    return a*x+y
\end{minted}
\captionof{listing}{the operation re-defined and decorated with the fuse-decorator \label[code]{cupy-fuse}}

Decorating the operation with \texttt{cupy.fuse} indicates that the kernels should be fused into a single kernel.
The resulting kernel is displayed in listing \ref{cupy-fused-kernel}.
The code cannot be mistaken for handwritten code because of the many unnecessary it includes.
The type-definitions in the device-functions \texttt{cupy\_mulitply} and \texttt{cupy\_add} are not used anywhere and similarly there is an excessive number of floats declared in the main kernel on line 39 (originally all of these declarations were on separate lines, here displayed on a single line for conciseness) then those floats are cast to other floats, which again is completely unnecessary.
Such superfluous bloat in the code however is no cause for concern, modern compilers recognize that all those definitions and type-casts are not needed and the code that gets executed will not be affected by them.


\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{C++}
#include <cupy/complex.cuh>
#include <cupy/carray.cuh>
#include <cupy/atomics.cuh>

__device__ void cupy_multiply(float &in0, float &in1, float &out0)
{
  typedef float in0_type;
  typedef float in1_type;
  typedef float out0_type;

  out0 = in0 * in1;
}

__device__ void cupy_add(float &in0, float &in1, float &out0)
{
  typedef float in0_type;
  typedef float in1_type;
  typedef float out0_type;

  out0 = in0 + in1;
}

extern "C" __global__ void
saxpy(const float v0,
      const CArray<float, 1, 1, 1> _raw_v1,
      const CArray<float, 1, 1, 1> _raw_v2,
      CArray<float, 1, 1, 1> _raw_v5,
      CIndexer<1, 1> _ind)
{
  ;
#pragma unroll 1
  CUPY_FOR(i, _ind.size())
  {
    _ind.set(i);
    const float &v1 = _raw_v1[_ind.get()];
    const float &v2 = _raw_v2[_ind.get()];
    float &v5 = _raw_v5[_ind.get()];
    // 2 operations
    float v3, v4, v0_0, v0_1, v0_2, v1_0, v1_1, v1_2;
    // op  # 0
    v0_0 = static_cast<float>(v0);
    v0_1 = static_cast<float>(v1);
    v0_2 = static_cast<float>(v3);
    cupy_multiply(v0_0, v0_1, v0_2);
    v3 = static_cast<float>(v0_2);
    // op  # 1
    v1_0 = static_cast<float>(v3);
    v1_1 = static_cast<float>(v2);
    v1_2 = static_cast<float>(v4);
    cupy_add(v1_0, v1_1, v1_2);
    v4 = static_cast<float>(v1_2);
    v5 = v4;
    ;
  };
}
\end{minted}
\captionof{listing}{the operation re-defined and decorated with the fuse-decorator \label[code]{cupy-fused-kernel}}

Another way of defining GPU calculations using CuPy is through the \texttt{RawKernel}-module as displayed in listing \ref{cupy-raw}.
This module gives developers full control over the CUDA-Code that will be compiled by the compiler and allows for a fast, responsive developer experience.
By utilizing an interactive Python environment like a Jupyter-notebook one can rapidly alter the kernel without having to set up a C/C++-Project and recompiling the whole program for every change made or losing application state.
While this kernel is no faster than the kernel automatically generated by CuPy, there is something to be said about the conciseness and simplicity of a handwritten kernel.
The complete kernel fits within a few lines of Code and is very readable.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{Python}
import cupy
import numpy as np
import math

# manually defining the kernel in C/C++
saxpy_rawKernel = cupy.RawKernel(
    """
    extern "C" __global__
    void saxpy(float a, float* x, float* y, float* z, long n) {
        long idx = (blockDim.x * blockIdx.x + threadIdx.x);
        if(idx < n){
            z[idx] = a*x[idx] + y[idx];
        }
    }
    ""","saxpy"
)

# defining a function to call the kernel with an appropriate grid
def saxpy(a,x,y,z):
    n = x.shape[0]
    blocksize = 512
    gridsize = math.ceil(n / blocksize)
    saxpy_rawKernel((gridsize,),(blocksize,),(a, x, y, z, n))

# creating inputs on the host
x_host = np.arange(10_000, dtype=np.float32)
y_host = np.arange(10_000, dtype=np.float32)

# copying them to device memory
x = cupy.asarray(x_host)
y = cupy.asarray(y_host)
a = cupy.float32(5)
z = cupy.empty_like(x)

# calling the function, after this function call z contains the result
saxpy(a,x,y,z)

# retrieving the result from the GPU
z_host = cupy.asnumpy(z)

# checking the result for correctness
np.allclose(a_host*x_host+y_host,z_host)
\end{minted}
\captionof{listing}{CuPy exampled using a handwritten CUDA-Kernel \label[code]{cupy-raw}}

The very same CUDA-Kernel can also be expressed using purely Python with a largely unchanged syntax (see listing \ref{cupy-raw-python}).
The kernel defined on lines 3-7 is a drop-in replacement for the kernel defined on lines 6-16 in listing \ref{cupy-raw} without any further changes except the needed import of cupyx.
Note the absence of any types.
The type information is inferred at run-time, which means that this code could be resulted for double-precision or integers of various.
This advantage comes at the cost of having to be very careful about the parameters passed to this function otherwise one might inadvertently work with double-precision numbers, which would slow the calculation down.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{Python}
from cupyx import jit

@jit.rawkernel()
def saxpy_kernel(a, x, y, z, n):
    idx = jit.blockDim.x * jit.blockIdx.x + jit.threadIdx.x
    if (idx < n):
        z[idx] = a * x[idx] + y[idx]
\end{minted}
\captionof{listing}{CuPy exampled using a handwritten Python-CUDA-Kernel \label[code]{cupy-raw-python}}

When using this feature CuPy warns about this functionality being experimental and the possibility of the syntax possibly changing at a future point in time.

\subsubsection{Numba}

Numba's \texttt{vectorize} and \texttt{guvectorize} decorators allow users to turn functions which operate on n-dimensional arrays into \texttt{ufuncs} (universal function) or generalized \texttt{ufuncs}.
As universal functions operate element-wise on arrays, they are embarrassingly parallel by definition and prime candidates for acceleration using GPUs \cite{Universa12:online}.
Listing \ref{numba-ufunc} demonstrates the saxpy-kernel as an ufunc.
Explicitly controlling the memory-flow between the host and the device by using functions like \texttt{numba\_cuda.to\_device} and \texttt{copy\_to\_host} is entirely optional but good practice.
The \texttt{vectorize} decorator always generates a single kernel, un-fused operations are not a concern.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{Python}
import numba
import numba.cuda
import numpy as np
import math

# defining the kernel as a typed-ufunc in Python
@numba.vectorize(
    [numba.float32(numba.float32,numba.float32,numba.float32)],
    target="cuda")
def saxpy(a, x, y):
    return a*x+y


# creating inputs on the host
x_host = np.arange(10_000, dtype=np.float32)
y_host = np.arange(10_000, dtype=np.float32)

# copying them to device memory
a = np.float32(5)
x = numba.cuda.to_device(x_host)
y = numba.cuda.to_device(y_host)
z = numba.cuda.device_array_like(x_host)

# calling the function, after this function call z contains the result
z = saxpy(a,x,y)

# retrieving the result from the GPU
z_host = z.copy_to_host()

# checking the result for correctness
np.allclose(a*x_host+y_host,z_host)
\end{minted}
\captionof{listing}{Numba exampled using an ufunc \label[code]{numba-ufunc}}

Numba also offers a path to define CUDA-Kernels (as demonstrated in listing \ref{numba-raw-python}) purely using Python-Syntax very similar to the previous CuPy-example (listing \ref{cupy-raw-python}) with the only difference being slight differences in the CUDA-specific syntax.
A Numba CUDA-kernel for exampled is called with the following syntax \texttt{kernel\_name[gridsize, blocksize, stream, shared\_memory](kernel\_arguments)} whereas a CuPy kernel is invoked using \texttt{kernel\_name(gridsize, blocksize, kernel\_arguments, shared\_memory=0)}.
There are however large differences between Numba and CuPy in the next steps towards generating executable code out of that Python code describing a kernel.
CuPy generates C/C++-code, which is very close to the original Python kernel and easily inspectable, from the kernel, unlike Numba, which generates a series of intermediate representation including NVVM-IR \cite{NVVMIRCU80:online} (a subset of the LLVM-IR designed to represent GPU-Kernels) and PTX-Code \cite{PTXISACU80:online} (Parallel Thread Execution).
Both of these formats are the assembly languages for two different virtual machines and very-low level (essentially as low level as the x86-assembly language), for example, these languages lack the concepts of for-loops.
For-loops and other features developers have grown to be accustomed to can only be expressed as conditional-jumps between different sections of the program.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{Python}
import numba
import numba.cuda
import numpy as np
import math

# defining the kernel in Python
@numba.cuda.jit()
def saxpy_kernel(a, x, y, z, n):
	# alternatively:
	# idx = numba.cuda.blockDim.x * numba.cuda.blockIdx.x + numba.cuda.threadIdx.x
    idx = numba.cuda.grid(1)
    if (idx < n):
        z[idx] = a * x[idx] + y[idx]

# defining a function to call the kernel with an appropriate grid
def saxpy(a,x,y,z):
    n = x.shape[0]
    blocksize = 512
    gridsize = math.ceil(n / blocksize)
    saxpy_kernel[gridsize,blocksize](a,x,y,z,n)


# creating inputs on the host
x_host = np.arange(10_000, dtype=np.float32)
y_host = np.arange(10_000, dtype=np.float32)

# copying them to device memory
a = np.float32(5)
x = numba.cuda.to_device(x_host)
y = numba.cuda.to_device(y_host)
z = numba.cuda.device_array_like(x_host)

# calling the function, after this function call z contains the result
saxpy(a,x,y,z)

# retrieving the result from the GPU
z_host = z.copy_to_host()

# checking the result for correctness
np.allclose(a*x_host+y_host,z_host)
\end{minted}
\captionof{listing}{Numba exampled using a handwritten CUDA-Kernel \label[code]{numba-raw-python}}

Listing \ref{numba-PTX} displays a section of the resulting PTX-Code.
And while it is entirely possible to reason about and convince oneself of the correctness of this code, it is a very time-consuming process and quickly becomes infeasable as more complex kernels (usually the compiler would transform the code more heavily, only very simple examples have this almost perfect injective mapping between lines written in the kernel and PTX-commands) are examined.
As a consequence of this Numba is very opaque to users.
The actual computation takes place on the highlighted line 42, the entire saxpy-function compiled down to a single FMA-instruction.
Comments were added to explain the commands.

To understand the section presented it is also necessary to know in which registers the passed arguments were stored in.
\begin{itemize}
	\item a is stored in \texttt{\%f1}
	\item a pointer to x is stored in \texttt{\%rd2}
	\item a pointer to y is stored in \texttt{\%rd4}
	\item a pointer to z is stored in \texttt{\%rd6}
	\item n is stored in \texttt{\%rd8}
\end{itemize}

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	obeytabs=true,
	linenos,
	highlightlines = 42
	]{text}
# read threadIdx.x into register r1
mov.u32					%r1, 	%tid.x;
# read blockIdx.x into register r2
mov.u32 				%r2, 	%ctaid.x;
# read blockDim.x into register r3
mov.u32					%r3, 	%ntid.x;
# calculate blockDim.x * blockIdx.x + threadIdx.x and store result in r4
mad.lo.s32     				%r4, 	%r3, 	%r2, 	%r1;
# cast r4 to signed 64-bit integer and store result in rd1
cvt.s64.s32     			%rd1, 	%r4;
# checks whether rd1 is greater than rd8, if it p1 becomes true
setp.ge.s64     			%p1, 	%rd1, 	%rd8;
# jump to label if p1 is true
@%p1 bra     				$L__BB0_2;

# casting rd1 to unsigned 32-bit integer and storing result in r5
cvt.u32.u64				%r5,	%rd1;
# set p2 if r5 is less than zero
setp.lt.s32				%p2,	%r5,	0;
# set rd9 to rd3 or 0 depending on p2
selp.b64				%rd9,	%rd3,	 0,	%p2;
# add rd9 and rd1 into rd10
add.s64					%rd10,	%rd9,	 %rd1;
# cast to global address space
cvta.to.global.u64			%rd11,	%rd2;
# shift left by 2 bit and store result in rd12
shl.b64					%rd12,	%rd10,	 2;
# add rd12 and rd11 into rd13
add.s64					%rd13,	%rd11,	 %rd12;
# load the contents from rd13 into the register f2
ld.global.f32				%f2,	[%rd13];
# the next line perform a similar calculation memory addresses to get y[idx]
selp.b64				%rd14,	%rd5,	 0,	%p2;
add.s64					%rd15,	%rd14,	 %rd1;
cvta.to.global.u64			%rd16,	%rd4;
shl.b64					%rd17,	%rd15,	 2;
add.s64					%rd18,	%rd16,	 %rd17;
ld.global.f32				%f3,	[%rd18];
# here the fused multiply-add is performed
# f2 contains x[idx], f1 is alpha and f3 is y[idx]
# the result is stored in f4
fma.rn.f32				%f4,	%f2,	 %f1, 	%f3;
# followed by a calculation where to store the result in global memory
selp.b64				%rd19,	%rd7,	 0,	%p2;
add.s64					%rd20,	%rd19,	 %rd1;
cvta.to.global.u64			%rd21,	%rd6;
shl.b64					%rd22,	%rd20,	 2;
add.s64					%rd23,	%rd21,	 %rd22;
# finally f4 is stored in z[idx]
st.global.f32				[%rd23],%f4;
\end{minted}
\captionof{listing}{Section of the resulting PTX-Code \label[code]{numba-PTX}}

\subsubsection{PyCUDA}

PyCUDA does not offer as much as functionality CuPy or Numba: it does not support expressing CUDA-Kernels using purely Python and it has no abstractions over CUDA (like shown with CuPy in listing \ref{cupy-naive} or with Numba in listing \ref{numba-ufunc}).
What it does offer is a functionality very similar to that of CuPy shown in listing \ref{cupy-raw}, CUDA C/C++-Kernels defined in Python strings, that are compiled just-in-time and a truly complete set of bindings.

The syntax used is slightly different from the one used in CuPy.
An example containing the saxpy-kernel is presented in listing \ref{pycuda-raw}.
Compared to CuPy it is a more verbose API, that closely follows the C/C++ API, one can for example allocate memory without associating a shape or datatype associated with it (line 37) or use a more convenient way of directly copying an existing Numpy array to the device memory (line 38).
The way kernels are launched is also slightly different, with the grid- and blockdimension being passed to the function as keyword-arguments.
Performance-wise there are no differences between this library and CuPy, since both libraries are just thin wrappers around the CUDA-API.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{Python}
import pycuda.driver as cuda
import pycuda.autoinit
from pycuda.compiler import SourceModule
import numpy as np
import math

# defining the kernel
mod = SourceModule(
    """
    extern "C" __global__
    void saxpy(float a, float* x, float* y, float* z, long n) {
        long idx = (blockDim.x * blockIdx.x + threadIdx.x);
        if(idx < n){
            z[idx] = a*x[idx] + y[idx];
        }
    }
    """
)

saxpy_kernel = mod.get_function("saxpy")

def saxpy(a, x, y, z):
    n = np.int64(x_host.shape[0])
    blocksize = 512
    gridsize = math.ceil(n / blocksize)
    saxpy_kernel(a, x, y, z, n, block=(blocksize, 1, 1), grid=(gridsize, 1, 1))


# creating inputs on the host
a = np.float32(5)
x_host = np.arange(10_000, dtype=np.float32)
y_host = np.arange(10_000, dtype=np.float32)
z_host = np.empty(10_000, dtype=np.float32)

# copying them to device memory
a = np.float32(5)
x = cuda.mem_alloc(x_host.nbytes)
y = pycuda.gpuarray.to_gpu(y_host)
z = cuda.mem_alloc(x_host.nbytes)
cuda.memcpy_htod(x, x_host)

# calculating the result on the GPU
saxpy(a, x, y, z)

# retrieving the result from the GPU
cuda.memcpy_dtoh(z_host, z)

# verifiying the result
np.allclose(z_host, a*x_host + y_host)
\end{minted}
\captionof{listing}{source code of the saxpy-kernel using PyCuda \label[code]{pycuda-raw}}

\subsubsection{PyOpenCL}

PyOpenCL takes an approach different from all the other introduced packages.
Instead of targeting GPUs via the CUDA or ROCm ecosystems, this package targets the OpenCL platform, which enables it to support GPUs from all three major vendors and many CPUs.
OpenCL uses similar concepts as CUDA, but a different terminology (as listed in table \ref{CUDA-OPENCL-Terminoloy}).
The most striking difference is the use of the term "shared memory" in CUDA and "local memory" in OpenCL for the same concept.
Both of these terms refer to memory that is shared between threads or work-item within a block or work-group, from a different viewpoint the memory is not shared between the threads but local to a SM and its set of resident workers.

\begin{table}[H]
	\centering
	\begin{tabular}{l|ll} \toprule
		Category  & CUDA term       & OpenCL term     \\ \midrule
		Execution & Kernel          & Kernel          \\
		{}        & Thread          & Work-item       \\
		{}        & Warp            & Wavefront       \\
		{}        & Block           & Work-group      \\
		{}        & Grid            & N-D Range       \\
		{}        & Stream          & Command Queue   \\ \midrule
		Memory    & Global Memory   & Global Memory   \\
		{}        & Device Memory   & Device Memory   \\
		{}        & Constant Memory & Constant Memory \\
		{}        & Shared Memory   & Local Memory    \\
		{}        & Local Memory    & Private Memory  \\
		\bottomrule
	\end{tabular}
	\caption{Terms of equal meaning in CUDA and OpenCL adapted from \cite{HARVEY20111093}}
	\label{CUDA-OPENCL-Terminoloy}
\end{table}

Listing \ref{pyopencl-raw} shows the saxpy-kernel implemented using this library.
The basic structure (defining the kernel(s), copying data to the device, executing the kernel and retrieving the result from the device) of the code is unchanged, only the way those operations are described has changed slightly.


\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{Python}
import numpy as np
import pyopencl as cl

ctx = cl.create_some_context()
queue = cl.CommandQueue(ctx)

# defining and compiling the kernel
prg = cl.Program(ctx, """
__kernel void saxpy(
    const float a,
	__global const float *x,
	__global const float *y,
	__global float *z)
{
  int gid = get_global_id(0);
  z[gid] = a*x[gid] + y[gid];
}
""").build()

knl = prg.saxpy


# creating inputs on the host
a = np.float32(5.0)
x_host = np.arange(10_000, dtype=np.float32)
y_host = np.arange(10_000, dtype=np.float32)
z_host = np.empty(10_000, dtype=np.float32)

# copying them to device memory
mf = cl.mem_flags
x = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=x_host)
y = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=y_host)
z = cl.Buffer(ctx, mf.WRITE_ONLY, z_host.nbytes)

# calculating the result on the GPU
global_size = x_host.shape[0]
local_size = 1
knl(queue, (global_size,), (local_size,), a, x, y, z)

# retrieving the result from the GPU
cl.enqueue_copy(queue, z, z_host)

# verifiying the result
np.allclose(z_host, a*x_host + y_host)
\end{minted}
\captionof{listing}{source code of the saxpy-kernel using PyOpenCL \label[code]{pyopencl-raw}}

Since this package does not use CUDA, we can not make any assumptions about the kernel's performance.
When using a library that targets CUDA we can be sure that the kernel always executes equally fast, no matter which package interfaces with the device driver's API, we only need to profile it once using one of the profiling tools CUDA provides us with (Nsight Compute and Nsight Systems).

When utilizing OpenCL those profilers cannot be used and to our knowledge there are no comparable profiler tools for OpenCL used with Nvidia hardware (for AMD GPUs there is rocprofiler \cite{GitHubRO33:online}) available.
This however does not imply that OpenCL code cannot be profiled at all, the kernel-calls such as the one on line 38 of listing \ref{pyopencl-raw} returns a data-structure, which contains information about the time the task was queued, started and finished.
Another possible work-around takes advantage of the fact that OpenCL-toolchain also produces an intermediate representation, which can be output to a file and analyzed.
One could take that PTX-code, insert it into a CUDA program and profile the resulting program.

\paragraph{Porting a non-trivial kernel to OpenCL}\mbox{}\\

To get a sense of the performance we translate the kernel used for the first (1D forward FFT of the rows of an image using shared/local memory) stage of the program discussed in section \ref{opencv_demo}.
The process of porting consists of adapting the already written CUDA-Kernels by changing specific syntax (like swapping the idiomatic \texttt{int tid = blockDim.x * blockIdx.x + threadIdx.x} to \texttt{int gid = get\_global\_id(0)}) and making sure to replace all C++ features like templates, classes (including operator overloading) and lambda-functions.
For the function the kernel calls (referred to as device-functions) only very few changes are needed (as displayed in listings \ref{device_function_cuda} and \ref{device_function_opencl}).
The qualifier \texttt{\_\_device\_\_} is not needed in OpenCL, since re-use between device and host code is not supported in OpenCL and OpenCL-Code is usually defined in its own source file and always JIT compiled to achieve portability.
Aside from that there were slight modifications needed in the \texttt{raise\_omega}-function, the interface of the \texttt{sincos} functions are not identical and OpenCL does not support taking the addresses of a vector elements (as done in the CUDA-Code).

\begin{minipage}[t]{.48\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true
	]{C}
/*
Function to multiply the two complex numbers x1 and x2
The result is stored in x1, which is why x1 needs to be passed by reference
*/
__device__ inline void complex_mult(float2* x1, float2 x2)
{
    float real_result = x1->x*x2.x-x1->y*x2.y;
    float imag_result = x1->x*x2.y+x2.x*x1->y;
    x1->x = real_result;
    x1->y = imag_result;
}

/*
Function to raise a complex number with unit-magnitude by an integer power.
Magnitude remains unchanged, phase is multiplied by the exponent
*/
__device__ inline float2 raise_omega(float2 omega, int pow)
{
    float angle = atan2(omega.y, omega.x);
    angle *= pow;
    float2 retval;
    sincosf(angle, &retval.y, &retval.x);
    return retval;
}
\end{minted}
	\captionof{listing}{used device functions for CUDA \label[code]{device_function_cuda}}
\end{minipage}\hfill%
\begin{minipage}[t]{.48\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true
	]{C}
/*
Function to multiply the two complex numbers x1 and x2
The result is stored in x1, which is why x1 needs to be passed by reference
*/
void complex_mult(float2 *x1, float2 x2)
{
    float real_result = x1->x*x2.x-x1->y*x2.y;
    float imag_result = x1->x*x2.y+x2.x*x1->y;
    x1->x = real_result;
    x1->y = imag_result;
}

/*
Function to raise a complex number with unit-magnitude by an integer power.
Magnitude remains unchanged, phase is multiplied by the exponent
*/
float2 raise_omega(float2 omega, int pow)
{
    float angle = atan2(omega.y, omega.x);
    angle *= pow;
    float cos;
    float sine = sincos(angle, &cos);
    return (float2)(cos,sine);
}
\end{minted}
	\captionof{listing}{used device functions for OpenCL \label[code]{device_function_opencl}}
\end{minipage}%

Porting the kernel itself is also a rather straightforward process of adapting syntax and finding equivalent functions in the respective libraries, the changed section are contrasted in listings \ref{kernel_function_cuda} and \ref{kernel_function_opencl}.
The first and most important change between the two snippets lies in the fact that the CUDA-function is a templated-function, which enables it to express the forward and backward transform with the same function controlled by the boolean parameter \texttt{inverse}.
This is not possible in OpenCL and would instead have to be implemented using multiple functions or templating the Python-string, representing the function, at run-time.
Further changes affect platform specific syntax like \texttt{barrier(CLK\_LOCAL\_MEM\_FENCE)} instead of \texttt{\_\_syncthreads()} and \texttt{get\_local\_id(0)} instead of \texttt{threadIdx.x}.

\begin{minipage}[t]{.48\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true
	]{C++}
template <unsigned int size, bool inverse>
__global__ void my_fft_u2c(
	const unsigned char * input,
	float *output,
	int original_size,
	float omega_real,
	float omega_imag)
{
  __shared__ float t1[size * 2];
  __shared__ float t2[size * 2];

  int tid = blockDim.x*blockIdx.x+threadIdx.x;
  int iteration = 0;

  t1[threadIdx.x*4] = input[tid*2];
  t1[threadIdx.x*4+1] = 0;
  t1[threadIdx.x*4+2] = input[tid*2+1];
  t1[threadIdx.x*4+3] = 0.;
  ...
  float2 omega = make_float2(omega_real, omega_imag);
  float2 twiddle = raise_omega(omega, (inverse?1:-1) * posInProb * n_Problems);
  __syncthreads();
  ...
  output[tid*4] = input_[threadIdx.x*4] * (inverse ? 1. / size : 1);
}
\end{minted}
	\captionof{listing}{kernel expressed in CUDA \label[code]{kernel_function_cuda}}
\end{minipage}\hfill%
\begin{minipage}[t]{.48\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true
	]{C}
__kernel void my_fft_u2c(
    __global const unsigned char *input,
    __global float *output,
    int original_size,
    float omega_real, 
    float omega_imag)
{
  __local float t1[1024 * 2];
  __local float t2[1024 * 2];

  int gid = get_global_id(0);
  int iteration = 0;

  t1[get_local_id(0)*4] = input[gid*2];
  t1[get_local_id(0)*4+1] = 0;
  t1[get_local_id(0)*4+2] = input[gid*2+1];
  t1[get_local_id(0)*4+3] = 0;
  ...
  float2 omega = (float2)(omega_real, omega_imag);
  float2 twiddle = raise_omega(omega, -posInProb*n_Problems);
  barrier(CLK_LOCAL_MEM_FENCE);
  ...
  output[gid*4] = input_[get_local_id(0)*4];
  ...
}
\end{minted}
	\captionof{listing}{modified sections of the kernel for OpenCL \label[code]{kernel_function_opencl}}
\end{minipage}%

\paragraph{Performance comparison between OpenCL and CUDA}\mbox{}\\

After having ported the FFT-Kernel it can be tested and benchmarked using PyOpenCL.
The performance of the CUDA-Code is listed in table \ref{opencv_profile} as 155  $\mu\text{s}$ for a 1024 by 1024 input image, whereas the OpenCL-Code reports an execution time of 217 $\mu\text{s}$.
This performance regression by about 40\% can most-likely be attributed to the OpenCL compiler producing less optimized PTX-Code than NVCC.

To confirm this suspicion, the generated PTX-Code can be retrieved and integrated into a CUDA program.
This allows us to profile both PTX-Codes (the one generated by OpenCL and the one generated by the Nvidia Compiler) with the same tools for reliable information.
This experiment showed that the the regression in performance can in fact be attributed to worse PTX-Code being generated by the OpenCL toolchain, the kernel also takes around 220 $\mu\text{s}$ when executed using the CUDA-API.

\begin{figure}[H]
	\centering
	\includegraphics[width = 12cm]{../comparing_gpu_packages/performance_comparison_ptx/comparison.pdf}
	\caption{Comparing the run-times of the equivalent FFT-Kernels, both times were measured using Nsight Systems}
	\label{opencl-cuda-comparison}
\end{figure}

Using Nsight Compute for a detailed analysis of both resulting kernels, which should perform exactly the same computations, reveals that they are structured differently (note: Nsight Compute cannot be used to accurately measure the run-times of the kernels due to its profiling overhead, Nsight Systems is able to perform this analysis).
The version compiled using the OpenCL toolchain made much less use of fused FP32 operations like FMA (fused-multiply and addtion), which increased the overall number of executed instructions by 37.52 \% and slowed the program down accordingly.
Similar behavior could be observed whenever the PTX-Code is compiled by a different toolchain (e.g. NVCC, NVRTC, Clang, Numba, OpenCL).
Even the same source code (or in this case almost identical versions expressed in slightly different dialects of C) is not guaranteed or likely to result in the same PTX-Code or performance.

This however does not imply that OpenCL is always worse than equivalent CUDA code.
It is entirely possible that the OpenCL-Compiler could create PTX-Code as fast by employing additional optimization techniques.

% TODO: 
% compiler trajectory
% instruction distribution

\subsubsection{PyTorch}

Pytorch allows the creation and movement of its n-dimensional arrays (referred to as tensors) between different devices like the CPU or GPUs.
This approach of using PyTorch as a GPU-aware drop-in replacement to NumPy is very similar to using CuPy for this task.
Operations between Tensors located on the same GPU are automatically executed on that GPU using CUDA-Code.
The saxpy example shown in listing \ref{pytorch-saxpy} executes three kernels:

\begin{itemize}
	\item{an arange-kernel to populate the $\mathbf{x}$-tensor}
	\item {a kernel to multiplies the $\mathbf{x}$ and $a$-tensors}
	\item {a kernel to add the result of $a\mathbf{x}$ and $\mathbf{y}$}
\end{itemize}

So just like CuPy it executes several separate elementwise-kernels, which could easily be fused to improve efficiency and computational intensity.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{Python}
import torch
import numpy as np

# creating inputs on the device
x = torch.arange(10_000, device="cuda:0", dtype=torch.float32)

# creating input on the host and copying it to the device
y_host = np.arange(10_000, dtype=np.float32)
y = torch.tensor(y_host, device="cuda")
# a is a tensor on the default device (the cpu)
a = torch.tensor(5.0)


def saxpy(a,x,y):
    return a*x+y

# calculating the result on the GPU
# the function itself is device-agnostic and 
# calls either CPU- or device-Code depending on its inputs.
z = saxpy(a, x, y)

# copying result back to CPU and converting to Numpy
z_host = z.to(device="cpu").numpy()

# verifiying result
np.allclose(a.numpy()*x.to("cpu").numpy() + y_host, z_host)
\end{minted}
\captionof{listing}{source code of the saxpy-kernel using PyTorch \label[code]{pytorch-saxpy}}

With PyTorch the solution to unfused kernels is called TorchScript.
TorchScript is a JIT-Compiler, which can be used to either trace PyTorch programs once and export them as a standalone module for use in C++-programs without any Python involved or to compile functions as they are called to benefit from compile-time optimizations.
TorchScript is used by decorating the function.
This use-case of TorchScript is showcased in listing \ref{pytorch-saxpy-TorchScript}.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{Python}
@torch.jit.script
def saxpy_jit(a,x,y):
    return a*x+y
\end{minted}
\captionof{listing}{source code of the saxpy-kernel using PyTorch \label[code]{pytorch-saxpy-TorchScript}}

When executing a Python file containing such a decorated function, one can set the environment-variable \texttt{PYTORCH\_JIT\_LOG\_LEVEL} to have debug-information about the JIT-Compilation process reported.
To receive the resulting CUDA-C++ one can set the variable to \texttt{>>cuda\_codegen}.
This instructs PyTorch to log all the debug information related to the generation of the CUDA kernel (displayed in listing \ref{pytorch-saxpy-fused}).
This kernel is properly fused (both multiplication and addition is performed in this kernel as visible on line 8) and should map to the efficient FMA-instruction (confirmed by profiling the kernel with Nsight Compute and examining the instructions executed).
Interestingly the code avoids defining the idiomatic global thread-id like \texttt{int tid = blockDim.x * blockIdx.x + threadIdx.x} and instead accesses the \texttt{blockIdx.x} and \texttt{threadIdx.x} multiple times over the course of the program and never accesses \texttt{blockDim.x}, but hard-codes it as 512.
This is not an issue, since the compiler is smart enough to optimize away the repeating accesses to \texttt{threadIdx.x} and \texttt{blockIdx.x} and the fixed \texttt{blockDim.x} is also not an issue, because the Kernel is only ever compiled for this set of inputs without any consideration for re-use and generality.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{C}
extern "C" __global__
void fused_mul_add(float* ty_1, float* ta_1, float* tx_1, float* aten_add) {
	{
	if ((long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)<10000ll ? 1 : 0) {
	    float v = __ldg(ta_1 + 0ll);
    float v_1 = __ldg(tx_1 + (long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x));
    float v_2 = __ldg(ty_1 + (long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x));
    aten_add[(long long)(threadIdx.x) + 512ll * (long long)(blockIdx.x)] = v * v_1 + v_2;
  }}
}
\end{minted}
\captionof{listing}{source code of the saxpy-kernel using PyTorch \label[code]{pytorch-saxpy-fused}}

Note: the fact that the data type \texttt{long long} is used in listing \ref{pytorch-saxpy-fused} is not of significance.
When using the NVCC compiler this is an 8 byte integer, equivalent to a standard \texttt{long}.
The same behavior is observed when using g++ or clang on regular C/C++ programs.
This is compliant with the standards, which only guarantee \texttt{long long} to be as large as \texttt{long} and at least 8 bytes \cite{Fundamen16:online}.

\subsubsection{JAX}

JAX is conceptually very similar to other frameworks designed for deep-learning frameworks like PyTorch or TensorFlow.
Besides the use in the machine-learning domain it also duplicates Numpy's functionality and can be used as a GPU and TPU accelerated library for scientific computing.
Major difference are the way JAX handles devices memory (by default 90\% of the available device memory is pre-allocated at startup) and the precision of floating point numbers (by default single precision is \emph{enforced} and extra steps are needed to enable double-precision support, this choice is owed to the limited device memory and performance disparity between single and double-precision floating point numbers on GPUs \cite{JAXThe45:online}).
Listing \ref{jax-saxpy} outlines how the saxpy-kernel could be defined using JAX.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true,
	linenos
	]{Python}
import jax
import os
import numpy as np

# creating inputs on the device
# no device is specified, JAX always uses the GPU or TPU if available
x = jax.numpy.arange(10_000, dtype=jax.numpy.float32)
y = jax.numpy.arange(10_000, dtype=jax.numpy.float32)
a = jax.numpy.float32(5)

def saxpy(a,x,y):
    return a*x+y

# calculating the result on the GPU
# the function itself is device-agnostic and 
# calls either CPU- or device-Code depending on its inputs.
z = saxpy(a,x,y)

# verifiying result
# copying results back to the host is done implicitly 
np.allclose(z, a*x+y)
\end{minted}
\captionof{listing}{source code of the saxpy-kernel using JAX \label[code]{jax-saxpy}}

Just like PyTorch, JAX also compiles two kernels for the function call of \texttt{saxpy}, one for the multiplication of \texttt{a$\cdot$x} and one for the addition of \texttt{y}.
In fact JAX has no way of knowing that the multiplication will be followed by an addition at the time the bytecode to multiply \texttt{a} and \texttt{x} is executed (the path taken by the CPython interpreter is very similar to the outline in figure \ref{py_mult_trace}).
Interestingly those kernels consist of 10 blocks with 256 threads each, so only 2560 threads are launched for 10000 multiplications and additions respectively.
This works because the kernels JAX compiled are vectorized, each thread is responsible for 4 operations and makes use of special vectorized instructions to load and store 4 floats at a time. This is beneficial because a lower number of warps has to be schedule, which is associated with a lower overhead and the vectorized memory accesses are very efficient. 
The downside of this approach is a lower occupancy caused by the smaller number of threads, especially on these relatively small inputs.
A section of the generated PTX-Code is presented in listing \ref{jax-ptx-mul}, with the vectorized section being highlighted.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	firstnumber=32,
	breaklines=true,
	obeytabs=true,
	highlightlines={42-48},
	linenos
]{text}
ld.param.u64 		%rd4, [fusion_param_0];
ld.param.u64 		%rd5, [fusion_param_2];
cvta.to.global.u64 	%rd1, %rd5;
ld.param.u64 		%rd6, [fusion_param_1];
cvta.to.global.u64	%rd2, %rd6;
cvta.to.global.u64 	%rd3, %rd4;
ld.global.nc.u32 	%r6, [%rd3];
cvt.rn.f32.s32 		%f1, %r6;
mul.wide.u32 		%rd7, %r1, 4;
add.s64 		%rd8, %rd2, %rd7;
ld.global.nc.v4.f32 	{%f2, %f3, %f4, %f5}, [%rd8];
mul.rn.f32 		%f6, %f2, %f1;
add.s64 		%rd9, %rd1, %rd7;
mul.rn.f32 		%f7, %f3, %f1;
mul.rn.f32 		%f8, %f4, %f1;
mul.rn.f32 		%f9, %f5, %f1;
st.global.v4.f32 	[%rd9], {%f6, %f7, %f8, %f9};
\end{minted}
\captionof{listing}{generated PTX-code of the multiplication kernel \label[code]{jax-ptx-mul}}

To receive a single, fused kernel the function has to be decorated with \texttt{jax.jit}.
The relevant section of the resulting kernel is shown in listing \ref{jax-ptx-fuse}. In the resulting code the \texttt{mul} and \texttt{add} instructions are both present in the same kernel but unlike PyTorch, JAX failed to converted them to FMA instructions, which would be a low-hanging optimization in this context.

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	firstnumber=32,
	breaklines=true,
	obeytabs=true,
	highlightlines={40-55},
	linenos
]{text}
ld.param.u64 	%rd5, [fusion_param_0];
ld.param.u64 	%rd6, [fusion_param_3];
cvta.to.global.u64 	%rd1, %rd6;
ld.param.u64 	%rd7, [fusion_param_1];
ld.param.u64 	%rd8, [fusion_param_2];
cvta.to.global.u64 	%rd2, %rd8;
cvta.to.global.u64 	%rd3, %rd7;
cvta.to.global.u64 	%rd4, %rd5;
ld.global.nc.f32 	%f1, [%rd4];
mul.wide.u32 	%rd9, %r1, 4;
add.s64 	%rd10, %rd3, %rd9;
ld.global.nc.v4.f32 	{%f2, %f3, %f4, %f5}, [%rd10];
mul.rn.f32 	%f6, %f1, %f2;
add.s64 	%rd11, %rd2, %rd9;
ld.global.nc.v4.f32 	{%f7, %f8, %f9, %f10}, [%rd11];
add.rn.f32 	%f11, %f6, %f7;
add.s64 	%rd12, %rd1, %rd9;
mul.rn.f32 	%f12, %f1, %f3;
add.rn.f32 	%f13, %f12, %f8;
mul.rn.f32 	%f14, %f1, %f4;
add.rn.f32 	%f15, %f14, %f9;
mul.rn.f32 	%f16, %f1, %f5;
add.rn.f32 	%f17, %f16, %f10;
st.global.v4.f32 	[%rd12], {%f11, %f13, %f15, %f17};
\end{minted}
\captionof{listing}{generated PTX-code of the fused kernel \label[code]{jax-ptx-fuse}}

\subsubsection{Official Python bindings}

\subsection{Taxonomy of Packages}

The discussed libraries operate on different levels of abstraction and stages of the compilation pipeline.
Some of them (PyCUDA, PyOpenCL, the official cuda-python bindings) do not offer the functionality to compile Python code to GPU-executable code at all, but just forward C/C++ to the respective compiler and return functions callable from within Python.
Figure \ref{cuda-python-trajectories} outlines how the compilation process of the remaining packages is designed.
The packages can be categorized into two distinct groups.
CuPy and PyTorch emit C/C++ and rely on NVCC to perform the later stages of the compilation process (compiling it to an intermediate representation, performing numerous optimization passes on that IR, and finally emitting an executable binary), whereas Numba and JAX forego the step of creating C/C++ representations and instead directly produce LLVM-IR compatible code, which is then injected into the regular NVCC compilation path at a later stage.

The importance of the LLVM project for modern compilers cannot be overstated.
As NVCC itself is also based on LLVM \cite{CUDALLVM96:online}, there is no path to compile CUDA Code without depending on components of LLVM.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{figures/compilation_trajectory.pdf}
	\caption{High-level overview over the compilation path of the discussed libraries for Nvidia targets; PyCUDA, PyOpenCL and the official python-cuda bindings were omitted from this graphic since those package do not offer compiling capabilities}
	\label{cuda-python-trajectories}
\end{figure}

\subsection{Gauging their popularity}

Table \ref{download_figures} compares the number of downloads of the discussed packages as retrieved from PyPI (Python Package Index) for the 6 month period ranging from February 2022 through July 2022.
This is helpful to get a rough idea of the relative popularity of the individual packages.
Numpy is included as a baseline, as virtually every Python installation used for scientific computing will have Numpy installed.
The packages Numpy, Numba, torch and jax, which are the most downloaded packages on this list, are not exclusively used for GPU-accelerated usage.
Numba, Pytorch and Jax can all be used without having access to a GPU and Numpy is designed solely for calculations on the CPU.

Of the remaining packages, which are solely used for accelerating workloads with GPUs, PyOpenXl is downloaded most often, suggesting a number of popular programs rely on it over the very similar PyCuda to support the wider range of device OpenCL is capable of targeting.

The relationship between the different flavors of CuPy-packages (ROCm and CUDA) is also of great interest.
The version supporting exclusively CUDA is downloaded roughly 12 times as often as the alternative version which supports both Nvidia and AMD hardware.
This is most likely due to the fact that the ROCm-version is in an experimental state of development.

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\input{../package_comparison/downloads/package_downloads.tex}
	\caption{Download figures for the discussed figures from PyPi}
	\label[table]{download_figures}
\end{table}

Note that not PyPI is not the only repository for Python packages and the presented numbers therefore do not represent the absolute usage numbers of those pacakges.
Additionally packages can also be installed by downloading the sources manually.
Nvidia's package containing the official Python bindings (listed in the table, PyPI and Github as \texttt{cuda-python}) for example do not even list retrieving the package from PyPI as a method to install the package on the projects Github page, even though Nvidia publishes the package on the Index \cite{GitHubNV49:online}.

\subsection{Interoperability between the frameworks}

After having discussed a variety of packages and libraries which facilitate access to the GPU using the Python language it is important to clarify that users do not necessarily have to choose a single preferred package out of the available options.
This section will introduce the two widely used interfaces used to exchange data residing on the GPU between the different libraries.
It is for example possible to pass an object created with CuPy to a GPU-function that was defined by using Numba functionality (or vice-versa).

The two most important of those interface are the cuda array interface \cite{CUDAArra5:online} and DLPack \cite{Welcomet97:online}.

\subsubsection{The CUDA Array Interface}

The cuda array interface is a dictionary attribute of an object (stored under the name \texttt{\_\_cuda\_array\_interface\_\_}), which contains information about data stored on a CUDA-capable GPU. This dictionary must provide the following information:

\begin{itemize}
	\item a tuple describing the shape of the n-dimensional array
	\item a typestring defining the type of the individual elements
	\item a pointer to the first element of the data (since Python lacks the concept of fixed size pointers this information is stored as a plain integer)
\end{itemize}

This is where an important concept of low-level programming becomes visible: Data in memory has no inherent meaning, but can be interpreted in different ways. It is entirely up to the user if an array of floating point numbers in memory should be treated as a one-dimensional, "flat list" of floating point numbers or as an n-dimensional grid of complex numbers each representated by two floating point numbers.

In optional entries the dictionary may also contain information about the strides (the numbers of bytes to skip to access neighboring elements in different dimensions) of the n-dimensional array and the stream in which the data may be altered and accessed to allow for synchronization with that stream before accessing the data. See Numbas documentation on the topic \cite{CUDAArra5:online} for the detailed explanation of the interface.

The cuda array interface is supported by the following discussed packages (and a number of packages not discussed): Numba, CuPy, Jax, PyCUDA.
\subsubsection{DLPack}
DLPack is very similar to the aforementioned cuda array interface with the addition of also containing information about the device and context the data is associated with. I.e. it is applicable to a wide range of environments including, but not limited, to accelerated computing using CUDA, it also supports describing data residing within CPU memory, on GPUs in non-CUDA contexts (like OpenCL, ROCm or SYCL) or on DSPs and FPGAs.

A number of popular packages integrate DLPack in their offerings including Numpy, CuPy and PyTorch.

\subsection{OpenCV Demonstration Program using Python}
\label{opencv-demo-python}

Even though many libraries, which utilize GPUs support either DLPack, the Cuda Array Interface or both, some of them choose to support neither.
One of those libraries is OpenCV.
As laid out in section \ref{opencv-compilation} OpenCV can be configured to provide Python bindings, this enables users to use OpenCV from within Python scripts.
This allows us to express the C++ program introduced in section \ref{opencv_demo} as a Python script.

This change from C++ to Python promises the well-known benefits of the Python language and its ecosystem:

\begin{itemize}
	\item Interactie Jupyter notebooks enabling fast development cycles
	\item No explicit compilation step
	\item Less set-up code, compact notation
\end{itemize}

This section will compare and contrast the C++ and Python version of the demonstration program and showcase how to adapt libraries, which support neither DLPack nor the Cuda Array Interface.

\subsubsection{Creation of matrices in the device memory}

Listings \ref{Creation ocv cpp} and \ref{Creation ocv python} demonstrate how to allocate matrices of a specific size on the GPU within the OpenCV context using C++ and Python respectively.
There is a fundamental difference in the construction of that matrix: In C++ one can choose to allocate the memory with a different library (in this case thrust was used) and hand the preallocated chunk of memory to OpenCV.
This ensures that the memory, the matrix will be stored in, is actually a single continious area of memory, which is an assumption we made when writing the CUDA kernels like the Stockham FFT kernel in listing \ref{fft_dif_it_cuda_shared}. By default OpenCVs \texttt{GpuMat}s are not necessarily continious but every row is aligned in memory depending on the specific hardware \cite{OpenCVcv92:online}, this is not of great concern when dealing with rows, which have a known power-of-2 size, but in more general cases this has to be kept in mind.

In Python this practice of pre-allocating memory is not possible.
While we have access to libraries which could allocate memory on the device (CuPy and PyTorch among others), it is not possible to pass the address of that memory to the \texttt{GpuMat} constructor, since that specific constructor signature (note how the prefix \texttt{CV\_WRAP} is missing on line 137 of the header file \texttt{cuda.hpp} \cite{opencvcu60:online}; The details of this mechanism are discussed in the OpenCV documentation \cite{OpenCVHo82:online}) is not exposed to the Python bindings.

\begin{minipage}[t]{.49\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true
	]{CPP}
int bsize = mx * my; // image size
// allocating array of type float2 for the specified number of pixels 
// on the device
thrustDvec<float2> dev_b_pre(bsize);
// creating opencv matrix from the allocated memory
// this gives meaning to the memory
cuda::GpuMat dev_b(my, mx, CV_32FC2, dev_b_pre.data().get());
\end{minted}
	\captionof{listing}{Creation of device matrices in C++ \label{Creation ocv cpp}}
\end{minipage}\hfill%
\begin{minipage}[t]{.49\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true
	]{Python}
// creating a opencv matrix with the given dimensions
// note that this uses a different constructor
dev_b = cv2.cuda.GpuMat(1024,1024,cv2.CV_32FC2)
\end{minted}
	\captionof{listing}{Equivalent code in Python \label{Creation ocv python}}
\end{minipage}%

\subsubsection{Passing Matrices to GPU kernels}

In order to operate on the matrix with CUDA kernels, one has to pass the pointer to the matrix' memory to a kernel (displayed in listing \ref{ocv kernel invoke cpp}).
Using C++ this is a straightforward process, since that address can be queried by taking the address of the \texttt{data} attribute of the GpuMat object. 
Using Python however this is not as easy, since the kernels themselves are wrapped by libraries like CuPy and those wrapped kernels do not take memory addresses (the memory address of the memory OpenCV allocated on the device is obtainable by querying the objects \texttt{cudaPtr()} method) as arguments but objects which implement either the CUDA Array Interface or DLPack.
This means we have to construct such an object describing the matrix and subsequently pass that object to a wrapped kernel.
To achieve this a class \texttt{GpuMatWrapper} as seen in listing \ref{GpuMatWrapper} was implemented.
This class does nothing more than representing an object with a valid CUDA Array Interface dictionary pointing to the OpenCV GpuMat. 

\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true
	]{Python}
class GpuMatWrapper:
	"""
	Acts as a cuda array interface compatible wrapper aroung opencv GpuMats
	"""
	def __init__(self, a: cv2.cuda_GpuMat):
		data_type = a.type()
		type_string = ""
		if data_type == 0:
			type_string = '<u1'
		elif data_type == 13:
			type_string = '<c8'
		self.__cuda_array_interface__ = {
			'shape': a.size(),
			'typestr': type_string,
			'descr': [('real', '<f4'), ('imag', '<f4')] if data_type == 13 else [('', type_string)],
			'data': (a.cudaPtr(), False),
			'version': 3
		}
\end{minted}
\captionof{listing}{Source code of the GpuMatWrapper class \label{GpuMatWrapper}}

Having that class enables us to operate on OpenCV GpuMats with all libraries discussed in section \ref{python-packages} as one would expect (shown in listing \ref{ocv kernel invoke python}).

\begin{minipage}[t]{.49\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true
	]{CPP}
// standard notation of invoking a kernel in CUDA C/C++
// this kernel performs the 1D-FFT across the rows of the dev_a matrix 
// and stores the result in the dev_b matrix
my_fft_u2c<1024, false><<<1024, 512>>>(
  (uchar *)dev_a.data,
  (float *)dev_b.data,
  1024, ...
);
\end{minted}
	\captionof{listing}{Invoking a kernel in C++ \label{ocv kernel invoke cpp}}
\end{minipage}\hfill%
\begin{minipage}[t]{.49\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\footnotesize,
	breaklines=true
	]{Python}
# Wrapping the matrices in CuPy matrices
# this is a zero-copy operation
a_cp = cp.asarray(GpuMatWrapper(dev_a))
b_cp = cp.asarray(GpuMatWrapper(dev_b))

# Passing those objects to a CuPy Kernel
fft_stockham_cuda_1024_u2c(
  (1024,), (512,), # the launch parameters
  (a_cp, b_cp, 1024, omega.real, omega.imag) # the kernel parameters
)
\end{minted}
	\captionof{listing}{Equivalent code in Python \label{ocv kernel invoke python}}
\end{minipage}%

With these adaptations in place porting the demonstration program from C++ to Python is a painless process.
The resulting Python code is not only more conciser, but also easier to modify than it's C++ counterpart, while retaining equivalent performance. The programs can be found in the \texttt{opencv\_demo}

\section{FFTs and Windowing}
\label{fft-introduction}

\subsection{The Discrete Fourier Transform}

The Discrete Fourier Transform (DFT) is a mathematical operation that transforms a signal from the time (or spatial) domain to the frequency domain.
It is a very important tool in signal processing and is used in many applications, such as audio processing, image processing and many more.
It is given by the following equation:

\begin{equation}
\label{eq:dft}
X_k = \frac{1}{N} \sum_{n=0}^{N-1} x_n e^{-i \frac{2 \pi}{N} k n}
\end{equation}


where $x_n$ is the $n$-th sample of the signal, $N$ is the number of samples and $k$ is the frequency bin.
The preceding factor of $\frac{1}{N}$ is a normalization factor, which is merely a convention and the DFT is not necessarily defined like this in all implementations (in Numpy this can be controlled by the \texttt{norm} argument of the \texttt{fft}-functions).
The only difference between the discrete fourier transform and its counterpart (the inverse discrete fourier) is the sign of the exponent and the normalization factor, to be consistent the product of the normalization factors has to be $\frac{1}{\sqrt{N}}$.
The basic DFT is a matrix multiplication of the signal vector $x$ with a matrix $W$ of size $N \times N$ and therefore has a time complexity of $\mathcal{O}(N^2)$ (as displayed in equation \ref{eq:dft-matrix}).

\begin{equation}
	\label{eq:dft-matrix}
	X = \frac{W x}{N}= \frac{1}{N} \begin{pmatrix}
		1 & 1 & 1 & \dots & 1 \\
		1 & e^{-i \frac{2 \pi}{N}} & e^{-i \frac{4 \pi}{N}} & \dots & e^{-i \frac{2 \pi (N-1)}{N}} \\
		1 & e^{-i \frac{4 \pi}{N}} & e^{-i \frac{8 \pi}{N}} & \dots & e^{-i \frac{4 \pi (N-1)}{N}} \\
		\vdots & \vdots & \vdots & \ddots & \vdots \\
		1 & e^{-i \frac{2 \pi (N-1)}{N}} & e^{-i \frac{4 \pi (N-1)}{N}} & \dots & e^{-i \frac{2 \pi (N-1)^2}{N}}
	\end{pmatrix}
	\begin{pmatrix}
		x_0 \\
		x_1 \\
		x_2 \\
		\vdots \\
		x_{N-1}
	\end{pmatrix}
\end{equation}

The matrix $W$ is called the DFT matrix and contains roots of unity.
Those roots of unity are called "twiddle-factors" in the context of FFT-transforms, since they rotate complex numbers when multiplied with them.
The entries of the individual rows can be thought of phasors, which are rotating counter-clockwise as one moves along the columns of the matrix.
This angular velocity increases as one moves down the rows of the matrix (it is a Vandermonde matrix).
For a 8 by 8 matrix matrix would be:

\begin{equation}
	\label{eq:dft-matrix-example}
	W = \begin{pmatrix}1&1&1&1&1&1&1&1\\
		1&e^{-i \frac{2 \pi1}{8}}&e^{-i \frac{2 \pi2}{8}}&e^{-i \frac{2 \pi3}{8}}&e^{-i \frac{2 \pi4}{8}}&e^{-i \frac{2 \pi5}{8}}&e^{-i \frac{2 \pi6}{8}}&e^{-i \frac{2 \pi7}{8}}\\
		1&e^{-i \frac{2 \pi2}{8}}&e^{-i \frac{2 \pi4}{8}}&e^{-i \frac{2 \pi6}{8}}&e^{-i \frac{2 \pi8}{8}}&e^{-i \frac{2 \pi10}{8}}&e^{-i \frac{2 \pi12}{8}}&e^{-i \frac{2 \pi14}{8}}\\
		1&e^{-i \frac{2 \pi3}{8}}&e^{-i \frac{2 \pi6}{8}}&e^{-i \frac{2 \pi9}{8}}&e^{-i \frac{2 \pi12}{8}}&e^{-i \frac{2 \pi15}{8}}&e^{-i \frac{2 \pi18}{8}}&e^{-i \frac{2 \pi21}{8}}\\
		1&e^{-i \frac{2 \pi4}{8}}&e^{-i \frac{2 \pi8}{8}}&e^{-i \frac{2 \pi12}{8}}&e^{-i \frac{2 \pi16}{8}}&e^{-i \frac{2 \pi20}{8}}&e^{-i \frac{2 \pi24}{8}}&e^{-i \frac{2 \pi28}{8}}\\
		1&e^{-i \frac{2 \pi5}{8}}&e^{-i \frac{2 \pi10}{8}}&e^{-i \frac{2 \pi15}{8}}&e^{-i \frac{2 \pi20}{8}}&e^{-i \frac{2 \pi25}{8}}&e^{-i \frac{2 \pi30}{8}}&e^{-i \frac{2 \pi35}{8}}\\
		1&e^{-i \frac{2 \pi6}{8}}&e^{-i \frac{2 \pi12}{8}}&e^{-i \frac{2 \pi18}{8}}&e^{-i \frac{2 \pi24}{8}}&e^{-i \frac{2 \pi30}{8}}&e^{-i \frac{2 \pi36}{8}}&e^{-i \frac{2 \pi42}{8}}\\
		1&e^{-i \frac{2 \pi7}{8}}&e^{-i \frac{2 \pi14}{8}}&e^{-i \frac{2 \pi21}{8}}&e^{-i \frac{2 \pi28}{8}}&e^{-i \frac{2 \pi35}{8}}&e^{-i \frac{2 \pi42}{8}}&e^{-i \frac{2 \pi49}{8}}\\
		\end{pmatrix}
\end{equation}

Knowing the property $e^{-i \frac{2 \pi (p + N q)}{N}} = e^{-i \frac{2 \pi p}{N}} \cdot e^{-i 2 \pi q} = e^{-i \frac{2 \pi p}{N}},\quad q \in \mathbb{Z}$ and designating $e^{\frac{-i 2 \pi}{N}}$ as $\omega_N$, we can further simplify the matrix and the original equation \ref{eq:dft} (the normalization factor was omitted).

\begin{equation}
	W = \begin{pmatrix}1&1&1&1&1&1&1&1\\
		1&\omega_{8}&\omega_{8}^{2}&\omega_{8}^{3}&\omega_{8}^{4}&\omega_{8}^{5}&\omega_{8}^{6}&\omega_{8}^{7}\\
		1&\omega_{8}^{2}&\omega_{8}^{4}&\omega_{8}^{6}&\omega_{8}^{0}&\omega_{8}^{2}&\omega_{8}^{4}&\omega_{8}^{6}\\
		1&\omega_{8}^{3}&\omega_{8}^{6}&\omega_{8}&\omega_{8}^{4}&\omega_{8}^{7}&\omega_{8}^{2}&\omega_{8}^{5}\\
		1&\omega_{8}^{4}&\omega_{8}^{0}&\omega_{8}^{4}&\omega_{8}^{0}&\omega_{8}^{4}&\omega_{8}^{0}&\omega_{8}^{4}\\
		1&\omega_{8}^{5}&\omega_{8}^{2}&\omega_{8}^{7}&\omega_{8}^{4}&\omega_{8}&\omega_{8}^{6}&\omega_{8}^{3}\\
		1&\omega_{8}^{6}&\omega_{8}^{4}&\omega_{8}^{2}&\omega_{8}^{0}&\omega_{8}^{6}&\omega_{8}^{4}&\omega_{8}^{2}\\
		1&\omega_{8}^{7}&\omega_{8}^{6}&\omega_{8}^{5}&\omega_{8}^{4}&\omega_{8}^{3}&\omega_{8}^{2}&\omega_{8}\\
		\end{pmatrix}
\end{equation}

Expressing the matrix in this way makes it clear that this matrix has special properties.
Those properties are the symmetry and the alternating ones ($\omega^0$) and minus ones ($\omega^4$) in the $N/2+1$ th-column.
The second property only emerges when N is a power of two.

\begin{equation}
	\label{eq:dft-simplified}
	X_k = \sum_{n=0}^{N-1} x_n \cdot \omega^{kn}
\end{equation}

\subsection*{The Fast Fourier Transform}

Using the mentioned properties of the matrix one can derive one version of the fast fourier transform (FFT) as follows.
This is equal to the decimation in time formulation originally proposed by Cooley and Tukey in their 1965 paper \cite{cooley1965algorithm} and as outlined by Chu \cite{ChuEleanor2000ItFb}. 
It is recognized as one of the 7 key algorithms in high performance computing. \cite{10.1145/1562764.1562783}

\subsubsection{Derivation of the Radix-2 Decimation in Time FFT}

Equation \ref{eq:dft-simplified} can be rewritten as follows.

\begin{equation}
	\label{eq:dft-simplified-2}
	X_k = \sum_{n=0}^{\frac{N}{2}-1} x_{2n} \cdot \omega_{N}^{2kn} + \sum_{n=0}^{\frac{N}{2}-1} x_{2n+1} \cdot \omega_{N}^{(2n+1)k}
\end{equation}

The factor $\omega_N^k$ can be extracted from the second summand.

\begin{equation}
	\label{eq:dft-simplified-3}
	X_k = \sum_{n=0}^{\frac{N}{2}-1} x_{2n} \cdot \omega_{N}^{2kn} + \omega_N^k \sum_{n=0}^{\frac{N}{2}-1} x_{2n+1} \cdot \omega_{N}^{2kn}
\end{equation}

Using the property $\omega_N^{2kn} = \omega_{\frac{N}{2}}^{kn}$ it can be recognized that the original problem was divided into two equal sub-problems half as large as the original problem.
Adding the results of the two sub-problems (which lends itself very well to FMA instructions) yields the result of the original problem, this combination is known as the butterfly operation.

\begin{equation}
	\label{eq:dft-simplified-4}
	X_k = \sum_{n=0}^{\frac{N}{2}-1} x_{2n} \cdot \omega_{\frac{N}{2}}^{kn} + \omega_N^k \sum_{n=0}^{\frac{N}{2}-1} x_{2n+1} \cdot \omega_{\frac{N}{2}}^{kn}
\end{equation}

The original problem of complexity $O(N^2)$ was divided into two smaller problems of $O(\frac{N}{2}^2)$ each.
This process can be repeated recursively until the sub-problems have a size of two.
In this process one gets $log_2(N)$ levels of recursion with N number of operations required per level, thus resulting in the overall complexity of $O(N \log(N))$.
All variations of the FFT are based on this principle of divide and conquer, their specific performance characteristics depend on implementation details (as investigated in section \ref{fft-case-study}).

\subsection{The cuFFT Library}
\label{cufft_section}

CuFFT \cite{cuFFTNVI56:online} is Nvidia's proprietary offering for GPU accelerated FFT-transforms.
It offers a C API and operates based on so-called plans, which are objects describing the parameters of the transform.
The basic workflow consists of creating a plan, executing the plan and finally destroying the plan.
Its API is closely modeled after FFTW3 \cite{FFTW3}.
Unlike FFTW3 however, cuFFT does not make it clear how it chooses a specific implementation for a given transform over another.
It seems plausible that Nvidia conducts a wide range of tests on their hardware and ships a wisdom-file with the library, which contains the results of those tests. FFTW3 offers the functionality to build such databases of optimal plans for various transforms by running a large set of tests.
The downside of this approach is that the wisdom-file is only valid for the hardware it was generated on (mostly depending on the CPU-memory pairing), so it is not possible to ship a single wisdom-file with the library.
Nvidia has the advantage of being a vendor of a limited variation of self-contained devices (the GPU's computing capabilities as well as the paired amount and speed of memory is known), so they could determine that information beforehand.
This means that there is little to no room for optimization on the user's side, since the library will always choose the same implementation for a given transform and users have no influence on that choice.

Libraries relying on cuFFT can only differentiate between them by making good use of the different API calls provided.
Possible design choices which could make a library more efficient than another one are keeping a cache of recently used plans and reusing them, or using batched transforms to reduce the overhead of creating and destroying plans and using the correct transforms (i.e. using a Real to Complex transform instead of automatically converting the input to a Complex array and then performing a more general Complex to Complex transform).

CuFFT offers the following transform types:
\begin{itemize}
	\item \texttt{C2C}: Complex to Complex
	\item \texttt{R2C}: Real to Complex
	\item \texttt{C2R}: Complex to Real
	\item \texttt{D2Z}: Double to Double Complex
	\item \texttt{Z2D}: Double Complex to Double
\end{itemize}

Besides the basic library Nvidia also offers a library called cuFFTXt (Xt stands for eXtended), with additional features including the functionality to distribute the workload over multiple GPUs and the ability to perform transforms on half-precision floating point numbers and a library called cuFFTDx.
CuFFTXt offers additional features including the functionality to distribute the workload over multiple GPUs and the ability to perform transforms on half-precision floating point numbers.
cuFFTDx is a library for integrating FFTs into user-written CUDA-kernels.
This is useful to make individual kernels do more work, instead of having multiple kernels, with a cuFFT kernel in between.
This is related to the idea of kernel fusion as discussed in section \ref{kernel-fusing}.
The size of transforms which can be performed with cuFFTDx however is rather limited (up to $2^{15}=32768$ depending on the used data-types and CUDA capability of the GPU).
%TODO: floating point precision

\subsection{Comprehensive Benchmarks for various Implementations available in Python}

Benchmarking tools such as Gearshifft \cite{10.1007/978-3-319-58667-0_11} play an important role in picking the right FFT implementation for a given problem.
Additionally benchmarking is not as trivial as it might seem at first glance, since the performance of a given implementation is dependent on many factors, such as the size of the transform (powers of two, or a numbers with low prime factorizations are faster to transform than arbitrary sizes), the kind of hardware (CPU, GPU, FPGA, ASIC) used and the memory bandwidth connecting that hardware to the main memory.
Measuring performance becomes even more complicated when dealing with Heterogeneous systems, since the function calls often happen asynchronously and the time it takes to execute a given function is not necessarily the time it takes to complete the operation.
Often the functions are merely scheduled on the device and the executing program (in our case Python scripts) have to be explicitly instructed to wait for the operation to complete (synchronization).
This connects with the concept of streams, as discussed in section \ref{stream_section}.
Operations can be enqued on different streams and the order of execution is only ever guaranteed for operations on the same stream.

The tests conducted in this section were performed with a convenient set of scripts with the aim to test the various implementations of the available FFT (or other DFT) algorithms on performance (run-time and device memory usage).
The aim is to provide users with vital information on which implementation works best on a given machine for a given workload.
Having convenient access to such information is crucial when dealing with today's high heterogeneity of computing systems ranging from small, embeddable systems (like the raspberry pi series or CUDA-capable alternatives like Nvidia's Jetson series) over traditional desktop workstation to large distributed clusters, since the performance of the individual libraries dependent on many factors and predicting performance is non-trivial.

Unless otherwise stated each test was performed 50 times and the median time of that 50 tests reported, this median time is usually very close to the minimum time and significantly smaller than the maximum due to an increased cost of the first invocation caused by JIT-, or general driver overhead.

\subsubsection{Libraries and Features tested}\mbox{}\\

The benchmarks should evaluate both CPU- and GPU-accelerated implementations of (multi-dimensional, forward) fourier transforms for a variety of power of 2 and odd sizes with different levels of precision (single and double).

CPU libraries:
\begin{itemize}
	\item Numpy's FFT libraries
	\item FFTW3 bindings
	\item Intel MKL
\end{itemize}

GPU accelerated (note: it is expected that some of these libraries rely on the cuFFT-implementation):
\label{tested-gpu-libraries}
\begin{itemize}
	\item CuPy
	\item PyVkFFT (using the OpenCL backend)\cite{vkfft, pyvkfft}
	\item PyTorch
	\item Jax
\end{itemize}

This analysis with many independent factors (library, direction, size/shape, kind of transform, precision) will quickly lead to many hundreds of possible configurations, which need to be efficiently testable and comparable.

Other noteworthy libraries include for example cuSignal \cite{cusignal} (a GPU-aware adaption of SciPy Signal); while very useful for signal processing, it was not tested because it is relying on CuPy (which in turn uses cuFFT) for the FFTs, which is already tested in this benchmark suite.

\paragraph{Architecture}\mbox{}\\

This tool utilizes Nvidia Nsight Systems as the underlying profiler, since it is one of the few profilers which are able to track CPU and GPU usage as well as the usage of GPU-dedicated memory on the process level (the inability to profile system memory usage with this tooling is unfortunate, but acceptable since we are mostly interested in GPU resources and host side RAM is a comparatively abundant and cheap resource, albeit usually an order of magnitude slower).
Other profilers like Scalene \cite{Berger_Scalene_Scripting-Language_Aware_2020} (a Python specific profiler that allows users to track CPU, GPU and memory metrics) for example use NVML (the Nvidia Management Library) to periodically retrieve GPU metrics.
We chose to use Nsight Systems because it can store all recorded information during a profiling session in an SQL-Lite database.
The information stored in this database can then be programmatically queried, filtered and used for comprehensive reports and analysis using well established tools from Python's ecosystem (pandas and matplotlib). 
Ultimately this section reports on the results of the execution of the defined algorithms with a selected set of libraries.

\begin{comment}
To streamline the setup of the necessary environment and to provide a path for reproducible testing separate docker containers for the individual tested packages could be created.
This ensures that the benchmarking tool can be set-up and run on any machine with a working docker installation (given a present GPU compatible with the respective library).
This is especially important for GPU-accelerated libraries, since they are often very sensitive to the CUDA driver versions and Python versions (Python packages, which target CUDA are built and linked against a specific version of the CUDA toolkit).
In such a setup the host system only needs to have a working docker installation and the necessary drivers for the GPU, while the docker container contains the remaining parts of the stack (the CUDA toolkit, the profiler, the Python interpreter, the Python packages and the code to generate and execute the test cases).
This allows one to easily test different combinations of CUDA toolkit versions, Python versions and Python packages without having to install them on the host system (conda environments and Python virtual environments provide similar functionality, but docker containers provide even more possibilities for encapsulation).
A recent CUDA driver can support multiple major CUDA toolkit versions, as seen in the tables provided by Nvidia \cite{cudacompat}.
This technique of containerization is enabled by the Nvidia Container Runtime, which is a docker runtime that allows the execution of docker containers with GPU support \cite{container-runtime}.
AMD also provides similar solutions to support GPU-accelerated docker containers \cite{rocm-docker}.
\end{comment}

\subsection{Results on local machine}

First we are going to present the results of the benchmarking tool on a local machine with a Nvidia RTX 3070 GPU (GA104 with 8 GB of dedicated memory) and an AMD Ryzen 5 3600 CPU.
Despite being listed in section \ref{tested-gpu-libraries}, the PyVkFFT library with the OpenCL backend was not tested on this machine, since it proved to be unstable and regularly crashed the system.

\subsubsection{1D Transforms}
\label{1d_transforms}

\begin{figure}[H]
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/plots/fft_timings_double_local.pdf}
	\caption{1D FFT benchmarks for double precision, only execution time is shown (no memory transfer time), only power of 2 sizes were tested, performance between two datapoints is \emph{not} linearly interpolatable}
	\label{fft_timings_double_local}
\end{figure}

Figure \ref{fft_timings_double_local} shows the performance of the different libraries for double precision.
The shaded area indicates the complete range of the measured time taken for the execution of a single FFT.
This range is especially wide for the GPU-accelerated libraries (CuPy, PyTorch and Jax), which is due to the cost of JIT-compilation of the kernels, which is necessary on the first run of a kernel.
The line (located in the lower parts of the shaded area) indicates the median of the measured times. 

The performances of the tested libraries can be roughly divided into two groups: the CPU-accelerated libraries (Numpy, pyfftw and mkl) and the GPU-accelerated libraries (CuPy, PyTorch and jax).

The GPU libraries are generally faster than the CPU libraries (at least for FFTs of sizes $2^{14}=16384$ and larger) and show little differences in their relative performance with only PyTorch deviating from the trend by being slightly faster than CuPy and Jax.
This small difference is likely due to the fact that PyTorch is using the same cuFFT implementation as CuPy and Jax, this suspicion can be confirmed by looking at the kernels called by the libraries.
The kernels executed by the three libraries to perform a single 1D-transformation of large size ($2^{25}$ - the largest datapoint recorded in figure \ref{fft_timings_single_local}) are shown in the tables \ref{cupy-33554432-double} through \ref{pytorch-33554432-double}.

It can be observed that all three libraries are using multiple (in this case three) invocations of a kernel called \texttt{regular\_fft} to calculate the transformation, but they do so in different ways.
CuPy and JAX both first convert the real-valued inputs to a complex-valued inputs using kernels called \texttt{cupy\_copy\_\_float64\_complex128} and \texttt{convert\_2} respectively followed by identical invocations of the \texttt{regular\_fft} kernel.

PyTorch on the other hand starts directly with the \texttt{regular\_fft} kernels and uses fewer blocks (although blocks of the same size) than the other two libraries.
This directly translates into a lower number of threads launched and more work being done by each thread.
This is likely the reason why PyTorch is slightly faster than the other two libraries.

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\input{../fft_benchmark_suite/tables/cupy_33554432_double.tex}
	\caption{Kernels called by the CuPy library for a double precision FFT of size $2^{25}=33554432$}
	\label{cupy-33554432-double}
\end{table}

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\input{../fft_benchmark_suite/tables/jax_33554432_double.tex}
	\caption{Kernels called by the Jax library for a double precision FFT of size $2^{25}=33554432$}
	\label{jax-33554432-double}
\end{table}

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\input{../fft_benchmark_suite/tables/pytorch_33554432_double.tex}
	\caption{Kernels called by the PyTorch library for a double precision FFT of size $2^{25}=33554432$}
	\label{pytorch-33554432-double}
\end{table}

The CPU libraries display a wider range of performance, which is due to the difference in backends used by the libraries.
In this configuration the mkl-fft library is the fastest, followed by pyfftw and Numpy.
Their respective backends are Intel's MKL, FFTW (linked against OpenBLAS) and Numpy's own adaptation of FFTPACK (called PocketFFT). \cite{pocketfft}

The dashed line indicates the theoretical relationship of the FFT-algorithm and the size of the input data ($O(n \log_2 n)$), the exact position of this line on the plot is arbitrary; important is how its slope relates to the slopes of the other lines, which stem from the measurements.
\begin{figure}[H]
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/plots/fft_timings_single_local.pdf}
	\caption{1D FFT benchmarks for single precision}
	\label{fft_timings_single_local}
\end{figure}

Figure \ref{fft_timings_single_local} shows the performance of the different libraries for single precision.
The differences in performance between single and double precision are especially pronounced for the GPU-accelerated libraries, while they are relatively small for the CPU libraries pyfftw and mkl-fft.
Figure \ref{precision-comparison} offers a detailed view into the performance differences between the different levels of precision for two selected libraries (PyTorch executing on the GPU and PyFFTW on the CPU). 
Note that Numpy does not support single precision FFTs, it always returns a double precision array, even if the input array is single precision. Numpy is only included in figure \ref{fft_timings_single_local} for completeness.

Tables \ref{complete-fft-table-cpu} and \ref{complete-fft-table-gpu} shows the median execution times for the different libraries for all tested sizes and single and double precision.
The table also shows the relative performance of the libraries compared to Numpy as a baseline.

Table \ref{complete-fft-table-cpu} shows the median execution times for the CPU based libraries.
It is of note that the alternatives libraries (pyfftw and mkl-fft) tend to a larger speedup the larger the input size, but only up to a certain point.
MKL-FFT reaches a maximum speedup of 21.9 for a single precision FFT of size $2^{21}$ and a maximum speedup of 9.7 for a double precision FFT of size $2^{20}$.
The larger speedup for single precision problems is expected, since Numpy is always using double precision for its FFTs, so the baseline is slightly skewed in those cases.
One can, however, observe that the difference in runtimes for single- and double precision problems is relatively small for CPU based libraries.
The difference between single and double precision when using the mkl-fft library for example is roughly a factor of 2, and a factor of 1.5 for pyfftw.

\begin{table}
	\centering
	\rowcolors{4}{}{lightgray}
	\begin{footnotesize}
	\input{../fft_benchmark_suite/tables/fft_timings_table_cpu.tex}
	\end{footnotesize}
	\caption{Median execution times and relative speedup compared to the Numpy baseline for the CPU based libraries for all tested sizes and single and double precision}
	\label{complete-fft-table-cpu}
\end{table}

Table \ref{complete-fft-table-gpu} shows the median execution times for the GPU based libraries.
The baseline for the speedups is once again Numpy, calculating double precision FFTs on the CPU.
The speedups for the GPU based libraries are much larger than for the CPU based libraries, even for double precision problems even though modern GPUs prioritize single precision operations.
The speedups get strictly larger for larger input sizes and reach values of up to 424.5 for a single precision FFT of size $2^{25}$ when using PyTorch and 85 for a double precision FFT of size $2^{25}$.
Note that this table only contains the executions times necessary for the calculation of the FFTs on the GPU, not the time needed to transfer the data to and from the GPU, which can be a bottleneck (as explained in the section discussing the roofline model \ref{roofline-section}).

\begin{table}
	\centering
	\rowcolors{4}{}{lightgray}
	\begin{footnotesize}
	\input{../fft_benchmark_suite/tables/fft_timings_table_gpu.tex}
	\end{footnotesize}
	\caption{Median execution times and relative speedup compared to the Numpy baseline for the CPU based libraries for all tested sizes and single and double precision}
	\label{complete-fft-table-gpu}
\end{table}

\paragraph{Comparison of performance when using different levels of precision}\mbox{}\\

Figure \ref{precision-comparison} shows the performance of two different libraries (PyTorch on the GPU and pyfftw on the CPU) over a range of different levels of precision.
For PyTorch half (\texttt{torch.float16}), single and double precision was tested, while for pyfftw quad-precision was also tested.
Note that the result of the half-precision transform has to be considered with caution, since it's very likely overflow the first entry (the sum of all entry elements, unless the transform is divided through the number of inputs) due to the limited range of half-precision floats. \cite{cuFFT}
PyFFTW quad-precision output will be the reference for evaluating the error of the other libraries.

\begin{figure}[H]
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/plots/fft_precision_comparison_local.pdf}
	\caption{Comparison of performance when using different levels of precision}
	\label{precision-comparison}
\end{figure}

\paragraph{Memory usage of the GPU based libraries}\mbox{}\\

The usage of GPU Memory is an important factor when evaluating GPU based libraries, since the amount of memory available on a GPU is often limited.
Figure \ref{memory-usage-local} shows the memory usage of the GPU based libraries for a single precision FFT of size $2^{25}$.

The high relative memory usage (10 to 100 times the size of the input) for small problem sizes can be safely ignored, since that usage can be attributed to the usage of the CUDA runtime library.
This memory usage is called device memory footprint and varies from CUDA version to CUDA version, this memory usage only becomes critical in the case of many separate processes using CUDA on the same GPU.

Of the GPU based libraries, JAX and CuPy have a significantly higher memory usage than the other library (PyTorch) for the same problem size.
The exact numbers are provided in table \ref{memory-usage-table-local}.
It is clear that the relative memory usage cannot be below 2, since space has to be allocated for the input and output arrays.
The relative memory usage of the GPU based libraries converges towards integer values as the problem size increases, which is expected since the memory usage is proportional to the size of the input array.

PyTorch uses 4 times the size of the input array for both single- and double precision problems.
This memory is most likely distributed in the following way: the input array, the output array and double the size of the input array for an intermediate array of the same size, but containing complex numbers and thus twice the size in memory of the input array.

CuPy and JAX use 9 or 8 times the size of the input array.
This is most likely due to those libraries instantiaiting extra copies of the input at some point or, as seen in the analysis of the kernels executed (tables \ref{cupy-33554432-double}-\ref{jax-33554432-double}), the creation of not necessarily needed complex-valued copies of the input array.

So even though the three tested libraries all share the same backend (cuFFT), the memory usage can vary significantly due to design choices made by the library developers.

\begin{figure}[H]
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/plots/fft_memory_local.pdf}
	\caption{Relative Memory usage to size of input array}
	\label{memory-usage-local}
\end{figure}

\begin{table}[H]
	\centering
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/tables/fft_memory_table_gpu_standalone.pdf}
	\caption{Memory usage of the GPU based libraries for various problem sizes, both in absolute and terms relative to the size of the input array}
	\label{memory-usage-table-local}
\end{table}

\subsubsection{Batched 1D Transforms}

As mentioned in section \ref{size-limitation} the performance of GPU algorithms are often greatly improved when problems fit into partitions of the memory, which share memory for fast synchronization (e.g. a single block of a CUDA kernel).
To still fully utilize the GPU, the GPU can perform multiple FFTs in parallel, this step is natural when calculating FFTs of higher-dimensions, but can also be applied to 1D FFTs.

\paragraph{Constant transform size}\mbox{}\\

Figure \ref{batched-double-size-1024} shows the performance of batched 1D transforms with a constant transform size of 1024.
The dashed line represents the linear continuation of the performance of the single transform of size 1024.
The measured performance is better than one would expect from that linear continuation.

Especially remarkable are the plateaus in the performance (especially pronounced for PyTorch) which stem from the fact that all of the GPU's multiprocessors can be utilized at the same time.
These plateaus are not as pronounced for the other libraries, since their overhead is higher and masks these effects.
This means that it takes virtually the same amount of time to calculate 50 transforms as it does to calculate a single transformation, as long as all of the blocks of the kernel can be scheduled to be executed at the same time.
As discussed in section \ref{GPU-structure} the used GPU has 46 multiprocessors, which means that at least 46 blocks can be scheduled for simultaneous execution.
In the example at hand, it seems that a jump occurs at every increase of the batch of size by 92 for the double-precision transform, which is the first time that not all blocks can be scheduled for execution at the same time.
This suspicion can be confirmed by examining the kernels executed for the batch sizes of 92 and 93 (listed in tables \ref{batched-kernel-92} and \ref{batched-kernel-93}).
For both settings the executed kernels are the same, but the number of blocks is different.
When batching 93 the number of blocks scheduled for execution is 47, which is one more than the number of multiprocessors and the execution time doubles.
This implies that performance can vary greatly even on slightly different GPU models.
The next larger GPU model (RTX 3070 Ti), for example, has 48 multiprocessors, which means the jumps in performance would occur at slightly larger batch sizes and the performance would be twice as good as on the non Ti model for certain edge cases.

Regardless of the plateaus in execution time needed, the performance of the batched transforms is significantly better than the performance of the single transforms, since this are the situations GPUs excel in - calculating multiple relatively small tasks (which do not require communication across individual multiprocessors) of the same kind, which are independent of each other, in parallel.
One can see that the time needed to calculate the transformations does not increase as fast as the batch size increases (it is a linear relationship, but its slope depends on the number of multiprocessors).
Calculating 800 transforms in parallel takes about 1.5 times as long as calculating a single transform.

\begin{figure}[H]
	\centering
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/plots/double_gpu_batched_1024.pdf}
	\caption{Performance of batched 1D transforms with a constant transform size of 1024 for double precision on a GPU (the steps are apparently not of equal size due to logarithmic scaling of the y-axis)}
	\label{batched-double-size-1024}
\end{figure}

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\begin{footnotesize}
	\input{../fft_benchmark_suite/tables/pytorch_double_batched_92.tex}
	\end{footnotesize}
	\caption{Kernels executed for 92 batched transforms of size 1024, double precision PyTorch on the GPU}
	\label{batched-kernel-92}
\end{table}

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\begin{footnotesize}
	\input{../fft_benchmark_suite/tables/pytorch_double_batched_93.tex}
	\end{footnotesize}
	\caption{Kernels executed for 93 batched transforms of size 1024, double precision PyTorch on the GPU}
	\label{batched-kernel-93}
\end{table}

The batch size has less effect on the performance of the single precision transforms, since these transforms are an order of magnitude faster than the process of launching a kernel (at least for JAX and to a slightly lesser extent for CuPy; in the case of JAX all batch sizes take roughly the same amount of time).
PyTorch again is the fastest library, but the plateaus are not as pronounced as in the case of the double precision transforms.
This could be due to the fact that the GPU is not dedicated to those computations, but is also used for other tasks (like rendering the UI to the displays), which also put some load on the FP32 units of the GPU, whereas the FP64 units are not used at all by unless explicitly requested by GPGPU applications.

\begin{figure}
	\centering
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/plots/single_gpu_batched_1024.pdf}
	\caption{Performance of batched 1D transforms with a constant transform size of 1024 for single precision on a GPU}
	\label{batched-single-size-1024}
\end{figure}

The benefit of using batched transforms is much less pronounced for the CPU based libraries, since CPUs do not have the capabilities for such massively parallel computations.
Of course there is some benefit because a batched execution lends itself better to vectorization and parallelization than a single transform of such a small size (1024) and batching transformations is highly encouraged even when using CPU based libraries.
Figure \ref{batched-double-size-1024-gpu} shows the performance of the CPU based libraries for the same problem size as the GPU based libraries and double precision. 
The respective graphic for single precision since the differences between single- and double-precision are much less pronounced for the CPU based libraries.
The absence of any plateaus in the performance of the CPU based libraries is expected but still noteworthy.
Numpy's and MKL-FFTW's performances follow the linear continuation of the performance of the single transform of size 128 very closely, while pyFFTW's performance is a lot noisier.
It can be as fast as MKL-FFTW, but in the median case it tends to be slower.
In fact the median performance of pyFFTW is almost constant over the tested batch-sizes, which is a sign of a lot of overhead or bad multi-threading slowed down by other processes running on the same machine.

\begin{figure}[H]
	\centering
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/plots/double_cpu_batched_1024.pdf}
	\caption{Performance of batched 1D transforms with a constant transform size of 1024 for double precision on a CPU}
	\label{batched-double-size-1024-gpu}
\end{figure}

\paragraph{Constant batch size}\mbox{}\\

A similar phenomenon to the plateaus observed in the previous section is expected when the batch size is held constant and the transform size is increased.
In this case the jump would occur whenever a fewer number of blocks could be fitted onto a multiprocessor. This could happen due to a number of factors, namely the number of threads per block, the shared memory usage per block and the number of registers used per block.
Measurements however show that this effect is not as visible as one might expect, because the effect of the varying occupancy is overshadowed by the fact that the performance of the FFT algorithm is highly dependent on the transform size.
In the previous tests only power of two transform sizes were used, which are the most efficient transform sizes for the FFT algorithm.
However, when testing with non-power of two transform sizes, entirely different algorithms and kernels are used such as the Bluestein algorithm, which allows for arbitrary transform sizes \cite{rabiner1985bluestein}.
Those kernels also have different resource requirements, which leads to many plateaus as the transform size is increased. 

Figure \ref{batched-double-1024-4096} shows the performance of 1D FFTs for all problem sizes between 1024 and 4096, which have a largest prime factor of 2, 3, 5 or 7 or are a prime number.
This plot shows that the performance for problem sizes with low prime factors are faster than the performance for problem sizes, that happen to be prime numbers, by a factor of 2 to 3.
Additionally, the performance for prime numbers exhibits a pronounced jump around 2048.
Inspecting the respective kernels executed reveals the reason for this jump.

\begin{figure}[H]
	\centering
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/plots/fft_batched_primes.pdf}
	\caption{Performance of batched 1D transforms with a constant batch size of 92 for double precision on a GPU with PyTorch}
	\label{batched-double-1024-4096}
\end{figure}

As listed in tables \ref{pytorch-2039} and \ref{pytorch-2053} the kernels executed for the prime number sizes 2039 and 2053 are different.
For the larger prime number the kernel \texttt{regular\_bluestein\_fft} is replaced with 4 instances of the kernel \texttt{multi\_bluestein\_fft}.
The kernel \texttt{regular\_bluestein\_fft} was launched with exactly 92 blocks (the same as the number of batched transforms), which suggests that this was the largest bluestein transform that could be calculated within a block on this particular GPU.
Once a transform does not fit within a single block anymore the execution needs to be split into multiple kernels, since CUDA makes no guarantees about the order in which blocks within a kernel are executed.
Separating different stages of a transform acts a way to introduce synchronization, as the order of the kernels is guaranteed.
This introduces the drawback of additional round-trips of the data between the GPU registers and the global memory, which is why the performance of the multi-kernel execution is slower than the performance of the single kernel execution.

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\begin{footnotesize}
	\input{../fft_benchmark_suite/tables/pytorch_double_batched_2039_92.tex}
	\end{footnotesize}
	\caption{Kernels executed for 92 batched transforms of size 2039, double precision PyTorch on the GPU}
	\label{pytorch-2039}
\end{table}

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\begin{footnotesize}
	\input{../fft_benchmark_suite/tables/pytorch_double_batched_2053_92.tex}
	\end{footnotesize}
	\caption{Kernels executed for 92 batched transforms of size 2053, double precision PyTorch on the GPU}
	\label{pytorch-2053}
\end{table}

\subsubsection{Error analysis}

The single precision FFTs on the GPU are faster than the equivalent double precision CPU based FFTs, in this section we will investigate the accuracy of the different libraries.

To judge the accuracy we will create a random signal containing values in the $\left[-1,1\right]$ range using Numpy and then copy it to the GPU, if necessary, perform the FFT, copy the result back to the CPU and then compare the result to the result of a CPU based quad precision FFT based on the L2 error.
This test is similar to the one conducted in \cite{vkfft}, but also including different CPU based libraries as well as half-precision GPU-based FFTs where available (cuFFT only supports half-precision transforms of power-of-two sized transforms).

Figure \ref{error-all} gives an overview of the error for all tested libraries and precision levels.
As expected the error greatly depends on the used precision level, but cuFFT based libraries are not necessarily less accurate than their CPU based counterparts.
The benchmarks do not display significant differences between the different libraries for the same precision level, except for Numpy.
Numpy offers the best accuracy for single precision transforms out of all tested libraries.

\begin{figure}[H]
	\centering
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/plots/precision_all.pdf}
	\caption{L2 error of the different libraries and precision levels}
	\label{error-all}
\end{figure}

Figure \ref{error-single-gpu} shows the error for single precision transforms on the GPU, which is probably the most relevant precision level for applications which could benefit from the GPU-acceleration.
The figure shows that the difference between the different libraries is not zero, but it is small enough to be negligible for most applications.
In this particular test PyTorch is the most accurate GPU-based library.
In general the error is smaller for power-of-two sized transforms.

\begin{figure}[H]
	\centering
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/plots/precision_gpu_single.pdf}
	\caption{L2 error of the different libraries and precision levels for single precision transforms on the GPU}
	\label{error-single-gpu}
\end{figure}

\subsubsection{Effect of memory copy overhead on performance}

The previous tests have shown that calculating FFTs is faster than CPU options for most cases (single as well as double precision, batched as well as single transforms), but these tests assumed that the data already resides on the GPU.
This can be a justified assumption in some cases, on Nvidia's Jetson series for example the GPU has no dedicated memory, but a shared, physically unified memory with the CPU, therefore on that and similar platforms there is no need to transfer data to and from the GPU. \cite{Jetson-um}

On most other platforms however the GPU has dedicated memory, which requires data to be copied to and from the GPU over the PCI-E bus.
This can be a significant overhead, especially for small transforms, where the overhead of the memory copy can be larger than the actual computation time.
Table \ref{memory-copy-overhead-table} lists the performance and potential speedup for the different libraries, when the data is copied to and from the GPU.
It is not entirely clear why CuPy performs best followed by PyTorch and then JAX.
Investigations into the recorded profiles show that the amount of data copied is exactly the same across the libraries but the achieved bandwidth varies and that JAX distributes the copy-tasks across several streams, which might introduce additional overhead.
The HtoD movements are generally faster (reaching 13 GB/s with all libraries) than the DtoH movements (PyTorch and JAX: 3.7 GB/s, CuPy: 7.2 GB/s).
The difference is not explained by CuPy using pinned memory, since the DtoH movements appear as pageable for all libraries. 
It seems likely that CuPy uses a different memory allocator for its host-side memory, which might be write-able faster than the default allocator (might be achieved by using a different alignment, in order to allow CPU-SIMD instructions to copy the data).


\begin{table}
	\centering
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/tables/fft_timings_mem_table_gpu_standalone.pdf}
	\caption{Performance of the different libraries for different transform sizes, when the data is copied to and from the GPU, related to table \ref{complete-fft-table-gpu}}
	\label{memory-copy-overhead-table}
\end{table}

It can be seen that the overhead of the memory copy gets relatively smaller the larger the transform size gets and that the speedups (again compared to a Numpy baseline) are non-existing for small transform sizes and low for even for large transforms.
The speedups only reach values of up to 24 instead of the values seen in table \ref{complete-fft-table-gpu}, where only the computation time is considered.
This makes the case for using GPUs for single transforms weak, since similar speedups were also achieved by highly optimized CPU based libraries (MKL-FFT reached values in the 10-22 range for single precision transforms and values ranging from 5-10 for double precision transforms).
The fact that data movement between across the PCI-E bus amounts to $>90\%$ of the total run-time for this task, also puts the discussed differences in run-time for power-of-two sized transforms in the best case and prime-sized transforms in the worst case into perspective.
The run-time difference becomes negligible when the data is copied to and from the GPU, since the data movement is the dominant factor in the run-time.
This relates back to the roofline model (section \ref{roofline-section}), which explains how the arithmetic intensity of an algorithm relates to the performance of the algorithm.

The increase of the speed-up as the transform size increases can be explained by the increasing arithmetic intensity of the FFT algorithm as transform sizes grow ($O(n \log(n))$), whereas the data needed to be copied only grows linearly.
Taking the cost of memory copies into account, also increases the appeal of operating with lower precision, since the less memory needs to be copied and the arithmetic intensity automatically increases as it is defined as the ratio of the number of floating point operations to the number of bytes read from memory.
Single precision transforms are almost exactly twice as fast as their double precision counterparts, which can be mostly attributed to the fact that only half the amount of data needs to be copied to and from the GPU.
The relatively weak FP64 performance of the GPU is not reflected in the results, since the data movement is the dominant factor in the run-time.
This was not the case in the results listed in table \ref{complete-fft-table-cpu}, there are significant differences in the run-time for single and double precision transforms, but they do not matter in a memory bound regime.
Every factor other than the interconnect speed becomes irrelevant.

\paragraph{Windowing}\mbox{}\\

Applying windowing functions to the input data before performing the FFT is a common practice in signal processing.
From the perspective of accelerated computing it is an embarrassingly parallel operation, which can be easily parallelized on the GPU.
The run-time of such an operation is primarily defined by reading and writing the data to and from the GPU memory (memory bound operation).
Ideally the windowing operation should take place within the FFT-kernel, to avoid the overhead of a memory round trip (kernel fusion).
CuFFT offers the possibility to use so-called callback functions to supply the FFT-Kernel with user-defined device functions, which are executed before or after the FFT is performed, when the data is still residing in the GPU's registers.

Unfortunately the discussed libraries do not offer the possibility to use callback functions, except for CuPy. \cite{cupyfftc93:online}
This test is therefore limited to a comparison between CuPy with and without the callback function.
Another drawback of callback functions is the fact that they need to be written in CUDA C.
Listings \ref{hann-windowing-callback-cuda-c} and \ref{hann-windowing-cupy} display the two different approaches to window a signal with a Hann-Window.


\begin{minipage}[t]{.49\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\tiny,
	breaklines=true
	]{CPP}
__device__ cufftComplex CB_ConvertInputC(
  void *dataIn,
  size_t offset,
  void *callerInfo,
  void *sharedPtr)
{
  int len = *((int *)callerInfo);

  cufftComplex* floatIn = (cufftComplex*)dataIn;
  cufftComplex element = floatIn[offset];
  float hann_factor = (0.5f - 0.5f*
    cospif((2.f*(float)(offset))/(float(len)-1.f)));
  element.x = element.x*hann_factor;
  element.y = element.y*hann_factor;
  return element;
}
__device__ cufftCallbackLoadC d_loadCallbackPtr = CB_ConvertInputC;
\end{minted}
	\captionof{listing}{Hann-Windowing via callback function in CUDA C}
	\label{hann-windowing-callback-cuda-c}
\end{minipage}\hfill%
\begin{minipage}[t]{.49\textwidth}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	bgcolor=LightGray,
	fontsize=\tiny,
	breaklines=true
	]{Python}
  def cupy_hann(n):
    return ((cp.ones(n, dtype=cp.float32) - cp.cos(cp.linspace(0, 2*cp.pi, n,  endpoint=False,dtype=cp.float32)))*0.5)

  cp.fft.fft(cupy_hann(size)*a)
\end{minted}
	\captionof{listing}{Hann-Windowing via multiplication with a CuPy array}
	\label{hann-windowing-cupy}
\end{minipage}%

The approach of using a callback function is more efficient, as it does not only avoid an additional memory round trip, but also avoids the need to store the windowing function in GPU memory.
Another drawback of the approach outlined in listing \ref{hann-windowing-callback-cuda-c} is the fact that it requires a re-compilation of the cuFFT before the first invocation (after all the callback function will be compiled into the kernel). This re-compilation can take up to 20 seconds, which is something that should be kept in mind (the kernel does not need to be re-compiled for different transform sizes).
The version listed in listing \ref{hann-windowing-cupy} is more convenient, since it does not require the user to write CUDA C (note how the CUDA C code is specific to single precision transforms) code, but allocates additional memory on the GPU: the original input data, the windowing function and the windowed data.
A third option is to calculate the windowing function once (either on the CPU or on the GPU), store it on the GPU and apply it to the input data by multiplication.
Table \ref{windowing-table} shows the results of benchmarks comparing the three different approaches.

The table lists a speedup of up to $400\%$ for the callback function in comparison to the option using repeated calls to the windowing function.
The version utilizing a callback is also a fair bit faster than the version using a cached window, with the upside of not having to construct and store the window function.

\begin{table}[h]
	\centering
	\rowcolors{4}{}{lightgray}
	\begin{footnotesize}
	\input{../fft_benchmark_suite/tables/fft_timings_callback.tex}
	\end{footnotesize}
	\caption{Comparison of the run-time of the three different approaches to windowing}
	\label{windowing-table}
\end{table}

The tables \ref{cupy-callback-table} to \ref{cupy-nocallback-cached-table} show the executed kernels by the respective variants tested for the transformation size of $2^{25}$.

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\begin{footnotesize}
	\input{../fft_benchmark_suite/tables/cupy_callback.tex}
	\end{footnotesize}
	\caption{Kernels executed in CuPy with the callback function}
	\label{cupy-callback-table}
\end{table}

Table \ref{cupy-callback-table} lists the kernels executed when using the callback function.
It can be seen that there is no extra kernel for the windowing, but the first FFT-kernel is altered to include the windowing function (the name \texttt{regular\_fft\_callback} reflects that change).
Here the windowing comes at virtually no cost, as the kernel is executed in the same time as the original kernel.

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\begin{scriptsize}
	\input{../fft_benchmark_suite/tables/cupy_nocallback.tex}
	\end{scriptsize}
	\caption{Kernels executed when repeatedly calling the cupy-hann function}
	\label{cupy-nocallback-table}
\end{table}

Table \ref{cupy-nocallback-table} shows the kernels executed when repeatedly calling the windowing function.
In this case there are many kernels, which need to be executed every time to construct the window function.
This is not especially efficient but might occur in practice, as it is a very convenient.
Note that this approach could be sped up by fusing all the kernels preceding the FFT into a single kernel, this could be achieved with PyTorch's JIT compiler.

\begin{table}[H]
	\centering
	\rowcolors{2}{}{lightgray}
	\begin{scriptsize}
	\input{../fft_benchmark_suite/tables/cupy_nocallbackcached.tex}
	\end{scriptsize}
	\caption{Kernels executed when re-using the result from cupy-hann}
	\label{cupy-nocallback-cached-table}
\end{table}

Table \ref{cupy-nocallback-cached-table} shows the kernels executed when re-using the result from the windowing function. This is also a quite efficient approach, as the windowing function is only calculated once and then re-used.
The multiplication with the input data is fast, as it is an element-wise operation, which is only memory bound.
Re-using a window function, residing on the host's memory, however would be much slower, as the data would have to be transferred to the GPU.

In summary, the callback functions for FFTs are a way to gain performance, but its effect is not as pronounced as one might expect.
It is unfortunate that not all libraries support this feature.

\subsubsection{Transforms of higher dimensionality}

As explored in section \ref{opencv_demo} FFTs of higher dimensionality involve FFTs along the individual axis of the array.
Between transforms along different axis it can be beneficial to re-order the data in memory (in 2D: transposing matrix), in order to improve the data locality and thus the performance of the subsequent FFT stages.

Tables \ref{highdim-cpu-table} and \ref{highdim-gpu-table} show the timings for 2D FFTs using CPU-based and GPU-based libraries, respectively.
The tables only list the times and speedups for the execution, excluding the time for the data transfer.
The listed size refers to the size of the input data, which is a 2D array of size $N \times N$ of real valued numbers.

The speedups achieved on the CPU (compared to the baseline of the Numpy FFT) are mostly in-line with the results for 1D FFTs.
The highest speedup was achieved by MKL-FFT (28.8 in single precision and 20.7 in double precision).

\begin{table}[H]
	\centering
	\rowcolors{4}{}{lightgray}
	\begin{footnotesize}
	\input{../fft_benchmark_suite/tables/fft_timings_highdim_table_cpu.tex}
	\end{footnotesize}
	\caption{Timings for 2D FFTs using CPU-based libraries}
	\label{highdim-cpu-table}
\end{table}

The GPU-based libraries manage to achieve even higher speedups than those measured for 1D FFTs.
Single precision transforms achieved speedups of up to 640 and double precision transforms of up to 120.
The equivalent test for 1D FFTs (see table \ref{complete-fft-table-gpu}) achieved speedups of up to 420 and 85, respectively.
The increase in speedup can be attributed to the better utilization of the GPU's streaming multiprocessors.
A great number of smaller FFTs require less synchronization between the different threads and thus allow for a higher degree of parallelism. 

\begin{table}[H]
	\centering
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/tables/fft_timings_highdim_table_gpu_standalone.pdf}
	\caption{Timings for 2D FFTs using GPU-based libraries}
	\label{highdim-gpu-table}
\end{table}

Transforms of higher dimensionality are also more likely to benefit from offloading to the GPU, as every number transferred is re-used in multiple FFTs.
This increases the arithmetic intensity, which enables the GPU to achieve higher performance.
Table \ref{memory-copy-overhead-table-2D} shows the performance of the different libraries for different transform sizes, when the data is copied to and from the GPU.
In this scenario the GPU can offer significant speedups of up to a factor of 60 for single precision and 25 for double precision.
The fastest library is again CuPy, as it achieves higher copy speeds than the other libraries.
This result makes a strong case for GPGPU acceleration of large (or batched) FFTs of higher dimensionality.
The GPU-based solution is faster than all CPU-based solutions, even for double precision, albeit not by a large margin (note that the baseline is Numpy, which is outperformed by libraries like MKL-FFT).

\begin{table}[H]
	\centering
	\includegraphics[width = \textwidth]{../fft_benchmark_suite/tables/fft_timings_mem_table_gpu_2D_standalone.pdf}
	\caption{Performance of the different libraries for different transform sizes, when the data is copied to and from the GPU, related to table \ref{highdim-gpu-table}}
	\label{memory-copy-overhead-table-2D}
\end{table}

\begin{comment}
\subsection{Results on cloud machines}

In order to test the tool on a wider variety of hardware we also ran the benchmarking tool on a variety of cloud machines with a selection of popular Nvidia GPUs (A100, V100, P6000) to cover the high-end models of recent architectures (Ampere, Volta and Pascal respectively), which are commonly found in high performance computing environments.
The most recent iteration of Nvidia's GPU architecture, Hopper, which is currently only available on H100 systems was ommited, due to the lack of availability of such systems on the cloud.

This test is especially insightful, since it allows us to draw conclusions about the benefit of faster, more expensive GPUs, in a setting which is bottlenecked by the memory bandwidth of the GPU and the PCI-E bus.

Of course one must not overlook that such GPUs with significantly more dedicated memory (up to 80 GB on the A100) would allow for different workflows, where the data (or multiple copies of the data in different stages of a signal processing pipeline) can be kept on the GPU, instead of being transferred back and forth between the CPU and the GPU.
\end{comment}

\section{Conclusion and Future Directions}
In this section we summarize the main findings of the thesis and provide an outlook on future research directions and emerging trends in the field.

\subsection{Key Findings}
The tests conducted show that one can achieve tremendous speed ups for common tasks often occuring in signal processing pipelines.
For especially well suited problems, those that can be broken down in thousands of smaller, independent problems, speed-ups in the range of 400 to 600 can be achieved.
Even though large parts of those, often claimed, high speed-ups vanish, when taking account the overhead associated with copying the data to and from the GPU, offloading certain tasks to the GPU can be worth the additional effort and complexity.
For such problems offloading to the GPU the overhead associated with copying the necessary data to and from the GPU is worth it, despite the overhead.
We encourage readers to apply the introduced techniques and tools to their own problems, as the speedups can be significant and allow for otherwise impossible use-cases.

\subsection{Implications for Signal Processing and GPU Programming}
To achieve the best performance it is of utmost importance to perform as many consecutive steps of the signal processing pipeline on the GPU to eliminate the need for expensive data movements.
This is a convenient requirement as the final step in many modern signal processing pipelines, especially in those real-time applications were performance is critical, is often a deep-learning model.
Those models are often complex and require powerful GPU-like hardware to achieve the desired performance, it is therefore a logical step to move as much signal processing to the already available GPU.
Such a situation, for example, is encountered in modern cars where the pre-processed data obtained from sensors like lidars and radars is used as input for deep-learning models to identify the position of surrounding vehicles and pedestrians.
In this scenario the processed data would not needed to be transferred back to the host's memory and half of the costly memory transfers could be avoided completely.

\subsubsection{New Frameworks}
Apart from the frameworks discussed, there are other emerging or already existing solutions worth mentioning.
This category of tools include Halide, DaCe and Triton.

Halide \cite{Halide} is a library and domain specific language, which aims to separate the order of operations (schedule) from the defined operations themselves (algortihm).
While Halide was originally designed as a library for image processing, it is very likely that the introduced concepts, which make it a popular library for accelerated image processing, would also apply to more generale signal processing.

DaCe \cite{dace} (short for data-centric parallel programming) is a set of tools for analyzing, re-ordering and optimizing graphs of computations needed.
It is especially appealing due to its graphical views.  

OpenAI's Triton \cite{triton} meanwhile is a programming language that treats blocks of threads as the lowest level of abstraction instead of individual threads.
This is a different programming style that lends itself very well to writing kernels which optimize for cache re-use and memory coalescing.
The drawback of this approach is that it does not allow for kernels, which require irregular memory access patterns like the FFT kernels discussed.

All of these tools have in common that they involve another layer of abstraction and an additional compilation step and try to seperate the expression of the algorithm from the actual algorithm in order to adapt it optimally to the hardware used.
Additionally they can all be used to generate code for multiple backends (CUDA, OpenCL, x86) by emitting an LLVM compatible intermediate represenation. 
By using these tools programmers hand over fine-grained control over the kernel in exchange for a wider range of automatic optimizations.
The fact that multiple tools, aiming to solve very similar problems, exist, shows that there are shortcomings in the currently offered tools and that there is demand for more abstract, higher-level tools.
It is likely that some of these approaches will mature and complement the current landscape of GPU programming tools.

\subsubsection{Future Research Directions}
Future further research in this area could focus on mixed-precision arithmetic and lower precision data-types (TF32, FP16, BF16), since newer GPU hardware is able to process vastly more data per time when dealing with these data-types, while FP64 performance is stagnating and not a priority.

Another possible direction would be to investigate the use of multiple GPUs in parallel.
In such a setting the GPUs could either perform completely independent taks, or they could be used to perform the same task on different parts of the data.
In any case, such a setup would require even more careful consideration of the data movements between the individual GPUs and the host's memory.


%\newpage
%\listoflistings
% Literatur
\newpage
%\fancyhead[L]{Literatur}
\fancyhead[L]{References}
%\addcontentsline{toc}{section}{Literatur} 
\addcontentsline{toc}{section}{References}
\printbibliography
\end{document}
